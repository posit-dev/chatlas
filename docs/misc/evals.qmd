---
title: Evals
callout-appearance: simple
---

As your chat app moves from prototype to production, it's essential to ensure it continues to behave as expected. That is, if you change the model, prompts, tools, or any other part of your app, how can you be sure you aren't degrading the user experience? This is where **evaluations** (aka "evals") come in.

Without evals, testing your chat app is often a manual, vibes-based, process:

```python
from chatlas import ChatOpenAI

chat = ChatOpenAI(system_prompt="You are a math tutor.")

# Manually check each response
chat.chat("What is 15 * 23?")  # Did it get this right?
chat.chat("What is the meaning of life?")  # Did it give a good answer?
```

This approach:

- ❌ Doesn't scale beyond a few examples
- ❌ Requires manual verification of each answer

Which makes it difficult/impossible to:

- ❌ Catch regressions when changing models, prompts, tools, etc
- ❌ Quantify how well you're meeting requirements
- ❌ Continuously deploy improvements with confidence

Evals help address these problems by providing a structured way to define expectations and quantitatively measure how well your chat app meets them. Here, we'll explore how to evaluate your `chat` app using the [Inspect AI](https://inspect.aisi.org.uk/) framework, which integrates seamlessly with chatlas.

::: callout-note
## Prerequisites

To use Inspect AI, you'll need [`inspect-ai`](https://pypistats.org/packages/inspect-ai), which comes with the `eval` extra:

```bash
pip install 'chatlas[eval]'
```
:::

## Get started

[Inspect AI](https://inspect.aisi.org.uk/) is a "batteries-included" framework specifically designed for evaluating LLM applications. Its main components include: **datasets** (i.e., test cases), **solvers** (i.e., your `chat` instance), and **scorers** (i.e., the grading logic). These components come together into a `Task`, which can produce evaluation result(s).

### Create a Task

To create a `Task`, you'll need to:

* Collect a `dataset` of representative `input` and `target` reponses.
* Translate your `chat` instance into a `solver` via the [`.to_solver()`](../reference/Chat.qmd#to_solver) method.
* Choose a `scorer` to grade the responses.

Later on, we'll discuss how to generate good datasets and scorers, but here's a starting template for creating a simple eval:


```{.python filename="my_eval.py"}
from chatlas import ChatOpenAI
from inspect_ai import Task, task
from inspect_ai.dataset import csv_dataset
from inspect_ai.scorer import model_graded_qa

chat = ChatOpenAI(system_prompt="You are a helpful assistant.")

@task
def my_eval():
    return Task(
        dataset=csv_dataset("my_eval_dataset.csv"),
        solver=chat.to_solver(),
        scorer=model_graded_qa(model="openai/gpt-4o-mini")
    )
```

```{.markdown filename="my_eval_dataset.csv"}
input,           target
What is 2 + 2?,  4
What is 10 * 5?, 50
```

### Get results

Once you have a script with one or more `@task`-decorated functions (see above), you can run them via the `inspect` CLI:

```bash
inspect eval my_eval.py
```

This runs all tasks in `my_eval.py`, passing a `grader_model` as a parameter to the task function. Once complete, you can interactively view the results with:

```bash
inspect view
```

<!-- TODO: insert screenshot of viewer -->

To learn more about running and viewing, see the Inspect AI docs on [eval options](https://inspect.aisi.org.uk/options.html) and the [log viewer](https://inspect.aisi.org.uk/logger.html).

::: callout-tip
### VSCode

Inspect also provides a [VS Code extension](https://inspect.aisi.org.uk/vscode.html) for running and viewing evals directly within the editor.
:::

## Understanding solvers 

The [`.to_solver()`](../reference/Chat.qmd#to_solver) method translates your `chat` instance into an Inspect `solver`. Think of it as "wrapping" your `chat` app so that it can be used generate responses for an Inspect eval. More specifically, think `chat.to_solver()` as creating a new object (the `solver`) that, when given an `input` from the eval `dataset`, will internally call your `chat` instance to generate a response. This `solver` captures important state from your `chat` instance, such as model, system prompt, conversation history, registered tools, model parameters, etc.

This hopefully provides some intuition about how to set up your evals, but let's also walk through a few important situations below.

### Conversation history

For evals that depend on conversation history, use the [`.set_turns()`](../reference/Chat.qmd#set_turns) method to pre-populate the chat history before converting it to a solver. For example:

```{.python filename="my_eval.py"}
chat = ChatOpenAI(system_prompt="You are a helpful assistant.")

# Could also use .chat() to build up history, but this is 
# faster, cheaper, and deterministic
chat.set_turns([
    Turn("user", "My name is Gregg."),
    Turn("assistant", "Hello Gregg! How can I assist you today?"),
])

@task
def my_eval():
    return Task(
        dataset=csv_dataset("my_eval_dataset.csv"),
        solver=chat.to_solver(),
        scorer=model_graded_qa(model="openai/gpt-4o-mini")
    )
```

This way, the `dataset` doesn't need to include the full conversation history for each `input` prompt; the `solver` will already provide the relevant background context.

```{.markdown filename="my_eval_dataset.csv"}
input,            target
What is my name?, Your name is Gregg -- how can I help you today?
```

### Tool calling

In addition to preserving conversation history, [`.to_solver()`](../reference/Chat.qmd#to_solver) will also preserve any registered tools on your `chat` instance. This means that if your `chat` app uses tools, you can still convert it to a `solver` for evals.

```{.python filename="my_eval.py"}
chat = ChatOpenAI(system_prompt="You are a helpful assistant.")

def get_current_date():
    """Get the current date in YYYY-MM-DD format."""
    return datetime.datetime.now().strftime("%Y-%m-%d")

chat.register_tool(get_current_date)

@task
def my_eval():
    return Task(
        dataset=csv_dataset("my_eval_dataset.csv"),
        solver=chat.to_solver(),
        scorer=model_graded_qa(model="openai/gpt-4o-mini")
    )
```

```{.markdown filename="my_eval_dataset.csv"}
input,            target
What is today's date?, The current date should be in YYYY-MM-DD format and should be October 2, 2025 or later.
```

::: callout-note
### What's in a `target`?

Depending on the scorer you choose, the `target` column in your dataset may represent different things. For example, with `model_graded_qa()`, the `target` provides grading guidance that the grader model will use to determine if the `solver`'s response is satisfactory. This could include specific criteria, desired answer formats, or other instructions. We'll discuss this more in the [Scorers](#scorers) section below.
:::

## Understanding scorers

A scorer assigns a grade to the `solver`'s response (i.e., `output`) using the relevant `input`+`target`. Inspect comes with many [built-in scorers](https://inspect.aisi.org.uk/scorers.html), ranging from simple exact matching to more complex LLM-based grading. 

In many cases, you'll want some of the more advanced LLM-based options like [`model_graded_qa()`](https://inspect.aisi.org.uk/reference/inspect_ai.scorer.html#model_graded_qa) and [`model_graded_fact()`](https://inspect.aisi.org.uk/reference/inspect_ai.scorer.html#model_graded_fact), since they can handle a wide range of `input`, `output`, and grading criteria. These scorers use another LLM (an entirely different one from the solver) to produce a fail/pass grade based on the `input`'s `target` and the solver's response. This is a good place to start (0-1 response), but you can also enable `partial_credit` to get 0.5 score for partially correct answers.[^partial-credit]

[^partial-credit]: One place where `partial_credit` can be useful is when evaluating code generation tasks. For example, if the solver generates code that is mostly correct but has a small bug, you might want to give it partial credit rather than marking it as entirely incorrect.

The main difference between these two graders is their intended use cases: `model_graded_fact()` is specifically for fact-based questions whereas `model_graded_qa()` is more general-purpose. In practice, this means that, for `model_graded_fact()`, `target` should contain the correct answer(s), whereas with `model_graded_qa()`, `target` should contain grading criteria and/or guidance. To understand better what the scorer actually sees in each case, it can help to actually look at the prompts they receive under the hood.

::: {.callout-tip collapse="true"}
### Show me the prompts!

Below are the prompt templates used by `model_graded_qa()` and `model_graded_fact()`.
Recall that `input` and `target` come from your eval `dataset`, and `output` is the `solver`'s response.

::: {.panel .panel-tabset}

### model_graded_qa

```markdown
You are assessing a submitted answer on a given task based on a criterion. 
Here is the data:

[BEGIN DATA]
************
[Task]: {input}
************
[Submission]: {output}
************
[Criterion]: {target}
************
[END DATA]

Does the submission meet the criterion?

{instructions}
```
:::


### model_graded_fact

```markdown
You are comparing a submitted answer to an expert answer on a given question. 
Here is the data:

[BEGIN DATA]
************
[Question]: {input}
************
[Expert]: {target}
************
[Submission]: {output}
************
[END DATA]

Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.

Does the submission contain the content in the expert answer?

{instructions}
```

And the default `instructions` are:

```
After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.  Please choose ONE option for the grade: either "C" for correct answers, {partial_prompt}or "I" for incorrect answers.

For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.

First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.
```
:::


To learn more about scorers, see the [Inspect AI docs](https://inspect.aisi.org.uk/scorers.html) for details.


## Collecting datasets

In Inspect, a `dataset` can be represented as a data frame with columns, minimally, `input` and `target`. A "sample" is a row of a dataset. `input` defines questions inputted by end users and `target` defines the target answer and/or grading guidance for it. What sorts of input prompts should you include, though?

In short, inputs should be **natural**. Rather than "setting up" the model with exactly the right context and phrasing, "[i]t's important that the dataset... represents the types of interactions that your AI will have in production" [@husain2024judge].

If your system is going to answer a set of questions similar to some set that already exists -- support tickets, for example -- use the actual tickets themselves rather than writing your own from scratch. In this case, refrain from correcting spelling errors, removing unneeded context, or doing any "sanitizing" before providing the system with the input; you want the distribution of inputs to resemble what the system will encounter in the wild as closely as possible.

If there is no existing resource of input prompts to pull from, still try to avoid this sort of unrealistic set-up. I'll specifically call out multiple choice questions here -- while multiple choice responses are easy to grade automatically, your inputs should only provide a system with multiple choices to select from if the production system will also have access to multiple choices [@press2024benchmarks]. If you're writing your own questions, I encourage you to read the ["Dimensions for Structuring Your Dataset" section](https://hamel.dev/blog/posts/llm-judge/#dimensions-for-structuring-your-dataset) from @husain2024judge, which provides a few axes to keep in mind when thinking about how to generate data that resembles what your system will ultimately see:

> You want to define dimensions that make sense for your use case. For example, here are ones that I often use for B2C applications:
>
> Features: Specific functionalities of your AI product.
>
> Scenarios: Situations or problems the AI may encounter and needs to handle.
>
> Personas: Representative user profiles with distinct characteristics and needs.

The other part of the "how" is the mechanics. You probably don't want to paste in a bunch of questions into a call to `tibble()`, escape a bunch of quotes, etc. Instead, here's how I've written evals:

-   Consider what metadata you want to keep track of along with your `input` and `target`. Do you want to tag questions with categories? Include a source URL? This isn't stuff that will actually be integrated into the eval, but that will be carried along throughout the evaluation to aid you with analysis down the line.
-   Prompt a model (Claude 4 Sonnet is quite good at this) to make you a Shiny app that will help you generate samples quickly (and, hopefully, joyfully). The app might provide you with a free text box to paste in the `input` and another for the `target`, fields for whatever metadata you'd like to carry along, and a "Submit" button. When you click "Submit", the app should write the inputted values into a `.yaml` file in some directory and "clear" the app so that you can then add a new sample. Here's a [sample prompt](https://github.com/simonpcouch/choreseval/blob/main/data-raw/sample-generator-prompt.txt) that I used to help me write an eval dataset recently.
-   Then, once you've created a sufficient number of samples (30 is a good place to start), read all of the yaml files in that directory into R, bind rows, and you've got a dataset. Here's a [sample script](https://github.com/simonpcouch/choreseval/blob/51694ad44f09fdef234f7348f4409c494c585bd3/data-raw/chores-dataset.R) that does this for the eval linked in the previous bullet.