[
  {
    "objectID": "tool-calling/approval.html",
    "href": "tool-calling/approval.html",
    "title": "Approvals",
    "section": "",
    "text": "Tools are incredibly powerful, but also potentially dangerous ‚Äì especially if someone else‚Äôs tool can run arbitrary code on your machine without your knowledge or consent. For this reason, it‚Äôs often important for end users to have the ability to deny tool calls before they are executed. The main mechanism for denying tool calls in chatlas is to throw a ToolRejectError exception.\nToolRejectError is usually raised from one of two places: 1. in a tool request callback handler or 2. within a tool function itself. Generally speaking it is recommended to use the first approach, as it guarantees that tool calls are never executed without user consent. To get a feel for how this works, let‚Äôs consider requiring approval from the Python REPL.\n\nPython REPL\nIn both of the examples below, we will use the input() function to prompt the user for approval before executing a tool call. This will pause execution and wait for the user to respond with either ‚Äúyes‚Äù or ‚Äúno‚Äù.\ndef input_approval(prompt: str) -&gt; bool:\n    while True:\n        res = input(f\"{prompt} (yes/no): \")\n        if res.lower() == \"yes\":\n            return True\n        elif res.lower() == \"no\":\n            return False\n        else:\n            print(\"Please answer with 'yes' or 'no'.\")\n\nRequest callbackWithin tool\n\n\nimport os\nimport chatlas as ctl\n\nchat = ctl.ChatOpenAI()\n\ndef list_files():\n    \"List files in the user's current directory\"\n    return os.listdir(\".\")\n\nchat.register_tool(list_files)\n\ndef on_request(req: ctl.ContentToolRequest):\n    \"Request callback to approve or deny tool calls\"\n    allow = input_approval(\n        f\"Would you like to allow the tool call '{req.name}'? (yes/no): \"\n    )\n    if allow:\n        return # proceed with the tool call\n    raise ctl.ToolRejectError(\n        f\"The user has chosen to disallow the tool call '{req.name}'.\"\n    )\n\nchat.on_tool_request(on_request)\n\nchat.chat(\"What files are available in my current directory?\")\n\n\nimport os\nimport chatlas as ctl\n\nchat = ctl.ChatOpenAI()\n\ndef list_files():\n    \"List files in the user's current directory\"\n    allow = input_approval(\n        \"Would you like to allow access to your current directory? (yes/no): \"\n    )\n    if allow:\n        return os.listdir(\".\")\n    raise ctl.ToolRejectError(\n        \"The user has chosen to disallow the tool call.\"\n    )\n\nchat.register_tool(list_files)\nchat.chat(\"What files are available in my current directory?\")\n\n\n\n\n# üõ†Ô∏è tool request\nlist_files()\n# ‚ùå tool error\nTool call failed with error: 'The user has chosen to disallow the tool call.'\nIt looks like I am unable to access the list of files in your current directory at the moment. If you have any other questions or need assistance, feel free to ask!\n\n\n\nChatbots\nComing soon.",
    "crumbs": [
      "Tool calling",
      "Approvals"
    ]
  },
  {
    "objectID": "tool-calling/how-it-works.html",
    "href": "tool-calling/how-it-works.html",
    "title": "How it works",
    "section": "",
    "text": "Tool calling introduced us to the basic mechanics of how to define a tool function and supply it to the chat model, which can then use it to answer user prompts. In this article, we take a step back and outline how tool calling actually works under the hood, which is important to understand for getting the most out of it.\nWhen making a chat request to the chat model, the caller advertises one or more tools (defined by their function name, description, and a list of expected arguments), and the chat model can choose to respond with one or more ‚Äútool calls‚Äù. These tool calls are requests from the chat model to the caller to execute the function with the given arguments; the caller is expected to execute the functions and ‚Äúreturn‚Äù the results by submitting another chat request with the conversation so far, plus the results. The chat model can then use those results in formulating its response, or, it may decide to make additional tool calls.\nNote that the chat model does not directly execute any external tools! It only makes requests for the caller to execute them. It‚Äôs easy to think that tool calling might work like this:\nBut in fact it works like this:\nThe value that the chat model brings is not in helping with execution, but with knowing when it makes sense to call a tool, what values to pass as arguments, and how to use the results in formulating its response.",
    "crumbs": [
      "Tool calling",
      "How it works"
    ]
  },
  {
    "objectID": "tool-calling/how-it-works.html#footnotes",
    "href": "tool-calling/how-it-works.html#footnotes",
    "title": "How it works",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome go so far as to suggest tool calling is a superior approach to classic RAG (retrieval-augmented generation) for many use cases.‚Ü©Ô∏é",
    "crumbs": [
      "Tool calling",
      "How it works"
    ]
  },
  {
    "objectID": "misc/examples.html",
    "href": "misc/examples.html",
    "title": "Motivating examples",
    "section": "",
    "text": "chatlas makes it easy to access the wealth of large language models (LLMs) from Python. But what can you do with those models once you have access to them? This vignette will give you the basic vocabulary you need to use an LLM effectively and will show you some examples to ignite your creativity.\nIn this article we‚Äôll mostly ignore how LLMs work, using them as convenient black boxes. If you want to get a sense of how they actually work, we recommend watching Jeremy Howard‚Äôs posit::conf(2023) keynote: A hacker‚Äôs guide to open source LLMs."
  },
  {
    "objectID": "misc/examples.html#example-uses",
    "href": "misc/examples.html#example-uses",
    "title": "Motivating examples",
    "section": "Example uses",
    "text": "Example uses\nNow that you‚Äôve got the basic vocab under your belt, I‚Äôm going to fire a bunch of interesting potential use cases at you. While there are special purpose tools that might solve these cases faster and/or cheaper, an LLM allows you to rapidly prototype a solution. This can be extremely valuable even if you end up using those more specialised tools in your final product.\nIn general, we recommend avoiding LLMs where accuracy is critical. That said, there are still many cases for their use. For example, even though they always require some manual fiddling, you might save a bunch of time even with an 80% correct solution. In fact, even a not-so-good solution can still be useful because it makes it easier to get started: it‚Äôs easier to react to something rather than to have to start from scratch with a blank page.\n\nProgramming\nLLMs can also be useful to solve general programming problems. For example:\n\nYou can use LLMs to explain code, or even ask them to generate a diagram.\nYou can ask an LLM to analyse your code for potential code smells or security issues. You can do this a function at a time, or explore including the entire source code for your package or script in the prompt.\nYou could automatically look up the documentation for an Python class/function, and include it in the prompt to make it easier to figure out how to use that class/function.\nI find it useful to have an LLM document a function for me, even knowing that it‚Äôs likely to be mostly incorrect. Having something to react to make it much easier for me to get started.\nIf you‚Äôre working with code or data from another programming language, you ask an LLM to convert it to Python code for you. Even if it‚Äôs not perfect, it‚Äôs still typically much faster than doing everything yourself.\nYou could use GitHub‚Äôs REST API to find unlabelled issues, extract the text, and ask the LLM to figure out what labels might be most appropriate. Or maybe an LLM might be able to help people create better reprexes, or simplify reprexes that are too complicated?\nWrite a detailed prompt that teaches the LLM about something it wouldn‚Äôt otherwise know about. For example, you might write a guide to updating code to use a new version of a package. If you have a programmable IDE, you could imagine being able to select code, transform it, and then replace the existing text. A real example of this is the R package chores, which includes prompts for updating source code to use the latest conventions in R for documentation, testing, error handling, and more."
  },
  {
    "objectID": "misc/examples.html#miscellaneous",
    "href": "misc/examples.html#miscellaneous",
    "title": "Motivating examples",
    "section": "Miscellaneous",
    "text": "Miscellaneous\nTo finish up here are a few other ideas that seem cool but didn‚Äôt seem to fit the above categories:\n\nAutomatically generate alt text for plots, using content_image_plot().\nAnalyse the text of your statistical report to look for flaws in your statistical reasoning (e.g.¬†misinterpreting p-values or assuming causation where only correlation exists).\nUse your existing company style guide to generate a brand.yaml specification to automatically style your reports, apps, dashboards and plots to match your corporate style guide."
  },
  {
    "objectID": "why-chatlas.html",
    "href": "why-chatlas.html",
    "title": "chatlas",
    "section": "",
    "text": "Compared to other popular Python LLM libraries, chatlas focuses on making the barrier to entry as low as possible. It also makes certain design decisions that make common tasks less tedious and generally help you be more productive and prototype faster.\nA few good examples of this are:\n\nThe Chat class retains conversation history for you.\nStreaming output ‚Äújust works‚Äù out of the box ‚Äì no extra code needed.\nchatlas handles the details of tool calling for you ‚Äì just provide a function with a docstring + type hints. You‚Äôll even get smart default streaming output of tool requests/results for free.\n\nTo drive these points home, let‚Äôs walk through a few examples:\n\nConversation history\nTake, for example, a simple multi-turn conversation:\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(\n    model=\"gpt-4.1\",\n    system_prompt=\"You are a terse assistant.\",\n)\n\nchat.chat(\"What is the capital of the moon?\")\nchat.chat(\"Are you sure?\")\nImplementing the same functionality with LangChain requires something much more complex:\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\n# The underlying chat model. It doesn't manage any state, so we need to wrap it.\nmodel = ChatOpenAI(model=\"gpt-4.1\")\n\n# This is how you provide a system message in Langchain. Surprisingly\n# complicated, isn't it?\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\"You are a terse assistant.\"),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\n# Wrap the model and prompt up with some history.\nhistory = InMemoryChatMessageHistory()\nclient = RunnableWithMessageHistory(prompt | model, lambda: history)\n\n# We're ready to chat with the model now. For this example we'll make a blocking\n# call, but there are ways to do async, streaming, and async streaming as well.\nresponse = client.invoke(\"What is the capital of the moon?\")\nprint(response.content)\n\n# The input of invoke() can be a message object as well, or a list of messages.\nresponse2 = client.invoke(HumanMessage(\"Are you sure?\"))\nprint(response2.content)\nOther frameworks like Pydantic AI and llm do a better\nIt also:\n\nDesigns for the developer experience, with a focus on rich streaming output at the console, in notebooks, and in web apps."
  },
  {
    "objectID": "structured-data/article-summary.html",
    "href": "structured-data/article-summary.html",
    "title": "Article summary",
    "section": "",
    "text": "The following example, which closely inspired by the Claude documentation, shows how .extract_data() can be used to summarize an article.\n\n\nClick to show text\n\ntext = \"\"\"\nPolicy\nThird-party testing as a key ingredient of AI policy\nMar 25, 2024‚óè18 min read\n\nWe believe that the AI sector needs effective third-party testing for frontier AI systems. Developing a testing regime and associated policy interventions based on the insights of industry, government, and academia is the best way to avoid societal harm‚Äîwhether deliberate or accidental‚Äîfrom AI systems.\n\nOur deployment of large-scale, generative AI systems like Claude has shown us that work is needed to set up the policy environment to respond to the capabilities of today‚Äôs most powerful AI models, as well as those likely to be built in the future. In this post, we discuss what third-party testing looks like, why it‚Äôs needed, and describe some of the research we‚Äôve done to arrive at this policy position. We also discuss how ideas around testing relate to other topics on AI policy, such as openly accessible models and issues of regulatory capture.\n\nPolicy overview\n\nToday‚Äôs frontier AI systems demand a third-party oversight and testing regime to validate their safety. In particular, we need this oversight for understanding and analyzing model behavior relating to issues like election integrity, harmful discrimination, and the potential for national security misuse. We also expect more powerful systems in the future will demand deeper oversight - as discussed in our ‚ÄòCore views on AI safety‚Äô post, we think there‚Äôs a chance that today‚Äôs approaches to AI development could yield systems of immense capability, and we expect that increasingly powerful systems will need more expansive testing procedures. A robust, third-party testing regime seems like a good way to complement sector-specific regulation as well as develop the muscle for policy approaches that are more general as well.\n\nDeveloping a third-party testing regime for the AI systems of today seems to give us one of the best tools to manage the challenges of AI today, while also providing infrastructure we can use for the systems of the future. We expect that ultimately some form of third-party testing will be a legal requirement for widely deploying AI models, but designing this regime and figuring out exactly what standards AI systems should be assessed against is something we‚Äôll need to iterate on in the coming years - it‚Äôs not obvious what would be appropriate or effective today, and the way to learn that is to prototype such a regime and generate evidence about it.\n\nAn effective third-party testing regime will:\n\nGive people and institutions more trust in AI systems\nBe precisely scoped, such that passing its tests is not so great a burden that small companies are disadvantaged by them\nBe applied only to a narrow set of the most computationally-intensive, large-scale systems; if implemented correctly, the vast majority of AI systems would not be within the scope of such a testing regime\nProvide a means for countries and groups of countries to coordinate with one another via developing shared standards and experimenting with Mutual Recognition agreements\n\nSuch a regime will have the following key ingredients [1]:\n\nEffective and broadly-trusted tests for measuring the behavior and potential misuses of a given AI system\nTrusted and legitimate third-parties who can administer these tests and audit company testing procedures\nWhy we need an effective testing regime\n\nThis regime is necessary because frontier AI systems‚Äîspecifically, large-scale generative models that consume substantial computational resources‚Äîdon‚Äôt neatly fit into the use-case and sector-specific frameworks of today. These systems are designed to be 'everything machines' - Gemini, ChatGPT, and Claude can all be adapted to a vast number of downstream use-cases, and the behavior of the downstream systems always inherits some of the capabilities and weaknesses of the frontier system it relies on.\n\nThese systems are extremely capable and useful, but they also present risks for serious misuse or AI-caused accidents. We want to help come up with a system that greatly reduces the chance of major misuses or accidents caused by AI technology, while still allowing for the wide deployment of its beneficial aspects. In addition to obviously wanting to prevent major accidents or misuse for its own sake, major incidents are likely to lead to extreme, knee-jerk regulatory actions, leading to a 'worst of both worlds' where regulation is both stifling and ineffective. We believe it is better for multiple reasons to proactively design effective and carefully thought through regulation.\n\nSystems also have the potential to display emergent, autonomous behaviors which could lead to serious accidents - for instance, systems might insert vulnerabilities into code that they are asked to produce or, when asked to carry out a complex task with many steps, carry some actions which contradict human intentions. Though these kinds of behaviors are inherently hard to measure, it‚Äôs worth developing tools to measure for them today as insurance against these manifesting in widely deployed systems.\n\nAt Anthropic, we‚Äôve implemented self-governance systems that we believe should meaningfully reduce the risk of misuse or accidents from the technologies we‚Äôve developed. Our main approach is our Responsible Scaling Policy (RSP), which commits us to testing our frontier systems, like Claude, for misuses and accident risks, and to deploy only models that pass our safety tests. Multiple other AI developers have subsequently adopted or are adopting frameworks that bear a significant resemblance to Anthropic's RSP.\n\nHowever, although Anthropic is investing in our RSP (and other organizations are doing the same), we believe that this type of testing is insufficient as it relies on self-governance decisions made by single, private sector actors. Ultimately, testing will need to be done in a way which is broadly trusted, and it will need to be applied to everyone developing frontier systems. This type of industry-wide testing approach isn‚Äôt unusual - most important sectors of the economy are regulated via product safety standards and testing regimes, including food, medicine, automobiles, and aerospace.\n\nWhat would a robust testing regime look like?\n\nA robust third-party testing regime can help identify and prevent the potential risks of AI systems. It will require:\n\nA shared understanding across industry, government, and academia of what an AI safety testing framework looks like - what it should and shouldn‚Äôt include\nAn initial period where companies complete practice runs of implementing such testing, sometimes with third-party oversight, to make sure the tests work, are feasible to run, and can be validated by a third party\nA two-stage testing regime: there should be a very fast, automated testing stage that companies apply to their systems. This stage should cover a wide area and be biased towards avoiding false negatives. If this stage spots potential problems, there should be a more thorough secondary test, likely using expert human-led elicitation\nIncreased resources to the parts of government that will oversee and validate tests - building and analyzing tests is detailed, expensive, technical work, so governments will need to find a way to fund the entities that do this\nA carefully scoped set of mandated tests - we‚Äôll need specific, legally mandated tests where it becomes clear there are poor incentives for industry self-governance, and the benefits of public safety from government oversight outweigh the regulatory burdens. We should ensure this is a well scoped, small set of tests, or else we‚Äôll create regulatory burdens and increase the possibility of regulatory capture\nAn effective balance of the assurance of safety with ease of administration of these tests\n\nWhen it comes to tests, we can already identify one area today where testing by third-parties seems helpful and draws on the natural strengths of governments: national security risks. We should identify a set of AI capabilities that, if misused, could compromise national security, then test our systems for these capabilities. Such capabilities might include the ability to meaningfully speed up the creation of bioweapons or to carry out complex cyberattacks. (If systems are capable of this, then that would lead to us changing how we deployed the model - e.g, remove certain capabilities from broadly deployed models and/or gate certain model capabilities behind ‚Äòknow your customer‚Äô regimes, and ensuring relevant government agencies were aware we had systems with these capabilities.) We expect there are several areas where society will ultimately demand there be legitimate, third-party testing approaches, and national security is just one of them.\n\nWhen it comes to the third party doing the testing, there will be a multitude of them and the tests will be carried out for different reasons, which we outline here:\n\nPrivate companies: Companies may subcontract other companies to build tests and evaluations for their systems, as we have done with firms like Gryphon Scientific. We can also imagine companies doing tests for other companies where the tests are mandated by law but not carried out by government agencies, similar to how accounting firms audit the books of private companies.\nUniversities: Today, many researchers at many academic institutions have free or subsidized access to models developed by AI labs; in the future, we could imagine some of these research institutions administering their own testing initiatives, some of which may be supervised or elevated via government bodies.\nGovernments: Some tests (we suspect, a relatively small number) may be mandated by law and carried out by government actors - for instance, for testing for national security misuses of AI systems. Here, government agencies may carry out the tests directly.\n\nUltimately, we expect that third-party testing will be accomplished by a diverse ecosystem of different organizations, similar to how product safety is achieved in other parts of the economy today. Because broadly commercialized, general purpose AI is a relatively new technology, we don‚Äôt think the structure of this ecosystem is clear today and it will become clearer through all the actors above running different testing experiments. We need to start working on this testing regime today, because it will take a long time to build.\n\nWe believe that we - and other participants in AI development - will need to run multiple testing experiments to get this right. The stakes are high: if we land on an approach that doesn‚Äôt accurately measure safety but is easy to administer, we risk not doing anything substantive or helpful. If we land on an approach that accurately measures safety but is hard to administer, we risk creating a testing ecosystem that favors companies with greater resources and thus reduces the ability for smaller actors to participate.\n\nHow Anthropic will support fair, effective testing regimes\n\nIn the future, Anthropic will carry out the following activities to support governments in the development of effective third-party testing regimes:\n\nPrototyping a testing regime via implementing our RSP and sharing what we learn\nTesting third-party assessment of our systems via contractors and government partners\nDeepening our frontier red teaming work to give us and the broader sector a clearer sense of the risks of AI systems and their mitigations\nAdvocating for governments to fund the agencies and organizations that could help to develop an effective third-party testing regime (e.g, in the United States, NIST, the US AI Safety Institute, the National AI Research Resource, the usage of DoE supercomputers for AI testing, and so on)\nEncouraging governments to build their own ‚ÄòNational Research Clouds‚Äô (like the National AI Research Resource in the US) so that they can a) develop independent capacity in academia and government to build, study, and test frontier AI systems, and b) work on the science of evaluating AI systems, including those developed by private companies like Anthropic\n\nDeveloping a testing regime and associated policy interventions based on the insights of industry, government, and academia is the best way to avoid societal harm‚Äîwhether deliberate or accidental‚Äîfrom AI systems.\n\nHow testing connects to our broader policy priorities\n\nOur overarching policy goal is to have appropriate oversight of the AI sector. We believe this will mostly be achieved via there being an effective ecosystem for third-party testing and evaluation of AI systems. Here are some AI policy ideas you can expect to see us advocating for in support of that:\n\nGreater funding for AI testing and evaluation in government\n\nEffective testing and evaluation procedures are a necessary prerequisite of any effective form of AI policy. We think that governments should stand up and support institutions that develop AI evaluations, as well as bringing together industry, academia, and other stakeholders to agree on standards for the safety of AI systems. In the US, we specifically advocate for greater funding for NIST.\n\nSupport greater evaluation of AI systems through public sector infrastructure for doing AI research\n\nWe urgently need to increase the number and breadth of people working to test and evaluate AI systems, for both current and future risks. It‚Äôs therefore crucial that governments create experimental infrastructure to help academic researchers test out and evaluate frontier AI systems, and develop their own frontier systems for beneficial purposes. For more information, see our support for a US national research cloud via the CREATE AI Act, and our written Senate testimony.\n\nDeveloping tests for specific, national security-relevant capabilities\n\nWe should know if AI systems can be used in ways that strengthen or (if fielded by another entity) weaken national security. Whereas the private sector and academia can develop the vast majority of tests, some testing and evaluation questions relate to national security capabilities which are classified, so only certain governments are able to effectively evaluate them. Therefore, we want to support US Government efforts to develop ways of testing AI systems for national security-relevant capabilities. We will also continue our own work to better understand the capabilities of our own systems.\n\nScenario planning and test development for increasingly advanced systems\n\nOur Responsible Scaling Policy is designed to frontload work about evaluating and testing future, hypothetical capabilities of AI systems. This is to ensure we have the relevant tests to better assess and minimize accident and misuse risks from increasingly powerful AI systems. But we don‚Äôt claim that our RSP delineates all the tests that need to be run on increasingly powerful models. As AI advances driven by growing computational power increase, a broader set of actors should work to anticipate the future capabilities of AI systems, and develop tests for them.\nAspects of AI policy we believe are important to discuss\n\nWhile developing our policy approach, we‚Äôve also found ourselves returning again and again to a few specific issues such as openly accessible models and regulatory capture. We‚Äôve outlined our current policy thinking below but recognize these are complicated issues where people often disagree.\n\nOpenly-disseminated and/or open-source models: Science moves forward largely due to a culture of openness and transparency around research. This is especially true in AI, where much of the currently-unfolding revolution is built on the open publication of research and models like the Transformer, BERT, Vision Transformers, and so on. There is also a long history of open source and openly accessible systems increasing the robustness of the security environment by helping a greater number of people experiment with technologies and identify their potential weaknesses.\n\nWe believe that the vast majority of AI systems today (perhaps even all of them) are safe to openly disseminate and will be safe to broadly disseminate in the future. However, we believe in the future it may be hard to reconcile a culture of full open dissemination of frontier AI systems with a culture of societal safety.\n\nIf ‚Äî and ‚Äòif‚Äô is a key and unresolved point ‚Äî increasingly capable AI models can lead to detrimental effects, or hold the possibility of catastrophic accidents, then we‚Äôll need to adjust the norms of what is openly disseminated at the frontier.\n\nSpecifically, we‚Äôll need to ensure that AI developers release their systems in a way that provides strong guarantees for safety - for example, if we were to discover a meaningful misuse of our model, we might put in place classifiers to detect and block attempts to elicit that misuse, or we might gate the ability to finetune a system behind a ‚Äòknow your customer‚Äô rule along with contractual obligations to not finetune towards a specific misuse. By comparison, if someone wanted to openly release the weights of a model which was capable of the same misuse, they would need to both harden the model against that misuse (e.g, via RLHF or RLAIF training) and find a way to make this model resilient to attempts to fine-tune it onto a dataset that would enable this misuse. We will also need to experiment with disclosure processes, similar to how the security community has developed norms around pre-notification of disclosures of zero days.\n\nThough what we‚Äôve described is inherently very costly we also believe it is necessary - we must do everything we can to avoid AI systems enabling significant misuses or causing major accidents. But carrying out any restrictions on the open dissemination of AI systems depends on there being broad agreement on what unacceptable misuses of AI systems or system behaviors are.\n\nAnthropic is not an impartial actor here - we are a company that primarily develops proprietary systems, and we don‚Äôt have the legitimacy to make claims here about what should or shouldn‚Äôt be acceptable in openly disseminated systems. Therefore, to resolve questions of open source models we need legitimate third parties to develop testing and evaluation approaches that are broadly accepted as legitimate, we need these third parties (or other trusted entities) to define a narrow and serious set of misuses of AI systems as well as adverse AI system behaviors, and we will need to apply these tests to models that are both controlled (e.g., via API) or openly disseminated (e.g., via the weights being released).\n\nThird party testing of openly disseminated and closed proprietary models can generate the essential information we need to understand the safety properties of the AI landscape [2]. If we don‚Äôt do this, then you could end up in a situation where either a proprietary model or openly accessible model directly enables a serious misuse or causes a major AI accident - and if that happens, there could be significant harm to people and also likely adverse regulations applied to the AI sector.\nRegulatory capture: Any form of policy can suffer regulatory capture by a sufficiently motivated and well-resourced actor: for example, a well-capitalized AI company. Some of the ideas we discuss above about openly accessible models are the kinds of things which themselves are prone to regulatory capture. It‚Äôs important that the AI ecosystem remains robust and competitive - AI is a complicated field and humanity‚Äôs best chance at getting it right likely comes from there being a diverse, broad set of actors engaged in its development and oversight.\n\nWe generally advocate for third-party testing and measurement initiatives because they seem like the kind of policy infrastructure that helps us to identify and prevent concrete harms as well as building capacity that exists independently of large companies. Therefore, we think that focusing on the development of third-party testing capacity can reduce the risk of regulatory capture and create a level playing field for developers. Conversely, industry-led consortia might have a tendency to favor approaches that involve high compliance costs on the parts of companies regardless of their scale - an approach that inherently advantages larger businesses which can spend more money on policy compliance.\nWhy we‚Äôre being careful in what we advocate for in AI policy\n\nWhen developing our policy positions, we assume that regulations tend to create an administrative burden both for the party that enforces the regulation (e.g, the government), and for the party targeted by the regulation (e.g, AI developers). Therefore, we should advocate for policies that are both practical to enforce and feasible to comply with. We also note that regulations tend to be accretive - once passed, regulations are hard to remove. Therefore, we advocate for what we see as the ‚Äòminimal viable policy‚Äô for creating a good AI ecosystem, and we will be open to feedback.\n\nWhy AI policy is important\n\nThe AI systems of today and those of the future are immensely powerful and are capable of yielding great benefits to society. We also believe these systems have the potential for non-trivial misuses, or could cause accidents if implemented poorly. Though the vast majority of our work is technical in nature, we‚Äôve come to believe that testing is fundamental to the safety of our systems - it‚Äôs not only how we better understand the capabilities and safety properties of our own models, but also how third-parties can validate claims we make about AI systems.\n\nWe believe that building out a third-party testing ecosystem is one of the best ways for bringing more of society into the development and oversight of AI systems. We hope that by publishing this post we‚Äôve been able to better articulate the benefits of third-party testing as well as outline our own position for others to critique and build upon.\n\nFootnotes\n\n[1] Some countries may also experiment with ‚Äòregulatory markets‚Äô where AI developers can buy and sell AI testing services and compete with one another to try to build and deploy successively safer, more useful systems.\n\n[2] For example, if you openly release an AI model, it‚Äôs relatively easy for a third-party to fine-tune that model on a dataset of their own choosing. Such a dataset could be designed to optimize for a misuse (e.g, phishing or offensive hacking). If you were able to develop technology that made it very hard to fine-tune an AI model away from its original capability distribution, then it‚Äôd be easier to confidently release models without potentially compromising on downstream safety.\n\"\"\"\n\nfrom chatlas import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\nclass ArticleSummary(BaseModel):\n    \"\"\"Summary of the article.\"\"\"\n\n    author: str = Field(description=\"Name of the article author\")\n\n    topics: list[str] = Field(\n        description=\"Array of topics, e.g. ['tech', 'politics']. Should be as specific as possible, and can overlap.\"\n    )\n\n    summary: str = Field(\n        description=\"Summary of the article. One or two paragraphs max\"\n    )\n\n    coherence: int = Field(\n        description=\"Coherence of the article's key points, 0-100 (inclusive)\"\n    )\n\n    persuasion: float = Field(\n        description=\"Article's persuasion score, 0.0-1.0 (inclusive)\"\n    )\n\n\nchat = ChatOpenAI()\nchat.extract_data(text, data_model=ArticleSummary)\n\n{\n  \"author\": \"Anthropic\",\n  \"topics\": [\n    \"AI policy\",\n    \"third-party testing\",\n    \"AI safety\",\n    \"regulatory framework\",\n    \"open-source AI\"\n  ],\n  \"summary\": \"This article argues for the necessity of third-party testing in the AI sector to mitigate potential risks associated with frontier AI systems. It emphasizes developing a comprehensive testing regime, involving collaboration between industry, government, and academia, to ensure AI technologies are safe and free from misuse. The article describes how such testing could prevent societal harm while fostering trust in AI systems. Anthropic outlines its policy priorities and initiatives in line with these goals, stressing the need for effective evaluation systems and national security measures to address the challenges posed by increasingly capable AI models.\",\n  \"coherence\": 85,\n  \"persuasion\": 0.75\n}",
    "crumbs": [
      "Structured data",
      "Article summary"
    ]
  },
  {
    "objectID": "structured-data/sentiment-analysis.html",
    "href": "structured-data/sentiment-analysis.html",
    "title": "Sentiment analysis",
    "section": "",
    "text": "Sentiment analysis is a common task in natural language processing (NLP) that involves determining the sentiment or emotional tone of a piece of text. This can be useful for various applications, such as social media monitoring, customer feedback analysis, and more.\nThe following examples, which closely inspired by the Claude documentation, hint at some of the ways you can use structured data extraction.\n#| warning: false\nimport chatlas as ctl\nfrom pydantic import BaseModel, Field\n\ntext = \"The product was okay, but the customer service was terrible. I probably won't buy from them again.\"\n\nclass Sentiment(BaseModel):\n    \"\"\"Extract the sentiment scores of a given text. Sentiment scores should sum to 1.\"\"\"\n\n    positive_score: float = Field(\n        description=\"Positive sentiment score, ranging from 0.0 to 1.0\"\n    )\n\n    negative_score: float = Field(\n        description=\"Negative sentiment score, ranging from 0.0 to 1.0\"\n    )\n\n    neutral_score: float = Field(\n        description=\"Neutral sentiment score, ranging from 0.0 to 1.0\"\n    )\n\n\nchat = ctl.ChatOpenAI()\nchat.extract_data(text, data_model=Sentiment)\n\n{\"positive_score\": 0.1, \"negative_score\": 0.7, \"neutral_score\": 0.2}\n\n\n\n\n\n\n\nThe data model does specify that the scores should sum to 1, but this is not enforced by the model. The model will do its best to provide a reasonable output, but it may not always adhere to this constraint.",
    "crumbs": [
      "Structured data",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "structured-data/multi-modal.html",
    "href": "structured-data/multi-modal.html",
    "title": "Multi-modal input",
    "section": "",
    "text": "PDFs\nThis example comes from Google‚Äôs cookbook and extracts structured data from a PDF invoice. The goal is to extract the invoice number, date, and all list items with description, quantity, and gross worth, as well as the total gross worth.\nimport chatlas as ctl\nfrom pydantic import BaseModel, Field\n\n\nclass Item(BaseModel):\n    description: str = Field(description=\"The description of the item\")\n    quantity: float = Field(description=\"The Qty of the item\")\n    gross_worth: float = Field(description=\"The gross worth of the item\")\n\n\nclass Invoice(BaseModel):\n    \"\"\"Extract the invoice number, date and all list items with description, quantity and gross worth and the total gross worth.\"\"\"\n\n    invoice_number: str = Field(description=\"The invoice number e.g. 1234567890\")\n    date: str = Field(description=\"The date of the invoice e.g. 10/09/2012\")\n    items: list[Item] = Field(\n        description=\"The list of items with description, quantity and gross worth\"\n    )\n    total_gross_worth: float = Field(description=\"The total gross worth of the invoice\")\n\n\n_ = Invoice.model_rebuild()\n\nchat = ctl.ChatOpenAI()\nchat.extract_data(\n    \"https://storage.googleapis.com/generativeai-downloads/data/pdf_structured_outputs/invoice.pdf\",\n    data_model=Invoice,\n)\n\n{\n  'invoice_number': 'INV-123456789',\n  'date': '09/10/2023',\n  'items': [\n    {'description': 'Laptop', 'quantity': 2, 'gross_worth': 2000},\n    {'description': 'Smartphone', 'quantity': 5, 'gross_worth': 3500},\n    {'description': 'Tablet', 'quantity': 3, 'gross_worth': 1200}\n  ],\n  'total_gross_worth': 6700\n}\n\n\n\nImages\nThis example comes from Dan Nguyen (you can see other interesting applications at that link). The goal is to extract structured data from this screenshot:\n\n\n\nScreenshot of schedule A: a table showing assets and ‚Äúunearned‚Äù income\n\n\nEven without any descriptions, ChatGPT does pretty well:\n\nimport chatlas as ctl\nfrom pydantic import BaseModel, Field\nimport pandas as pd\n\nclass Asset(BaseModel):\n    assert_name: str\n    owner: str\n    location: str\n    asset_value_low: int\n    asset_value_high: int\n    income_type: str\n    income_low: int\n    income_high: int\n    tx_gt_1000: bool\n\nclass DisclosureReport(BaseModel):\n    assets: list[Asset]\n\nchat = ctl.ChatOpenAI()\ndata = chat.extract_data(\n    ctl.content_image_file(\"../images/congressional-assets.png\"),\n    data_model=DisclosureReport,\n)\npd.DataFrame(data[\"assets\"])\n\n\n\n\n\n\n\n\nassert_name\nowner\nlocation\nasset_value_low\nasset_value_high\nincome_type\nincome_low\nincome_high\ntx_gt_1000\n\n\n\n\n0\n11 Zinfandel Lane - Home & Vineyard [RP]\nJT\nSt. Helena/Napa, CA, US\n5000001\n25000000\nGrape Sales\n100001\n1000000\nTrue\n\n\n1\n25 Point Lobos - Commercial Property [RP]\nSP\nSan Francisco/San Francisco, CA, US\n5000001\n25000000\nRent\n100001\n1000000\nTrue",
    "crumbs": [
      "Structured data",
      "Multi-modal input"
    ]
  },
  {
    "objectID": "get-started/monitor.html",
    "href": "get-started/monitor.html",
    "title": "Monitor",
    "section": "",
    "text": "As mentioned in the debugging section, chatlas has support for gaining more insight into the behavior of your application through things like logging.\nHowever, in a production setting, you may want to go beyond simple logging and use more sophisticated observability tools Datadog, Logfire, etc., to monitor your application. These tools can give you a more structured way to view and monitor app performance, including things like latency, error rates, and other metrics. These tools tend to integrate well with open standards like OpenTelemetry (OTel), meaning if you ‚Äúinstrument‚Äù your app with OTel, you can view your app‚Äôs telemetry data in any observability tool that supports OTel. There are at least a few different ways to do this, but we‚Äôll cover some of the more simpler approaches here.",
    "crumbs": [
      "Get started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#openllmetry",
    "href": "get-started/monitor.html#openllmetry",
    "title": "Monitor",
    "section": "OpenLLMetry",
    "text": "OpenLLMetry\nThe simplest (and most model agnostic) way to instrument your app with OTel is to leverage openllmetry, which can be as easy as adding the following code to your app:\npip install traceloop-sdk\nfrom traceloop.sdk import Traceloop\n\nTraceloop.init(\n  app_name=\"my app name\",\n  disable_batch=True,\n  telemetry_enabled=False\n)\nThis approach does have the downside of requiring a Traceloop account, but it does provide a free tier, and makes it quite easy to get started visualizing your app‚Äôs telemetry data.\nIf you want to avoid the Traceloop account, you can also use their OTel instrumentation libraries (e.g., openai and anthropic) more directly. If Traceloop is not for you, however, you may prefer to use the ‚Äúofficial‚Äù OTel libraries directly, which are more truly vendor agnostic.",
    "crumbs": [
      "Get started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#opentelemetry",
    "href": "get-started/monitor.html#opentelemetry",
    "title": "Monitor",
    "section": "OpenTelemetry",
    "text": "OpenTelemetry\nTo use OpenTelemetry‚Äôs ‚Äúofficial‚Äù instrumentation libraries, you‚Äôll need to first install the relevant instrumentation packages for the model providers you are using.\n\nOpenAI\nMore than a handful of chatlas‚Äô model providers use the openai Python SDK under the hood (e.g., ChatOpenAI, ChatOllama, etc).\n\n\n\n\n\n\nHow to check if a provider uses the openai SDK\n\n\n\n\n\nTo be sure a particular provider uses the openai SDK, make sure the class of the .provider attribute is OpenAIProvider:\nfrom chatlas import ChatOpenAI\nchat = ChatOpenAI()\nchat.provider\n# &lt;chatlas._openai.OpenAIProvider object at 0x103d2fdd0&gt;\n\n\n\nAs a result, you can use the opentelemetry-instrumentation-openai-v2 package to add OTel instrumentation your app. It even provides a way to add instrumentation without modifying your code (i.e., zero-code). To tweak the zero-code example to work with chatlas, just change the requirements.txt and main.py files to use chatlas instead of openai directly:\n\n\nmain.py\n\nfrom chatlas import ChatOpenAI\nchat = ChatOpenAI()\nchat.chat(\"Hello world!\")\n\nYou may also want to tweak the environment variables in .env to target the relevant OTel collector and service name.\n\n\nAnthropic\nBoth the ChatAnthropic() and ChatBedrockAnthropic() providers use the anthropic Python SDK under the hood. As a result, you can use the opentelemetry-instrumentation-anthropic package to add OTel instrumentation your app.\nTo do this, you‚Äôll need to install the package:\npip install opentelemetry-instrumentation-anthropic\nThen, add the following instrumentation code to your app:\nfrom opentelemetry.instrumentation.anthropic import AnthropicInstrumentor\nAnthropicInstrumentor().instrument()\n\n\nGoogle\nBoth the ChatGoogle() and ChatVertex() providers use the google-genai Python SDK under the hood. As a result, you can use the opentelemetry-instrumentation-google-genai package to add OTel instrumentation your app. It even provides a way to add instrumentation without modifying your code (i.e., zero-code). To tweak the zero-code example to work with chatlas, just change the requirements.txt and main.py files to use chatlas instead of google-genai directly:\n\n\nmain.py\n\nfrom chatlas import ChatGoogle\nchat = ChatGoogle()\nchat.chat(\"Hello world!\")",
    "crumbs": [
      "Get started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/models.html",
    "href": "get-started/models.html",
    "title": "Model choice",
    "section": "",
    "text": "Below is a table of model providers that come pre-packaged with chatlas.\n\n\n\n\n\n\nUsage pre-requisites\n\n\n\nEach model provider has its own set of pre-requisites. For example, OpenAI requires an API key, while Ollama requires you to install the Ollama CLI and download models. To see the pre-requisites for a given provider, visit the relevant usage page in the table below.\n\n\n\n\n\nName\nUsage\nEnterprise?\n\n\n\n\nAnthropic (Claude)\nChatAnthropic()\n\n\n\nGitHub model marketplace\nChatGithub()\n\n\n\nGoogle (Gemini)\nChatGoogle()\n\n\n\nGroq\nChatGroq()\n\n\n\nOllama local models\nChatOllama()\n\n\n\nOpenAI\nChatOpenAI()\n\n\n\nperplexity.ai\nChatPerplexity()\n\n\n\nAWS Bedrock\nChatBedrockAnthropic()\n‚úÖ\n\n\nAzure OpenAI\nChatAzureOpenAI()\n‚úÖ\n\n\nDatabricks\nChatDatabricks()\n‚úÖ\n\n\nSnowflake Cortex\nChatSnowflake()\n‚úÖ\n\n\nVertex AI\nChatVertex()\n‚úÖ\n\n\n\n\n\n\n\n\n\nOther providers\n\n\n\nIf you want to use a model provider that isn‚Äôt listed in the table above, you have two options:\n\nIf the model provider is OpenAI compatible (i.e., it can be used with the openai Python SDK), use ChatOpenAI() with the appropriate base_url and api_key.\nIf you‚Äôre motivated, implement a new provider by subclassing Provider and implementing the required methods.\n\n\n\n\nModel choice\nIn addition to choosing a model provider, you also need to choose a specific model from that provider. This is important because different models have different capabilities and performance characteristics. For example, some models are faster and cheaper, while others are more accurate and capable of handling more complex tasks.\nIf you‚Äôre using chatlas inside your organisation, you‚Äôll be limited to what your org allows, which is likely to be one provided by a big cloud provider (e.g.¬†ChatAzureOpenAI() and ChatBedrockAnthropic()). If you‚Äôre using chatlas for your own personal exploration, you have a lot more freedom so we have a few recommendations to help you get started:\n\nChatOpenAI() or ChatAnthropic() are both good places to start. ChatOpenAI() defaults to GPT-4o, but you can use model = \"gpt-4o-mini\" for a cheaper lower-quality model, or model = \"o1-mini\" for more complex reasoning. ChatAnthropic() is similarly good; it defaults to Claude 3.7 Sonnet which we have found to be particularly good at writing code.\nChatGoogle() is great for large prompts, because it has a much larger context window than other models. It allows up to 1 million tokens, compared to Claude 3.7 Sonnet‚Äôs 200k and GPT-4o‚Äôs 128k.\nChatOllama(), which uses Ollama, allows you to run models on your own computer. The biggest models you can run locally aren‚Äôt as good as the state of the art hosted models, but they also don‚Äôt share your data and and are effectively free.\n\n\n\nAuto complete\nIf you‚Äôre using an IDE that supports type hints, you can get autocompletion for the model parameter. This is particularly useful for getting the right model name, or simply to see what models are available.\n\n\n\nScreenshot of model autocompletion\n\n\n\n\nAuto provider\nChatAuto() is a special model provider that allows one to configure the model provider through environment variables. This is useful for having a single, simple, script that can run on any model provider, without having to change the code.",
    "crumbs": [
      "Get started",
      "Model choice"
    ]
  },
  {
    "objectID": "get-started/debug.html",
    "href": "get-started/debug.html",
    "title": "Debug",
    "section": "",
    "text": "Due to the nature of programming with LLMs, debugging can be a bit tricky. While chatlas notifies you of obvious problems like tool calling errors, it won‚Äôt notify you of things like ‚Äúthe model didn‚Äôt follow my instructions‚Äù or ‚Äúthe model is generating irrelevant responses‚Äù or ‚Äúthe model cut off in the middle of a response‚Äù.\nIn this situation, it can be helpful to inspect more information about what exactly is being sent to the model and what is being returned. This can be done both in an interactive setting, like a notebook, and in a production setting, like a web app.",
    "crumbs": [
      "Get started",
      "Debug"
    ]
  },
  {
    "objectID": "get-started/debug.html#completion-objects",
    "href": "get-started/debug.html#completion-objects",
    "title": "Debug",
    "section": "Completion objects",
    "text": "Completion objects\nIn an interactive setting, like a notebook, the .completion object on the relevant Turn can be a helpful way to inspect more information about the model‚Äôs response. This object will include provider specific information such as finish reasons, refusals, and other metadata.\nimport chatlas as ctl\nchat = ctl.ChatGoogle()\nchat.chat(\"How do I build a bomb?\")\nprint(chat.get_last_turn().completion)\n\n\nCompletion object\n\nChatCompletion(id='chatcmpl-BWRlb7BucKMKWV3W0ULSFCf4dcfRM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I don't have access to personal data about you unless you share it with me in this conversation. If you'd like to tell me your name, feel free to do so!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1747072931, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=35, prompt_tokens=21, total_tokens=56, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n\nIf the .completion object doesn‚Äôt provide helpful information, you‚Äôll want likely want to enable logging to get more information about what is being sent to the model to generate the completion.",
    "crumbs": [
      "Get started",
      "Debug"
    ]
  },
  {
    "objectID": "get-started/debug.html#logging",
    "href": "get-started/debug.html#logging",
    "title": "Debug",
    "section": "Logging",
    "text": "Logging\nSet the environment variable CHATLAS_LOG can be set to either debug or info to enable logging. The debug setting includes info level logs, as well as additional debug information (like more detailed HTTP request/response information).\nexport CHATLAS_LOG=debug\nSince chatlas delegates HTTP requests to other Python SDKs like openai, anthropic, etc., you can also work with those SDKs to enable and customize logging.",
    "crumbs": [
      "Get started",
      "Debug"
    ]
  },
  {
    "objectID": "get-started/chatbots.html",
    "href": "get-started/chatbots.html",
    "title": "Chatbots",
    "section": "",
    "text": "Here you‚Äôll learn how to build the most common type of LLM application: a chatbot. There‚Äôs a surprising amount of value in a chatbot that simply has a custom system prompt with instructions and useful knowledge (e.g., proprietary documents, data schemas, etc). Adding tools as well can lead to even more compelling user experiences like querychat and sidebot.\nChatbots are also a great use-case for chatlas since it makes multi-turn conversations trivial to implement. Various web frameworks like Shiny, Streamlit, Gradio, etc., provide a chat interface that you can combine with chatlas on the backend. You can even combine chatlas with something like textualize to create a fancy terminal-based chatbot.",
    "crumbs": [
      "Get started",
      "Chatbots"
    ]
  },
  {
    "objectID": "get-started/chatbots.html#web-based",
    "href": "get-started/chatbots.html#web-based",
    "title": "Chatbots",
    "section": "Web-based",
    "text": "Web-based\n\nShiny\nShiny is a great option for building a chatbot with chatlas. To get a basic Shiny chatbot, pass a chatlas stream to the .append_message_stream() method. Some reasons to use Shiny include:\n\nEasy to bookmark chat history\nEasy to embed inside a larger app\nEasy to theme and customize\nEasy to add features like input suggestions.\nReactivity can efficiently handle updates without caching/state management hacks\n\n\n\nShow app.py\n\nfrom chatlas import ChatAnthropic\nfrom shiny.express import ui\n\nchat_client = ChatAnthropic()\n\nchat = ui.Chat(\n    id=\"chat\",\n    messages=[\"Hello! How can I help you today?\"],\n)\nchat.ui()\n\nchat.enable_bookmarking(chat_client, bookmark_store=\"url\")\n\n@chat.on_user_submit\nasync def _(user_input: str):\n    # Async stream helps scale to many concurrent users\n    response = await chat_client.stream_async(user_input)\n    await chat.append_message_stream(response)\n\n\n\n\nScreenshot of a Shiny chatbot.\n\n\n\n\nGradio\nGradio is another option for building a chatbot with chatlas. To get a basic Gradio chatbot, pass a generator function to the gr.ChatInterface component‚Äôs fn argument, and make sure the generator yields the entire response as it is streamed.\n\n\nShow app.py\n\nimport gradio as gr\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI()\n\ndef generate(message, _):\n    res = \"\"\n    for chunk in chat.stream(message):\n        res += chunk\n        yield res\n\ngr.ChatInterface(fn=generate, type=\"messages\").launch()\n\n\n\n\nScreenshot of a gradio chatbot.\n\n\n\n\nStreamlit\nBuilding a chatbot that retains conversation history with streamlit is bit more involved since streamlit re-executes the script from top to bottom on every change.\nTo workaround this, you can use the st.session_state object to store the chat history:\n\n\nShow app.py\n\nimport streamlit as st\nfrom chatlas import ChatOpenAI, Turn\n\nwith st.sidebar:\n    openai_api_key = st.text_input(\n        \"OpenAI API Key\", key=\"chatbot_api_key\", type=\"password\"\n    )\n    \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n    \"[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)\"\n\nst.title(\"üí¨ Chatbot\")\n\nif \"turns\" not in st.session_state:\n    st.session_state[\"turns\"] = [\n        Turn(role=\"assistant\", contents=\"How can I help you?\"),\n    ]\n\nturns: list[Turn] = st.session_state.turns\n\nfor turn in turns:\n    st.chat_message(turn.role).write(turn.text)\n\n\nif prompt := st.chat_input():\n    if not openai_api_key:\n        st.info(\"Please add your OpenAI API key to continue.\")\n        st.stop()\n\n    st.chat_message(\"user\").write(prompt)\n\n    chat = ChatOpenAI(api_key=openai_api_key, turns=turns)\n    response = chat.stream(prompt)\n\n    with st.chat_message(\"assistant\"):\n        st.write_stream(response)\n\n    st.session_state[\"turns\"] = chat.get_turns()\n\n\n\n\nScreenshot of a streamlit chatbot.",
    "crumbs": [
      "Get started",
      "Chatbots"
    ]
  },
  {
    "objectID": "get-started/chatbots.html#terminal-based",
    "href": "get-started/chatbots.html#terminal-based",
    "title": "Chatbots",
    "section": "Terminal-based",
    "text": "Terminal-based\nTextualize is an excellent option for building a chatbot in the terminal.\nchatlas itself does, in a sense, already comes with a terminal-based ‚Äúchatbot‚Äù through the .chat() method. However, if you want to build a more fully featured terminal-based chatbot with things like hyperlinks, proper scrolling, etc., Textual would be a great option for doing so.\nThe code below implements a basic Textual chatbot ‚Äì the implementation derives from this blog post.\n\n\nShow app.py\n\nfrom chatlas import ChatOpenAI\nfrom textual.app import App, ComposeResult\nfrom textual.binding import Binding\nfrom textual.containers import Horizontal, ScrollableContainer\nfrom textual.widget import Widget\nfrom textual.widgets import Button, Footer, Header, Input, Markdown\n\n\nclass FocusableContainer(ScrollableContainer, can_focus=True):\n    \"\"\"Focusable container widget.\"\"\"\n\n\nclass MessageBox(Widget, can_focus=True):\n    \"\"\"Box widget for a message.\"\"\"\n\n    def __init__(self, text: str, role: str = \"assistant\") -&gt; None:\n        self.text = text\n        self.role = role\n        super().__init__()\n\n    def compose(self) -&gt; ComposeResult:\n        \"\"\"Yield message component.\"\"\"\n        yield Markdown(self.text, classes=f\"message {self.role}\")\n\n\nclass ChatApp(App):\n    \"\"\"Chat app.\"\"\"\n\n    TITLE = \"chatui\"\n    SUB_TITLE = \"A Chat interface directly in your terminal\"\n    CSS_PATH = \"static/styles.css\"\n\n    BINDINGS = [\n        Binding(\"q\", \"quit\", \"Quit\", key_display=\"Q / CTRL+C\"),\n        (\"ctrl+x\", \"clear\", \"Clear\"),\n    ]\n\n    def compose(self) -&gt; ComposeResult:\n        \"\"\"Yield components.\"\"\"\n        yield Header()\n        with FocusableContainer(id=\"conversation_box\"):\n            yield MessageBox(\"Hi! How can I help you today?\")\n        with Horizontal(id=\"input_box\"):\n            yield Input(placeholder=\"Enter your message\", id=\"message_input\")\n            yield Button(label=\"Send\", variant=\"success\", id=\"send_button\")\n        yield Footer()\n\n    def on_mount(self) -&gt; None:\n        \"\"\"Start the conversation and focus input widget.\"\"\"\n        self.chat_client = ChatOpenAI()\n        self.query_one(Input).focus()\n\n    def action_clear(self) -&gt; None:\n        \"\"\"Clear the conversation and reset widgets.\"\"\"\n        self.chat_client.set_turns([])\n        conversation_box = self.query_one(\"#conversation_box\")\n        conversation_box.remove()\n        self.mount(FocusableContainer(id=\"conversation_box\"))\n\n    async def on_button_pressed(self) -&gt; None:\n        \"\"\"Process when send was pressed.\"\"\"\n        await self.process_conversation()\n\n    async def on_input_submitted(self) -&gt; None:\n        \"\"\"Process when input was submitted.\"\"\"\n        await self.process_conversation()\n\n    async def process_conversation(self) -&gt; None:\n        \"\"\"Process a single question/answer in conversation.\"\"\"\n        message_input = self.query_one(\"#message_input\", Input)\n        # Don't do anything if input is empty\n        if message_input.value == \"\":\n            return\n        button = self.query_one(\"#send_button\")\n        conversation_box = self.query_one(\"#conversation_box\")\n\n        self.toggle_widgets(message_input, button)\n\n        # Create question message, add it to the conversation and scroll down\n        user_box = MessageBox(message_input.value, \"user\")\n        conversation_box.mount(user_box)\n        conversation_box.scroll_end(animate=False)\n\n        # Clean up the input without triggering events\n        with message_input.prevent(Input.Changed):\n            message_input.value = \"\"\n\n        # Take answer from the chat and add it to the conversation\n        response = await self.chat_client.chat_async(user_box.text)\n        content = await response.get_content()\n        conversation_box.mount(\n            MessageBox(content.removeprefix(\"\\n\").removeprefix(\"\\n\"))\n        )\n\n        self.toggle_widgets(message_input, button)\n        # For some reason single scroll doesn't work\n        conversation_box.scroll_end(animate=False)\n        conversation_box.scroll_end(animate=False)\n\n    def toggle_widgets(self, *widgets: Widget) -&gt; None:\n        \"\"\"Toggle a list of widgets.\"\"\"\n        for w in widgets:\n            w.disabled = not w.disabled\n\n\nif __name__ == \"__main__\":\n    ChatApp().run()\n\n\n\nShow static/styles.css\n\nMessageBox {\n    height: auto;\n}\n\n.message {\n    border: tall solid #343a40;\n}\n\n.assistant {\n    margin: 1 25 1 0;\n}\n\n.user {\n    margin: 1 0 1 25;\n}\n\n#input_box {\n    dock: bottom;\n    height: auto;\n    width: 100%;\n    margin: 0 0 2 0;\n    align-horizontal: center;\n    overflow-y: hidden;\n}\n\n#message_input {\n    width: 50%;\n    background: #343a40;\n}\n\n#send_button {\n    width: auto;\n}\n\n\n\n\nScreenshot of a textual chatbot.",
    "crumbs": [
      "Get started",
      "Chatbots"
    ]
  },
  {
    "objectID": "get-started/structured-data.html",
    "href": "get-started/structured-data.html",
    "title": "Structured data",
    "section": "",
    "text": "LLMs are quite good at finding structure in unstructured input like text, images, etc. Though not always perfect, this can be a very helpful way to reduce the amount of manual work needed to extract information from a large amount of text or documents. Here are just a few scenarios where this can be useful:",
    "crumbs": [
      "Get started",
      "Structured data"
    ]
  },
  {
    "objectID": "get-started/structured-data.html#basic-usage",
    "href": "get-started/structured-data.html#basic-usage",
    "title": "Structured data",
    "section": "Basic usage",
    "text": "Basic usage\nTo extract data, provide some input and a pydantic model to the .extract_data() method. The pydantic BaseModel defines the data structure to extract from the input. It should include the field names and types that you want to extract, and the LLM will do its best to fill in the values for those fields. Here‚Äôs a simple example text extraction:\nimport chatlas as ctl\nfrom pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nchat = ctl.ChatOpenAI()\nchat.extract_data(\n  \"My name is Susan and I'm 13 years old\", \n  data_model=Person,\n)\n\n{\"name\": \"Susan\", \"age\": 13}\n\nNote that input can be any type of content, including text, images, and pdfs:\nfrom chatlas import content_image_url\n\nclass Image(BaseModel):\n    primary_shape: str\n    primary_colour: str\n\nchat.extract_data(\n  content_image_url(\"https://www.r-project.org/Rlogo.png\"),\n  data_model=Image,\n)\n\n{\"primary_shape\": \"rectangle\", \"primary_colour\": \"blue\"}",
    "crumbs": [
      "Get started",
      "Structured data"
    ]
  },
  {
    "objectID": "get-started/structured-data.html#add-descriptions",
    "href": "get-started/structured-data.html#add-descriptions",
    "title": "Structured data",
    "section": "Add descriptions",
    "text": "Add descriptions\nIn addition to the model definition with field names and types, you may also want to provide the LLM with an additional context about what each field/model represents. In this case, include a Field(description=\"...\") for each field, and a docstring for each model. This is a good place to ask nicely for other attributes you‚Äôll like the value to have (e.g.¬†minimum or maximum values, date formats, ‚Ä¶). There‚Äôs no guarantee that these requests will be honoured, but the LLM will usually make a best effort to do so.\nclass Person(BaseModel):\n    \"\"\"A person\"\"\"\n\n    name: str = Field(description=\"Name\")\n\n    age: int = Field(description=\"Age, in years\")\n\n    hobbies: list[str] = Field(\n        description=\"List of hobbies. Should be exclusive and brief.\"\n    )",
    "crumbs": [
      "Get started",
      "Structured data"
    ]
  },
  {
    "objectID": "get-started/structured-data.html#advanced-data-types",
    "href": "get-started/structured-data.html#advanced-data-types",
    "title": "Structured data",
    "section": "Advanced data types",
    "text": "Advanced data types\nThis section covers some of the more advanced data types you may encounter when using .extract_data(), like data frames, required vs optional fields, and unknown keys.\n\n\n\n\n\n\nExamples\n\n\n\nBefore proceeding, consider exploring some of the examples in the structured data section, such as article summaries.\n\n\n\nData frames\nIf you want to define a data frame like data_model, you might be tempted to create a model like this, where each field is a list of scalar values:\nclass Persons(BaseModel):\n    name: list[str]\n    age: list[int]\nThis however, is not quite right because there‚Äôs no way to specify that each field should have the same length. Instead you need to turn the data structure ‚Äúinside out‚Äù, and instead create an array of objects:\nclass Person(BaseModel):\n    name: str\n    age: int\n\nclass Persons(BaseModel):\n    persons: list[Person]\nIf you‚Äôre familiar with the terms between row-oriented and column-oriented data frames, this is the same idea.\n\n\nRequired vs optional\nBy default, model fields are in a sense ‚Äúrequired‚Äù, unless None is allowed in their type definition. Including None is a good idea if there‚Äôs any possibility of the input not containing the required fields as LLMs may hallucinate data in order to fulfill your spec.\nFor example, here the LLM hallucinates a date even though there isn‚Äôt one in the text:\nclass ArticleSpec(BaseModel):\n    \"\"\"Information about an article written in markdown\"\"\"\n\n    title: str = Field(description=\"Article title\")\n    author: str = Field(description=\"Name of the author\")\n    date: str = Field(description=\"Date written in YYYY-MM-DD format.\")\n\nprompt = \"\"\"\nExtract data from the following text:\n\n&lt;text&gt;\n# Structured Data\nBy Carson Sievert\n\nWhen using an LLM to extract data from text or images, you can ask the chatbot to nicely format it, in JSON or any other format that you like.\n&lt;/text&gt;\n\"\"\"\n\nchat = ChatOpenAI()\nchat.extract_data(prompt, data_model=ArticleSpec)\n\n{\"title\": \"Structured Data\", \"author\": \"Carson Sievert\", \"date\": \"2023-10-01\"}\n\nNote that I‚Äôve used more of an explict prompt here. For this example, I found that this generated better results and that it‚Äôs a useful place to put additional instructions.\nIf I let the LLM know that the fields are all optional, it‚Äôll return None for the missing fields:\nclass ArticleSpec(BaseModel):\n    \"\"\"Information about an article written in markdown\"\"\"\n\n    title: str = Field(description=\"Article title\")\n    author: str = Field(description=\"Name of the author\")\n    date: str | None = Field(description=\"Date written in YYYY-MM-DD format.\")\n\nchat.extract_data(prompt, data_model=ArticleSpec)\n\n{\"title\": \"Structured Data\", \"author\": \"Carson Sievert\", \"date\": null}\n\n\n\nUnknown keys\nfrom chatlas import ChatAnthropic\n\n\nclass Characteristics(BaseModel, extra=\"allow\"):\n    \"\"\"All characteristics\"\"\"\n\n    pass\n\n\nprompt = \"\"\"\nGiven a description of a character, your task is to extract all the characteristics of that character.\n\n&lt;description&gt;\nThe man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket.\n&lt;/description&gt;\n\"\"\"\n\nchat = ChatAnthropic()\nchat.extract_data(prompt, data_model=Characteristics)\n\n{\n  \"height\": \"tall\",\n  \"facial_features\": [\n    \"beard\",\n    \"scar on left cheek\"\n  ],\n  \"voice\": \"deep\",\n  \"clothing\": [\n    \"black leather jacket\"\n  ]\n}\n\n\n\n\n\n\n\nUnknown key support\n\n\n\nThis example only works with Claude, not GPT or Gemini, because only Claude supports adding arbitrary additional properties.\nThat said, you could prompt an LLM to suggest a BaseModel for you from the unstructured input, and then use that to extract the data. This is a bit more work, but it can be done.",
    "crumbs": [
      "Get started",
      "Structured data"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html",
    "href": "reference/ChatGithub.html",
    "title": "ChatGithub",
    "section": "",
    "text": "ChatGithub(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://models.inference.ai.azure.com/',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on the GitHub model marketplace.\nGitHub (via Azure) hosts a wide variety of open source models, some of which are fined tuned for specific tasks.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nSign up at https://github.com/marketplace/models to get an API key. You may need to apply for and be accepted into a beta access program.\n\n\n\n\n\nimport os\nfrom chatlas import ChatGithub\n\nchat = ChatGithub(api_key=os.getenv(\"GITHUB_PAT\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GITHUB_PAT environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Github‚Äôs API.\n'https://models.inference.ai.azure.com/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nThis function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for the GitHub model marketplace.\n\n\n\nPasting an API key into a chat constructor (e.g., ChatGithub(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGITHUB_PAT=...\nfrom chatlas import ChatGithub\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGithub()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GITHUB_PAT=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#prerequisites",
    "href": "reference/ChatGithub.html#prerequisites",
    "title": "ChatGithub",
    "section": "",
    "text": "API key\n\n\n\nSign up at https://github.com/marketplace/models to get an API key. You may need to apply for and be accepted into a beta access program.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#examples",
    "href": "reference/ChatGithub.html#examples",
    "title": "ChatGithub",
    "section": "",
    "text": "import os\nfrom chatlas import ChatGithub\n\nchat = ChatGithub(api_key=os.getenv(\"GITHUB_PAT\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#parameters",
    "href": "reference/ChatGithub.html#parameters",
    "title": "ChatGithub",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GITHUB_PAT environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Github‚Äôs API.\n'https://models.inference.ai.azure.com/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#returns",
    "href": "reference/ChatGithub.html#returns",
    "title": "ChatGithub",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#note",
    "href": "reference/ChatGithub.html#note",
    "title": "ChatGithub",
    "section": "",
    "text": "This function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for the GitHub model marketplace.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#note-1",
    "href": "reference/ChatGithub.html#note-1",
    "title": "ChatGithub",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatGithub(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGITHUB_PAT=...\nfrom chatlas import ChatGithub\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGithub()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GITHUB_PAT=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html",
    "href": "reference/ChatAnthropic.html",
    "title": "ChatAnthropic",
    "section": "",
    "text": "ChatAnthropic(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    max_tokens=4096,\n    kwargs=None,\n)\nChat with an Anthropic Claude model.\nAnthropic provides a number of chat based models under the Claude moniker.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nNote that a Claude Pro membership does not give you the ability to call models via the API. You will need to go to the developer console to sign up (and pay for) a developer account that will give you an API key that you can use with this package.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatAnthropic requires the anthropic package: pip install \"chatlas[anthropic]\".\n\n\n\n\n\nimport os\nfrom chatlas import ChatAnthropic\n\nchat = ChatAnthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ModelParam]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the ANTHROPIC_API_KEY environment variable.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the anthropic.Anthropic() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.\n\n\n\n\n\n\nPasting an API key into a chat constructor (e.g., ChatAnthropic(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nANTHROPIC_API_KEY=...\nfrom chatlas import ChatAnthropic\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatAnthropic()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport ANTHROPIC_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#prerequisites",
    "href": "reference/ChatAnthropic.html#prerequisites",
    "title": "ChatAnthropic",
    "section": "",
    "text": "API key\n\n\n\nNote that a Claude Pro membership does not give you the ability to call models via the API. You will need to go to the developer console to sign up (and pay for) a developer account that will give you an API key that you can use with this package.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatAnthropic requires the anthropic package: pip install \"chatlas[anthropic]\".",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#examples",
    "href": "reference/ChatAnthropic.html#examples",
    "title": "ChatAnthropic",
    "section": "",
    "text": "import os\nfrom chatlas import ChatAnthropic\n\nchat = ChatAnthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#parameters",
    "href": "reference/ChatAnthropic.html#parameters",
    "title": "ChatAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ModelParam]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the ANTHROPIC_API_KEY environment variable.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the anthropic.Anthropic() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#returns",
    "href": "reference/ChatAnthropic.html#returns",
    "title": "ChatAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#note",
    "href": "reference/ChatAnthropic.html#note",
    "title": "ChatAnthropic",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatAnthropic(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nANTHROPIC_API_KEY=...\nfrom chatlas import ChatAnthropic\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatAnthropic()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport ANTHROPIC_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/interpolate.html",
    "href": "reference/interpolate.html",
    "title": "interpolate",
    "section": "",
    "text": "interpolate(prompt, *, variables=None, variable_start='{{', variable_end='}}')\nInterpolate variables into a prompt\nThis is a light-weight wrapper around the Jinja2 templating engine, making it easier to interpolate dynamic data into a prompt template. Compared to f-strings, which expects you to wrap dynamic values in { }, this function expects {{ }} instead, making it easier to include Python code and JSON in your prompt.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprompt\nstr\nThe prompt to interpolate (as a string).\nrequired\n\n\nvariables\nOptional[dict[str, Any]]\nA dictionary of variables to interpolate into the prompt. If not provided, the caller‚Äôs global and local variables are used.\nNone\n\n\nvariable_start\nstr\nThe string that marks the beginning of a variable.\n'{{'\n\n\nvariable_end\nstr\nThe string that marks the end of a variable.\n'}}'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe prompt with variables interpolated.\n\n\n\n\n\n\nfrom chatlas import interpolate\n\nx = 1\ninterpolate(\"The value of `x` is: {{ x }}\")",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate"
    ]
  },
  {
    "objectID": "reference/interpolate.html#parameters",
    "href": "reference/interpolate.html#parameters",
    "title": "interpolate",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprompt\nstr\nThe prompt to interpolate (as a string).\nrequired\n\n\nvariables\nOptional[dict[str, Any]]\nA dictionary of variables to interpolate into the prompt. If not provided, the caller‚Äôs global and local variables are used.\nNone\n\n\nvariable_start\nstr\nThe string that marks the beginning of a variable.\n'{{'\n\n\nvariable_end\nstr\nThe string that marks the end of a variable.\n'}}'",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate"
    ]
  },
  {
    "objectID": "reference/interpolate.html#returns",
    "href": "reference/interpolate.html#returns",
    "title": "interpolate",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nstr\nThe prompt with variables interpolated.",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate"
    ]
  },
  {
    "objectID": "reference/interpolate.html#examples",
    "href": "reference/interpolate.html#examples",
    "title": "interpolate",
    "section": "",
    "text": "from chatlas import interpolate\n\nx = 1\ninterpolate(\"The value of `x` is: {{ x }}\")",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate"
    ]
  },
  {
    "objectID": "reference/types.ContentToolRequest.html",
    "href": "reference/types.ContentToolRequest.html",
    "title": "types.ContentToolRequest",
    "section": "",
    "text": "types.ContentToolRequest()\nA request to call a tool/function\nThis content type isn‚Äôt meant to be used directly. Instead, it‚Äôs automatically generated by Chat when a tool/function is requested by the model assistant.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid\n\nA unique identifier for this request.\nrequired\n\n\nname\n\nThe name of the tool/function to call.\nrequired\n\n\narguments\n\nThe arguments to pass to the tool/function.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ntagify\nReturns an HTML string suitable for passing to htmltools/shiny‚Äôs Chat() component.\n\n\n\n\n\ntypes.ContentToolRequest.tagify()\nReturns an HTML string suitable for passing to htmltools/shiny‚Äôs Chat() component.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolRequest"
    ]
  },
  {
    "objectID": "reference/types.ContentToolRequest.html#parameters",
    "href": "reference/types.ContentToolRequest.html#parameters",
    "title": "types.ContentToolRequest",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nid\n\nA unique identifier for this request.\nrequired\n\n\nname\n\nThe name of the tool/function to call.\nrequired\n\n\narguments\n\nThe arguments to pass to the tool/function.\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolRequest"
    ]
  },
  {
    "objectID": "reference/types.ContentToolRequest.html#methods",
    "href": "reference/types.ContentToolRequest.html#methods",
    "title": "types.ContentToolRequest",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ntagify\nReturns an HTML string suitable for passing to htmltools/shiny‚Äôs Chat() component.\n\n\n\n\n\ntypes.ContentToolRequest.tagify()\nReturns an HTML string suitable for passing to htmltools/shiny‚Äôs Chat() component.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolRequest"
    ]
  },
  {
    "objectID": "reference/content_image_url.html",
    "href": "reference/content_image_url.html",
    "title": "content_image_url",
    "section": "",
    "text": "content_image_url(url, detail='auto')\nEncode image content from a URL for chat input.\nThis function is used to prepare image URLs for input to the chatbot. It can handle both regular URLs and data URLs.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#parameters",
    "href": "reference/content_image_url.html#parameters",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#returns",
    "href": "reference/content_image_url.html#returns",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#examples",
    "href": "reference/content_image_url.html#examples",
    "title": "content_image_url",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#raises",
    "href": "reference/content_image_url.html#raises",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/image_plot.html",
    "href": "reference/image_plot.html",
    "title": "content_image_plot",
    "section": "",
    "text": "content_image_plot(width=768, height=768, dpi=72)\nEncode the current matplotlib plot as an image for chat input.\nThis function captures the current matplotlib plot, resizes it to the specified dimensions, and prepares it for chat input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)"
  },
  {
    "objectID": "reference/image_plot.html#parameters",
    "href": "reference/image_plot.html#parameters",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72"
  },
  {
    "objectID": "reference/image_plot.html#returns",
    "href": "reference/image_plot.html#returns",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object."
  },
  {
    "objectID": "reference/image_plot.html#raises",
    "href": "reference/image_plot.html#raises",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer."
  },
  {
    "objectID": "reference/image_plot.html#examples",
    "href": "reference/image_plot.html#examples",
    "title": "content_image_plot",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)"
  },
  {
    "objectID": "reference/token_usage.html",
    "href": "reference/token_usage.html",
    "title": "token_usage",
    "section": "",
    "text": "token_usage()\nReport on token usage in the current session\nCall this function to find out the cumulative number of tokens that you have sent and received in the current session.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[TokenUsage] | None\nA list of dictionaries with the following keys: ‚Äúname‚Äù, ‚Äúinput‚Äù, and ‚Äúoutput‚Äù. If no tokens have been logged, then None is returned.",
    "crumbs": [
      "Reference",
      "Query token usage",
      "token_usage"
    ]
  },
  {
    "objectID": "reference/token_usage.html#returns",
    "href": "reference/token_usage.html#returns",
    "title": "token_usage",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nlist[TokenUsage] | None\nA list of dictionaries with the following keys: ‚Äúname‚Äù, ‚Äúinput‚Äù, and ‚Äúoutput‚Äù. If no tokens have been logged, then None is returned.",
    "crumbs": [
      "Reference",
      "Query token usage",
      "token_usage"
    ]
  },
  {
    "objectID": "reference/types.ContentImage.html",
    "href": "reference/types.ContentImage.html",
    "title": "types.ContentImage",
    "section": "",
    "text": "types.ContentImage\ntypes.ContentImage()\nBase class for image content.\nThis class is not meant to be used directly. Instead, use content_image_url, content_image_file, or content_image_plot.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImage"
    ]
  },
  {
    "objectID": "reference/types.ContentJson.html",
    "href": "reference/types.ContentJson.html",
    "title": "types.ContentJson",
    "section": "",
    "text": "types.ContentJson()\nJSON content\nThis content type primarily exists to signal structured data extraction (i.e., data extracted via Chat‚Äôs .extract_data() method)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\n\nThe JSON data extracted\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentJson"
    ]
  },
  {
    "objectID": "reference/types.ContentJson.html#parameters",
    "href": "reference/types.ContentJson.html#parameters",
    "title": "types.ContentJson",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nvalue\n\nThe JSON data extracted\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentJson"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Start a chat with a particular large language model (llm) provider.\n\n\n\nChatAnthropic\nChat with an Anthropic Claude model.\n\n\nChatAuto\nUse environment variables (env vars) to configure the Chat provider and model.\n\n\nChatAzureOpenAI\nChat with a model hosted on Azure OpenAI.\n\n\nChatBedrockAnthropic\nChat with an AWS bedrock model.\n\n\nChatDatabricks\nChat with a model hosted on Databricks.\n\n\nChatGithub\nChat with a model hosted on the GitHub model marketplace.\n\n\nChatGoogle\nChat with a Google Gemini model.\n\n\nChatGroq\nChat with a model hosted on Groq.\n\n\nChatOllama\nChat with a local Ollama model.\n\n\nChatOpenAI\nChat with an OpenAI model.\n\n\nChatPerplexity\nChat with a model hosted on perplexity.ai.\n\n\nChatSnowflake\nChat with a Snowflake Cortex LLM\n\n\nChatVertex\nChat with a Google Vertex AI model.\n\n\n\n\n\n\nMethods and attributes available on a chat instance\n\n\n\nChat\nA chat object that can be used to interact with a language model.\n\n\n\n\n\n\nSubmit image input to the chat\n\n\n\ncontent_image_file\nEncode image content from a file for chat input.\n\n\ncontent_image_plot\nEncode the current matplotlib plot as an image for chat input.\n\n\ncontent_image_url\nEncode image content from a URL for chat input.\n\n\n\n\n\n\nSubmit pdf input to the chat\n\n\n\ncontent_pdf_file\nPrepare a local PDF for input to a chat.\n\n\ncontent_pdf_url\nUse a remote PDF for input to a chat.\n\n\n\n\n\n\nInterpolate variables into prompt templates\n\n\n\ninterpolate\nInterpolate variables into a prompt\n\n\ninterpolate_file\nInterpolate variables into a prompt from a file\n\n\n\n\n\n\nAdd context to python function before registering it as a tool.\n\n\n\nTool\nDefine a tool\n\n\nToolRejectError\nError to represent a tool call being rejected.\n\n\n\n\n\n\nA provider-agnostic representation of content generated during an assistant/user turn.\n\n\n\nTurn\nA user or assistant turn\n\n\n\n\n\n\n\n\n\ntoken_usage\nReport on token usage in the current session\n\n\n\n\n\n\n\n\n\nProvider\nA model provider interface for a Chat.\n\n\n\n\n\n\n\n\n\ntypes.Content\nBase class for all content types that can be appear in a Turn\n\n\ntypes.ContentImage\nBase class for image content.\n\n\ntypes.ContentImageInline\nInline image content.\n\n\ntypes.ContentImageRemote\nImage content from a URL.\n\n\ntypes.ContentJson\nJSON content\n\n\ntypes.ContentText\nText content for a Turn\n\n\ntypes.ContentToolRequest\nA request to call a tool/function\n\n\ntypes.ContentToolResult\nThe result of calling a tool/function\n\n\ntypes.ChatResponse\nChat response object.\n\n\ntypes.ChatResponseAsync\nChat response (async) object.\n\n\ntypes.ImageContentTypes\nAllowable content types for images.\n\n\ntypes.MISSING_TYPE\nA singleton representing a missing value.\n\n\ntypes.MISSING\n\n\n\ntypes.SubmitInputArgsT\nA TypedDict representing the arguments that can be passed to the .chat()\n\n\ntypes.TokenUsage\nToken usage for a given provider (name).",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#chat-model-providers",
    "href": "reference/index.html#chat-model-providers",
    "title": "Function reference",
    "section": "",
    "text": "Start a chat with a particular large language model (llm) provider.\n\n\n\nChatAnthropic\nChat with an Anthropic Claude model.\n\n\nChatAuto\nUse environment variables (env vars) to configure the Chat provider and model.\n\n\nChatAzureOpenAI\nChat with a model hosted on Azure OpenAI.\n\n\nChatBedrockAnthropic\nChat with an AWS bedrock model.\n\n\nChatDatabricks\nChat with a model hosted on Databricks.\n\n\nChatGithub\nChat with a model hosted on the GitHub model marketplace.\n\n\nChatGoogle\nChat with a Google Gemini model.\n\n\nChatGroq\nChat with a model hosted on Groq.\n\n\nChatOllama\nChat with a local Ollama model.\n\n\nChatOpenAI\nChat with an OpenAI model.\n\n\nChatPerplexity\nChat with a model hosted on perplexity.ai.\n\n\nChatSnowflake\nChat with a Snowflake Cortex LLM\n\n\nChatVertex\nChat with a Google Vertex AI model.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#the-chat-object",
    "href": "reference/index.html#the-chat-object",
    "title": "Function reference",
    "section": "",
    "text": "Methods and attributes available on a chat instance\n\n\n\nChat\nA chat object that can be used to interact with a language model.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#image-input",
    "href": "reference/index.html#image-input",
    "title": "Function reference",
    "section": "",
    "text": "Submit image input to the chat\n\n\n\ncontent_image_file\nEncode image content from a file for chat input.\n\n\ncontent_image_plot\nEncode the current matplotlib plot as an image for chat input.\n\n\ncontent_image_url\nEncode image content from a URL for chat input.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#pdf-input",
    "href": "reference/index.html#pdf-input",
    "title": "Function reference",
    "section": "",
    "text": "Submit pdf input to the chat\n\n\n\ncontent_pdf_file\nPrepare a local PDF for input to a chat.\n\n\ncontent_pdf_url\nUse a remote PDF for input to a chat.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#prompt-interpolation",
    "href": "reference/index.html#prompt-interpolation",
    "title": "Function reference",
    "section": "",
    "text": "Interpolate variables into prompt templates\n\n\n\ninterpolate\nInterpolate variables into a prompt\n\n\ninterpolate_file\nInterpolate variables into a prompt from a file",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#tool-calling",
    "href": "reference/index.html#tool-calling",
    "title": "Function reference",
    "section": "",
    "text": "Add context to python function before registering it as a tool.\n\n\n\nTool\nDefine a tool\n\n\nToolRejectError\nError to represent a tool call being rejected.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#turns",
    "href": "reference/index.html#turns",
    "title": "Function reference",
    "section": "",
    "text": "A provider-agnostic representation of content generated during an assistant/user turn.\n\n\n\nTurn\nA user or assistant turn",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#query-token-usage",
    "href": "reference/index.html#query-token-usage",
    "title": "Function reference",
    "section": "",
    "text": "token_usage\nReport on token usage in the current session",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#implement-a-model-provider",
    "href": "reference/index.html#implement-a-model-provider",
    "title": "Function reference",
    "section": "",
    "text": "Provider\nA model provider interface for a Chat.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#user-facing-types",
    "href": "reference/index.html#user-facing-types",
    "title": "Function reference",
    "section": "",
    "text": "types.Content\nBase class for all content types that can be appear in a Turn\n\n\ntypes.ContentImage\nBase class for image content.\n\n\ntypes.ContentImageInline\nInline image content.\n\n\ntypes.ContentImageRemote\nImage content from a URL.\n\n\ntypes.ContentJson\nJSON content\n\n\ntypes.ContentText\nText content for a Turn\n\n\ntypes.ContentToolRequest\nA request to call a tool/function\n\n\ntypes.ContentToolResult\nThe result of calling a tool/function\n\n\ntypes.ChatResponse\nChat response object.\n\n\ntypes.ChatResponseAsync\nChat response (async) object.\n\n\ntypes.ImageContentTypes\nAllowable content types for images.\n\n\ntypes.MISSING_TYPE\nA singleton representing a missing value.\n\n\ntypes.MISSING\n\n\n\ntypes.SubmitInputArgsT\nA TypedDict representing the arguments that can be passed to the .chat()\n\n\ntypes.TokenUsage\nToken usage for a given provider (name).",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/ChatDatabricks.html",
    "href": "reference/ChatDatabricks.html",
    "title": "ChatDatabricks",
    "section": "",
    "text": "ChatDatabricks(\n    system_prompt=None,\n    model=None,\n    turns=None,\n    workspace_client=None,\n)\nChat with a model hosted on Databricks.\nDatabricks provides out-of-the-box access to a number of foundation models and can also serve as a gateway for external models hosted by a third party.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatDatabricks requires the databricks-sdk package: pip install \"chatlas[databricks]\".\n\n\n\n\n\n\n\n\nAuthentication\n\n\n\nchatlas delegates to the databricks-sdk package for authentication with Databricks. As such, you can use any of the authentication methods discussed here:\nhttps://docs.databricks.com/aws/en/dev-tools/sdk-python#authentication\nNote that Python-specific article points to this language-agnostic ‚Äúunified‚Äù approach to authentication:\nhttps://docs.databricks.com/aws/en/dev-tools/auth/unified-auth\nThere, you‚Äôll find all the options listed, but a simple approach that generally works well is to set the following environment variables:\n\nDATABRICKS_HOST: The Databricks host URL for either the Databricks workspace endpoint or the Databricks accounts endpoint.\nDATABRICKS_TOKEN: The Databricks personal access token.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nworkspace_client\nOptional['WorkspaceClient']\nA databricks.sdk.WorkspaceClient() to use for the connection. If not provided, a new client will be created.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatDatabricks"
    ]
  },
  {
    "objectID": "reference/ChatDatabricks.html#prerequisites",
    "href": "reference/ChatDatabricks.html#prerequisites",
    "title": "ChatDatabricks",
    "section": "",
    "text": "Python requirements\n\n\n\nChatDatabricks requires the databricks-sdk package: pip install \"chatlas[databricks]\".\n\n\n\n\n\n\n\n\nAuthentication\n\n\n\nchatlas delegates to the databricks-sdk package for authentication with Databricks. As such, you can use any of the authentication methods discussed here:\nhttps://docs.databricks.com/aws/en/dev-tools/sdk-python#authentication\nNote that Python-specific article points to this language-agnostic ‚Äúunified‚Äù approach to authentication:\nhttps://docs.databricks.com/aws/en/dev-tools/auth/unified-auth\nThere, you‚Äôll find all the options listed, but a simple approach that generally works well is to set the following environment variables:\n\nDATABRICKS_HOST: The Databricks host URL for either the Databricks workspace endpoint or the Databricks accounts endpoint.\nDATABRICKS_TOKEN: The Databricks personal access token.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatDatabricks"
    ]
  },
  {
    "objectID": "reference/ChatDatabricks.html#parameters",
    "href": "reference/ChatDatabricks.html#parameters",
    "title": "ChatDatabricks",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nworkspace_client\nOptional['WorkspaceClient']\nA databricks.sdk.WorkspaceClient() to use for the connection. If not provided, a new client will be created.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatDatabricks"
    ]
  },
  {
    "objectID": "reference/ChatDatabricks.html#returns",
    "href": "reference/ChatDatabricks.html#returns",
    "title": "ChatDatabricks",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatDatabricks"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html",
    "href": "reference/ChatGroq.html",
    "title": "ChatGroq",
    "section": "",
    "text": "ChatGroq(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://api.groq.com/openai/v1',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on Groq.\nGroq provides a platform for highly efficient AI inference.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nSign up at https://groq.com to get an API key.\n\n\n\n\n\nimport os\nfrom chatlas import ChatGroq\n\nchat = ChatGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GROQ_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Groq‚Äôs API.\n'https://api.groq.com/openai/v1'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nThis function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for groq.\n\n\n\nPasting an API key into a chat constructor (e.g., ChatGroq(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGROQ_API_KEY=...\nfrom chatlas import ChatGroq\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGroq()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GROQ_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#prerequisites",
    "href": "reference/ChatGroq.html#prerequisites",
    "title": "ChatGroq",
    "section": "",
    "text": "API key\n\n\n\nSign up at https://groq.com to get an API key.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#examples",
    "href": "reference/ChatGroq.html#examples",
    "title": "ChatGroq",
    "section": "",
    "text": "import os\nfrom chatlas import ChatGroq\n\nchat = ChatGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#parameters",
    "href": "reference/ChatGroq.html#parameters",
    "title": "ChatGroq",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GROQ_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Groq‚Äôs API.\n'https://api.groq.com/openai/v1'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#returns",
    "href": "reference/ChatGroq.html#returns",
    "title": "ChatGroq",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#note",
    "href": "reference/ChatGroq.html#note",
    "title": "ChatGroq",
    "section": "",
    "text": "This function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for groq.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#note-1",
    "href": "reference/ChatGroq.html#note-1",
    "title": "ChatGroq",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatGroq(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGROQ_API_KEY=...\nfrom chatlas import ChatGroq\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGroq()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GROQ_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html",
    "href": "reference/ChatBedrockAnthropic.html",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "ChatBedrockAnthropic(\n    model=None,\n    max_tokens=4096,\n    aws_secret_key=None,\n    aws_access_key=None,\n    aws_region=None,\n    aws_profile=None,\n    aws_session_token=None,\n    base_url=None,\n    system_prompt=None,\n    turns=None,\n    kwargs=None,\n)\nChat with an AWS bedrock model.\nAWS Bedrock provides a number of chat based models, including those Anthropic‚Äôs Claude.\n\n\n\n\n\n\n\n\nAWS credentials\n\n\n\nConsider using the approach outlined in this guide to manage your AWS credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatBedrockAnthropic, requires the anthropic package with the bedrock extras: pip install \"chatlas[bedrock-anthropic]\"\n\n\n\n\n\nfrom chatlas import ChatBedrockAnthropic\n\nchat = ChatBedrockAnthropic(\n    aws_profile=\"...\",\n    aws_region=\"us-east\",\n    aws_secret_key=\"...\",\n    aws_access_key=\"...\",\n    aws_session_token=\"...\",\n)\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\naws_secret_key\nOptional[str]\nThe AWS secret key to use for authentication.\nNone\n\n\naws_access_key\nOptional[str]\nThe AWS access key to use for authentication.\nNone\n\n\naws_region\nOptional[str]\nThe AWS region to use. Defaults to the AWS_REGION environment variable. If that is not set, defaults to 'us-east-1'.\nNone\n\n\naws_profile\nOptional[str]\nThe AWS profile to use.\nNone\n\n\naws_session_token\nOptional[str]\nThe AWS session token to use.\nNone\n\n\nbase_url\nOptional[str]\nThe base URL to use. Defaults to the ANTHROPIC_BEDROCK_BASE_URL environment variable. If that is not set, defaults to f\"https://bedrock-runtime.{aws_region}.amazonaws.com\".\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nkwargs\nOptional['ChatBedrockClientArgs']\nAdditional arguments to pass to the anthropic.AnthropicBedrock() client constructor.\nNone\n\n\n\n\n\n\nIf you encounter 400 or 403 errors when trying to use the model, keep the following in mind:\n\n\n\n\n\n\nIncorrect model name\n\n\n\nIf the model name is completely incorrect, you‚Äôll see an error like Error code: 400 - {'message': 'The provided model identifier is invalid.'}\nMake sure the model name is correct and active in the specified region.\n\n\n\n\n\n\n\n\nModels are region specific\n\n\n\nIf you encounter errors similar to Error code: 403 - {'message': \"You don't have access to the model with the specified model ID.\"}, make sure your model is active in the relevant aws_region.\nKeep in mind, if aws_region is not specified, and AWS_REGION is not set, the region defaults to us-east-1, which may not match to your AWS config‚Äôs default region.\n\n\n\n\n\n\n\n\nCross region inference ID\n\n\n\nIn some cases, even if you have the right model and the right region, you may still encounter an error like Error code: 400 - {'message': 'Invocation of model ID anthropic.claude-3-5-sonnet-20240620-v1:0 with on-demand throughput isn't supported. Retry your request with the ID or ARN of an inference profile that contains this model.'}\nIn this case, you‚Äôll need to look up the ‚Äòcross region inference ID‚Äô for your model. This might required opening your aws-console and navigating to the ‚ÄòAnthropic Bedrock‚Äô service page. From there, go to the ‚Äòcross region inference‚Äô tab and copy the relevant ID.\nFor example, if the desired model ID is anthropic.claude-3-5-sonnet-20240620-v1:0, the cross region ID might look something like us.anthropic.claude-3-5-sonnet-20240620-v1:0.\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#prerequisites",
    "href": "reference/ChatBedrockAnthropic.html#prerequisites",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "AWS credentials\n\n\n\nConsider using the approach outlined in this guide to manage your AWS credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatBedrockAnthropic, requires the anthropic package with the bedrock extras: pip install \"chatlas[bedrock-anthropic]\"",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#examples",
    "href": "reference/ChatBedrockAnthropic.html#examples",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "from chatlas import ChatBedrockAnthropic\n\nchat = ChatBedrockAnthropic(\n    aws_profile=\"...\",\n    aws_region=\"us-east\",\n    aws_secret_key=\"...\",\n    aws_access_key=\"...\",\n    aws_session_token=\"...\",\n)\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#parameters",
    "href": "reference/ChatBedrockAnthropic.html#parameters",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\naws_secret_key\nOptional[str]\nThe AWS secret key to use for authentication.\nNone\n\n\naws_access_key\nOptional[str]\nThe AWS access key to use for authentication.\nNone\n\n\naws_region\nOptional[str]\nThe AWS region to use. Defaults to the AWS_REGION environment variable. If that is not set, defaults to 'us-east-1'.\nNone\n\n\naws_profile\nOptional[str]\nThe AWS profile to use.\nNone\n\n\naws_session_token\nOptional[str]\nThe AWS session token to use.\nNone\n\n\nbase_url\nOptional[str]\nThe base URL to use. Defaults to the ANTHROPIC_BEDROCK_BASE_URL environment variable. If that is not set, defaults to f\"https://bedrock-runtime.{aws_region}.amazonaws.com\".\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nkwargs\nOptional['ChatBedrockClientArgs']\nAdditional arguments to pass to the anthropic.AnthropicBedrock() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#troubleshooting",
    "href": "reference/ChatBedrockAnthropic.html#troubleshooting",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "If you encounter 400 or 403 errors when trying to use the model, keep the following in mind:\n\n\n\n\n\n\nIncorrect model name\n\n\n\nIf the model name is completely incorrect, you‚Äôll see an error like Error code: 400 - {'message': 'The provided model identifier is invalid.'}\nMake sure the model name is correct and active in the specified region.\n\n\n\n\n\n\n\n\nModels are region specific\n\n\n\nIf you encounter errors similar to Error code: 403 - {'message': \"You don't have access to the model with the specified model ID.\"}, make sure your model is active in the relevant aws_region.\nKeep in mind, if aws_region is not specified, and AWS_REGION is not set, the region defaults to us-east-1, which may not match to your AWS config‚Äôs default region.\n\n\n\n\n\n\n\n\nCross region inference ID\n\n\n\nIn some cases, even if you have the right model and the right region, you may still encounter an error like Error code: 400 - {'message': 'Invocation of model ID anthropic.claude-3-5-sonnet-20240620-v1:0 with on-demand throughput isn't supported. Retry your request with the ID or ARN of an inference profile that contains this model.'}\nIn this case, you‚Äôll need to look up the ‚Äòcross region inference ID‚Äô for your model. This might required opening your aws-console and navigating to the ‚ÄòAnthropic Bedrock‚Äô service page. From there, go to the ‚Äòcross region inference‚Äô tab and copy the relevant ID.\nFor example, if the desired model ID is anthropic.claude-3-5-sonnet-20240620-v1:0, the cross region ID might look something like us.anthropic.claude-3-5-sonnet-20240620-v1:0.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#returns",
    "href": "reference/ChatBedrockAnthropic.html#returns",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAuto.html",
    "href": "reference/ChatAuto.html",
    "title": "ChatAuto",
    "section": "",
    "text": "ChatAuto(system_prompt=None, turns=None, *, provider=None, model=None, **kwargs)\nUse environment variables (env vars) to configure the Chat provider and model.\nCreates a :class:~chatlas.Chat instance based on the specified provider. The provider may be specified through the provider parameter and/or the CHATLAS_CHAT_PROVIDER env var. If both are set, the env var takes precedence. Similarly, the provider‚Äôs model may be specified through the model parameter and/or the CHATLAS_CHAT_MODEL env var. Also, additional configuration may be provided through the kwargs parameter and/or the CHATLAS_CHAT_ARGS env var (as a JSON string). In this case, when both are set, they are merged, with the env var arguments taking precedence.\nAs a result, ChatAuto() provides a convenient way to set a default provider and model in your Python code, while allowing you to override these settings through env vars (i.e., without modifying your code).\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nFollow the instructions for the specific provider to obtain an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nFollow the instructions for the specific provider to install the required Python packages.\n\n\n\n\n\nFirst, set the environment variables for the provider, arguments, and API key:\nexport CHATLAS_CHAT_PROVIDER=anthropic\nexport CHATLAS_CHAT_MODEL=claude-3-haiku-20240229\nexport CHATLAS_CHAT_ARGS='{\"kwargs\": {\"max_retries\": 3}}'\nexport ANTHROPIC_API_KEY=your_api_key\nThen, you can use the ChatAuto function to create a Chat instance:\nfrom chatlas import ChatAuto\n\nchat = ChatAuto()\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprovider\nOptional[AutoProviders]\nThe name of the default chat provider to use. Providers are strings formatted in kebab-case, e.g.¬†to use ChatBedrockAnthropic set provider=\"bedrock-anthropic\". This value can also be provided via the CHATLAS_CHAT_PROVIDER environment variable, which takes precedence over provider when set.\nNone\n\n\nmodel\nOptional[str]\nThe name of the default model to use. This value can also be provided via the CHATLAS_CHAT_MODEL environment variable, which takes precedence over model when set.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\n**kwargs\n\nAdditional keyword arguments to pass to the Chat constructor. See the documentation for each provider for more details on the available options. These arguments can also be provided via the CHATLAS_CHAT_ARGS environment variable as a JSON string. When provided, the options in the CHATLAS_CHAT_ARGS envvar take precedence over the options passed to kwargs. Note that system_prompt and turns in kwargs or in CHATLAS_CHAT_ARGS are ignored.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat instance using the specified provider.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no valid provider is specified either through parameters or environment variables.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAuto"
    ]
  },
  {
    "objectID": "reference/ChatAuto.html#prerequisites",
    "href": "reference/ChatAuto.html#prerequisites",
    "title": "ChatAuto",
    "section": "",
    "text": "API key\n\n\n\nFollow the instructions for the specific provider to obtain an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nFollow the instructions for the specific provider to install the required Python packages.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAuto"
    ]
  },
  {
    "objectID": "reference/ChatAuto.html#examples",
    "href": "reference/ChatAuto.html#examples",
    "title": "ChatAuto",
    "section": "",
    "text": "First, set the environment variables for the provider, arguments, and API key:\nexport CHATLAS_CHAT_PROVIDER=anthropic\nexport CHATLAS_CHAT_MODEL=claude-3-haiku-20240229\nexport CHATLAS_CHAT_ARGS='{\"kwargs\": {\"max_retries\": 3}}'\nexport ANTHROPIC_API_KEY=your_api_key\nThen, you can use the ChatAuto function to create a Chat instance:\nfrom chatlas import ChatAuto\n\nchat = ChatAuto()\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAuto"
    ]
  },
  {
    "objectID": "reference/ChatAuto.html#parameters",
    "href": "reference/ChatAuto.html#parameters",
    "title": "ChatAuto",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprovider\nOptional[AutoProviders]\nThe name of the default chat provider to use. Providers are strings formatted in kebab-case, e.g.¬†to use ChatBedrockAnthropic set provider=\"bedrock-anthropic\". This value can also be provided via the CHATLAS_CHAT_PROVIDER environment variable, which takes precedence over provider when set.\nNone\n\n\nmodel\nOptional[str]\nThe name of the default model to use. This value can also be provided via the CHATLAS_CHAT_MODEL environment variable, which takes precedence over model when set.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\n**kwargs\n\nAdditional keyword arguments to pass to the Chat constructor. See the documentation for each provider for more details on the available options. These arguments can also be provided via the CHATLAS_CHAT_ARGS environment variable as a JSON string. When provided, the options in the CHATLAS_CHAT_ARGS envvar take precedence over the options passed to kwargs. Note that system_prompt and turns in kwargs or in CHATLAS_CHAT_ARGS are ignored.\n{}",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAuto"
    ]
  },
  {
    "objectID": "reference/ChatAuto.html#returns",
    "href": "reference/ChatAuto.html#returns",
    "title": "ChatAuto",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat instance using the specified provider.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAuto"
    ]
  },
  {
    "objectID": "reference/ChatAuto.html#raises",
    "href": "reference/ChatAuto.html#raises",
    "title": "ChatAuto",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf no valid provider is specified either through parameters or environment variables.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAuto"
    ]
  },
  {
    "objectID": "reference/types.ImageContentTypes.html",
    "href": "reference/types.ImageContentTypes.html",
    "title": "types.ImageContentTypes",
    "section": "",
    "text": "types.ImageContentTypes\ntypes.ImageContentTypes\nAllowable content types for images.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ImageContentTypes"
    ]
  },
  {
    "objectID": "reference/types.ContentImageInline.html",
    "href": "reference/types.ContentImageInline.html",
    "title": "types.ContentImageInline",
    "section": "",
    "text": "types.ContentImageInline()\nInline image content.\nThis is the return type for content_image_file and content_image_plot. It‚Äôs not meant to be used directly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_content_type\n\nThe content type of the image.\nrequired\n\n\ndata\n\nThe base64-encoded image data.\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImageInline"
    ]
  },
  {
    "objectID": "reference/types.ContentImageInline.html#parameters",
    "href": "reference/types.ContentImageInline.html#parameters",
    "title": "types.ContentImageInline",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nimage_content_type\n\nThe content type of the image.\nrequired\n\n\ndata\n\nThe base64-encoded image data.\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImageInline"
    ]
  },
  {
    "objectID": "reference/types.MISSING.html",
    "href": "reference/types.MISSING.html",
    "title": "types.MISSING",
    "section": "",
    "text": "types.MISSING\ntypes.MISSING",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.MISSING"
    ]
  },
  {
    "objectID": "reference/ToolRejectError.html",
    "href": "reference/ToolRejectError.html",
    "title": "ToolRejectError",
    "section": "",
    "text": "ToolRejectError(reason='The user has chosen to disallow the tool call.')\nError to represent a tool call being rejected.\nThis error is meant to be raised when an end user has chosen to deny a tool call. It can be raised in a tool function or in a .on_tool_request() callback registered via a :class:~chatlas.Chat. When used in the callback, the tool call is rejected before the tool function is invoked.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreason\nstr\nA string describing the reason for rejecting the tool call. This will be included in the error message passed to the LLM. In addition to the reason, the error message will also include ‚ÄúTool call rejected.‚Äù to indicate that the tool call was not processed.\n'The user has chosen to disallow the tool call.'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nToolRejectError\nAn error with a message informing the LLM that the tool call was rejected (and the reason why).\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; import chatlas as ctl\n&gt;&gt;&gt;\n&gt;&gt;&gt; chat = ctl.ChatOpenAI()\n&gt;&gt;&gt;\n&gt;&gt;&gt; def list_files():\n...     \"List files in the user's current directory\"\n...     while True:\n...         allow = input(\n...             \"Would you like to allow access to your current directory? (yes/no): \"\n...         )\n...         if allow.lower() == \"yes\":\n...             return os.listdir(\".\")\n...         elif allow.lower() == \"no\":\n...             raise ctl.ToolRejectError(\n...                 \"The user has chosen to disallow the tool call.\"\n...             )\n...         else:\n...             print(\"Please answer with 'yes' or 'no'.\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; chat.register_tool(list_files)\n&gt;&gt;&gt; chat.chat(\"What files are available in my current directory?\")",
    "crumbs": [
      "Reference",
      "Tool calling",
      "ToolRejectError"
    ]
  },
  {
    "objectID": "reference/ToolRejectError.html#parameters",
    "href": "reference/ToolRejectError.html#parameters",
    "title": "ToolRejectError",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nreason\nstr\nA string describing the reason for rejecting the tool call. This will be included in the error message passed to the LLM. In addition to the reason, the error message will also include ‚ÄúTool call rejected.‚Äù to indicate that the tool call was not processed.\n'The user has chosen to disallow the tool call.'",
    "crumbs": [
      "Reference",
      "Tool calling",
      "ToolRejectError"
    ]
  },
  {
    "objectID": "reference/ToolRejectError.html#raises",
    "href": "reference/ToolRejectError.html#raises",
    "title": "ToolRejectError",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nToolRejectError\nAn error with a message informing the LLM that the tool call was rejected (and the reason why).",
    "crumbs": [
      "Reference",
      "Tool calling",
      "ToolRejectError"
    ]
  },
  {
    "objectID": "reference/ToolRejectError.html#examples",
    "href": "reference/ToolRejectError.html#examples",
    "title": "ToolRejectError",
    "section": "",
    "text": "&gt;&gt;&gt; import os\n&gt;&gt;&gt; import chatlas as ctl\n&gt;&gt;&gt;\n&gt;&gt;&gt; chat = ctl.ChatOpenAI()\n&gt;&gt;&gt;\n&gt;&gt;&gt; def list_files():\n...     \"List files in the user's current directory\"\n...     while True:\n...         allow = input(\n...             \"Would you like to allow access to your current directory? (yes/no): \"\n...         )\n...         if allow.lower() == \"yes\":\n...             return os.listdir(\".\")\n...         elif allow.lower() == \"no\":\n...             raise ctl.ToolRejectError(\n...                 \"The user has chosen to disallow the tool call.\"\n...             )\n...         else:\n...             print(\"Please answer with 'yes' or 'no'.\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; chat.register_tool(list_files)\n&gt;&gt;&gt; chat.chat(\"What files are available in my current directory?\")",
    "crumbs": [
      "Reference",
      "Tool calling",
      "ToolRejectError"
    ]
  },
  {
    "objectID": "reference/ChatVertex.html",
    "href": "reference/ChatVertex.html",
    "title": "ChatVertex",
    "section": "",
    "text": "ChatVertex(\n    model=None,\n    project=None,\n    location=None,\n    api_key=None,\n    system_prompt=None,\n    turns=None,\n    kwargs=None,\n)\nChat with a Google Vertex AI model.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGoogle requires the google-genai package: pip install \"chatlas[vertex]\".\n\n\n\n\n\n\n\n\nCredentials\n\n\n\nTo use Google‚Äôs models (i.e., Vertex AI), you‚Äôll need to sign up for an account with Vertex AI, then specify the appropriate model, project, and location.\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\nproject\nOptional[str]\nThe Google Cloud project ID (e.g., ‚Äúyour-project-id‚Äù). If not provided, the GOOGLE_CLOUD_PROJECT environment variable will be used.\nNone\n\n\nlocation\nOptional[str]\nThe Google Cloud location (e.g., ‚Äúus-central1‚Äù). If not provided, the GOOGLE_CLOUD_LOCATION environment variable will be used.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.\n\n\n\n\n\n\nimport os\nfrom chatlas import ChatVertex\n\nchat = ChatVertex(\n    project=\"your-project-id\",\n    location=\"us-central1\",\n)\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatVertex"
    ]
  },
  {
    "objectID": "reference/ChatVertex.html#prerequisites",
    "href": "reference/ChatVertex.html#prerequisites",
    "title": "ChatVertex",
    "section": "",
    "text": "Python requirements\n\n\n\nChatGoogle requires the google-genai package: pip install \"chatlas[vertex]\".\n\n\n\n\n\n\n\n\nCredentials\n\n\n\nTo use Google‚Äôs models (i.e., Vertex AI), you‚Äôll need to sign up for an account with Vertex AI, then specify the appropriate model, project, and location.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatVertex"
    ]
  },
  {
    "objectID": "reference/ChatVertex.html#parameters",
    "href": "reference/ChatVertex.html#parameters",
    "title": "ChatVertex",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\nproject\nOptional[str]\nThe Google Cloud project ID (e.g., ‚Äúyour-project-id‚Äù). If not provided, the GOOGLE_CLOUD_PROJECT environment variable will be used.\nNone\n\n\nlocation\nOptional[str]\nThe Google Cloud location (e.g., ‚Äúus-central1‚Äù). If not provided, the GOOGLE_CLOUD_LOCATION environment variable will be used.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatVertex"
    ]
  },
  {
    "objectID": "reference/ChatVertex.html#returns",
    "href": "reference/ChatVertex.html#returns",
    "title": "ChatVertex",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatVertex"
    ]
  },
  {
    "objectID": "reference/ChatVertex.html#examples",
    "href": "reference/ChatVertex.html#examples",
    "title": "ChatVertex",
    "section": "",
    "text": "import os\nfrom chatlas import ChatVertex\n\nchat = ChatVertex(\n    project=\"your-project-id\",\n    location=\"us-central1\",\n)\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatVertex"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html",
    "href": "reference/ChatAzureOpenAI.html",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "ChatAzureOpenAI(\n    endpoint,\n    deployment_id,\n    api_version,\n    api_key=None,\n    system_prompt=None,\n    turns=None,\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on Azure OpenAI.\nThe Azure OpenAI server hosts a number of open source models as well as proprietary models from OpenAI.\n\n\nimport os\nfrom chatlas import ChatAzureOpenAI\n\nchat = ChatAzureOpenAI(\n    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    deployment_id=\"REPLACE_WITH_YOUR_DEPLOYMENT_ID\",\n    api_version=\"YYYY-MM-DD\",\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n)\n\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nendpoint\nstr\nAzure OpenAI endpoint url with protocol and hostname, i.e.¬†https://{your-resource-name}.openai.azure.com. Defaults to using the value of the AZURE_OPENAI_ENDPOINT envinronment variable.\nrequired\n\n\ndeployment_id\nstr\nDeployment id for the model you want to use.\nrequired\n\n\napi_version\nstr\nThe API version to use.\nrequired\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the AZURE_OPENAI_API_KEY environment variable.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatAzureClientArgs']\nAdditional arguments to pass to the openai.AzureOpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html#examples",
    "href": "reference/ChatAzureOpenAI.html#examples",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "import os\nfrom chatlas import ChatAzureOpenAI\n\nchat = ChatAzureOpenAI(\n    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    deployment_id=\"REPLACE_WITH_YOUR_DEPLOYMENT_ID\",\n    api_version=\"YYYY-MM-DD\",\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n)\n\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html#parameters",
    "href": "reference/ChatAzureOpenAI.html#parameters",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nendpoint\nstr\nAzure OpenAI endpoint url with protocol and hostname, i.e.¬†https://{your-resource-name}.openai.azure.com. Defaults to using the value of the AZURE_OPENAI_ENDPOINT envinronment variable.\nrequired\n\n\ndeployment_id\nstr\nDeployment id for the model you want to use.\nrequired\n\n\napi_version\nstr\nThe API version to use.\nrequired\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the AZURE_OPENAI_API_KEY environment variable.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatAzureClientArgs']\nAdditional arguments to pass to the openai.AzureOpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html#returns",
    "href": "reference/ChatAzureOpenAI.html#returns",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/interpolate_file.html",
    "href": "reference/interpolate_file.html",
    "title": "interpolate_file",
    "section": "",
    "text": "interpolate_file(\n    path,\n    *,\n    variables=None,\n    variable_start='{{',\n    variable_end='}}',\n)\nInterpolate variables into a prompt from a file\nThis is a light-weight wrapper around the Jinja2 templating engine, making it easier to interpolate dynamic data into a static prompt. Compared to f-strings, which expects you to wrap dynamic values in { }, this function expects {{ }} instead, making it easier to include Python code and JSON in your prompt.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nUnion[str, Path]\nThe path to the file containing the prompt to interpolate.\nrequired\n\n\nvariables\nOptional[dict[str, Any]]\nA dictionary of variables to interpolate into the prompt. If not provided, the caller‚Äôs global and local variables are used.\nNone\n\n\nvariable_start\nstr\nThe string that marks the beginning of a variable.\n'{{'\n\n\nvariable_end\nstr\nThe string that marks the end of a variable.\n'}}'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe prompt with variables interpolated.\n\n\n\n\n\n\n\ninterpolate : Interpolating data into a prompt",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate_file"
    ]
  },
  {
    "objectID": "reference/interpolate_file.html#parameters",
    "href": "reference/interpolate_file.html#parameters",
    "title": "interpolate_file",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npath\nUnion[str, Path]\nThe path to the file containing the prompt to interpolate.\nrequired\n\n\nvariables\nOptional[dict[str, Any]]\nA dictionary of variables to interpolate into the prompt. If not provided, the caller‚Äôs global and local variables are used.\nNone\n\n\nvariable_start\nstr\nThe string that marks the beginning of a variable.\n'{{'\n\n\nvariable_end\nstr\nThe string that marks the end of a variable.\n'}}'",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate_file"
    ]
  },
  {
    "objectID": "reference/interpolate_file.html#returns",
    "href": "reference/interpolate_file.html#returns",
    "title": "interpolate_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nstr\nThe prompt with variables interpolated.",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate_file"
    ]
  },
  {
    "objectID": "reference/interpolate_file.html#see-also",
    "href": "reference/interpolate_file.html#see-also",
    "title": "interpolate_file",
    "section": "",
    "text": "interpolate : Interpolating data into a prompt",
    "crumbs": [
      "Reference",
      "Prompt interpolation",
      "interpolate_file"
    ]
  },
  {
    "objectID": "reference/Tool.html",
    "href": "reference/Tool.html",
    "title": "Tool",
    "section": "",
    "text": "Tool(func, name, description, parameters)\nDefine a tool\nDefine a Python function for use by a chatbot. The function will always be invoked in the current Python process.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[‚Ä¶, Any] | Callable[‚Ä¶, Awaitable[Any]]\nThe function to be invoked when the tool is called.\nrequired\n\n\nname\nstr\nThe name of the tool.\nrequired\n\n\ndescription\nstr\nA description of what the tool does.\nrequired\n\n\nparameters\ndict[str, Any]\nA dictionary describing the input parameters and their types.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_func\nCreate a Tool from a Python function\n\n\nfrom_mcp\nCreate a Tool from an MCP tool\n\n\n\n\n\nTool.from_func(func, *, model=None)\nCreate a Tool from a Python function\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[‚Ä¶, Any] | Callable[‚Ä¶, Awaitable[Any]]\nThe function to wrap as a tool.\nrequired\n\n\nmodel\nOptional[type[BaseModel]]\nA Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function‚Äôs type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTool\nA new Tool instance wrapping the provided function.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf there is a mismatch between model fields and function parameters.\n\n\n\n\n\n\n\nTool.from_mcp(session, mcp_tool)\nCreate a Tool from an MCP tool\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsession\n'MCPClientSession'\nThe MCP client session to use for calling the tool.\nrequired\n\n\nmcp_tool\n'MCPTool'\nThe MCP tool to wrap.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTool\nA new Tool instance wrapping the MCP tool.",
    "crumbs": [
      "Reference",
      "Tool calling",
      "Tool"
    ]
  },
  {
    "objectID": "reference/Tool.html#parameters",
    "href": "reference/Tool.html#parameters",
    "title": "Tool",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[‚Ä¶, Any] | Callable[‚Ä¶, Awaitable[Any]]\nThe function to be invoked when the tool is called.\nrequired\n\n\nname\nstr\nThe name of the tool.\nrequired\n\n\ndescription\nstr\nA description of what the tool does.\nrequired\n\n\nparameters\ndict[str, Any]\nA dictionary describing the input parameters and their types.\nrequired",
    "crumbs": [
      "Reference",
      "Tool calling",
      "Tool"
    ]
  },
  {
    "objectID": "reference/Tool.html#methods",
    "href": "reference/Tool.html#methods",
    "title": "Tool",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_func\nCreate a Tool from a Python function\n\n\nfrom_mcp\nCreate a Tool from an MCP tool\n\n\n\n\n\nTool.from_func(func, *, model=None)\nCreate a Tool from a Python function\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[‚Ä¶, Any] | Callable[‚Ä¶, Awaitable[Any]]\nThe function to wrap as a tool.\nrequired\n\n\nmodel\nOptional[type[BaseModel]]\nA Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function‚Äôs type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTool\nA new Tool instance wrapping the provided function.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf there is a mismatch between model fields and function parameters.\n\n\n\n\n\n\n\nTool.from_mcp(session, mcp_tool)\nCreate a Tool from an MCP tool\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsession\n'MCPClientSession'\nThe MCP client session to use for calling the tool.\nrequired\n\n\nmcp_tool\n'MCPTool'\nThe MCP tool to wrap.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTool\nA new Tool instance wrapping the MCP tool.",
    "crumbs": [
      "Reference",
      "Tool calling",
      "Tool"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html",
    "href": "reference/ChatGoogle.html",
    "title": "ChatGoogle",
    "section": "",
    "text": "ChatGoogle(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    kwargs=None,\n)\nChat with a Google Gemini model.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nTo use Google‚Äôs models (i.e., Gemini), you‚Äôll need to sign up for an account and get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGoogle requires the google-genai package: pip install \"chatlas[google]\".\n\n\n\n\n\nimport os\nfrom chatlas import ChatGoogle\n\nchat = ChatGoogle(api_key=os.getenv(\"GOOGLE_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GOOGLE_API_KEY environment variable.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the genai.Client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.\n\n\n\n\n\n\nPasting an API key into a chat constructor (e.g., ChatGoogle(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGOOGLE_API_KEY=...\nfrom chatlas import ChatGoogle\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGoogle()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GOOGLE_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#prerequisites",
    "href": "reference/ChatGoogle.html#prerequisites",
    "title": "ChatGoogle",
    "section": "",
    "text": "API key\n\n\n\nTo use Google‚Äôs models (i.e., Gemini), you‚Äôll need to sign up for an account and get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGoogle requires the google-genai package: pip install \"chatlas[google]\".",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#examples",
    "href": "reference/ChatGoogle.html#examples",
    "title": "ChatGoogle",
    "section": "",
    "text": "import os\nfrom chatlas import ChatGoogle\n\nchat = ChatGoogle(api_key=os.getenv(\"GOOGLE_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#parameters",
    "href": "reference/ChatGoogle.html#parameters",
    "title": "ChatGoogle",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GOOGLE_API_KEY environment variable.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the genai.Client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#returns",
    "href": "reference/ChatGoogle.html#returns",
    "title": "ChatGoogle",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#note",
    "href": "reference/ChatGoogle.html#note",
    "title": "ChatGoogle",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatGoogle(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGOOGLE_API_KEY=...\nfrom chatlas import ChatGoogle\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGoogle()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GOOGLE_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/types.ContentToolResult.html",
    "href": "reference/types.ContentToolResult.html",
    "title": "types.ContentToolResult",
    "section": "",
    "text": "types.ContentToolResult()\nThe result of calling a tool/function\nA content type representing the result of a tool function call. When a model requests a tool function, Chat will create, (optionally) echo, (optionally) yield, and store this content type in the chat history.\nA tool function may also construct an instance of this class and return it. This is useful for a tool that wishes to customize how the result is handled (e.g., the format of the value sent to the model).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\n\nThe return value of the tool/function.\nrequired\n\n\nmodel_format\n\nThe format used for sending the value to the model. The default, \"auto\", first attempts to format the value as a JSON string. If that fails, it gets converted to a string via str(). To force orjson.dumps() or str(), set to \"json\" or \"str\". Finally, \"as_is\" is useful for doing your own formatting and/or passing a non-string value (e.g., a list or dict) straight to the model. Non-string values are useful for tools that return images or other ‚Äòknown‚Äô non-text content types.\nrequired\n\n\nerror\n\nAn exception that occurred while invoking the tool. If this is set, the error message sent to the model and the value is ignored.\nrequired\n\n\nextra\n\nAdditional data associated with the tool result that isn‚Äôt sent to the model.\nrequired\n\n\nrequest\n\nNot intended to be used directly. It will be set when the :class:~chatlas.Chat invokes the tool.\nrequired\n\n\n\n\n\n\nWhen model_format is \"json\" (or \"auto\"), and the value has a .to_json()/.to_dict() method, those methods are called to obtain the JSON representation of the value. This is convenient for classes, like pandas.DataFrame, that have a .to_json() method, but don‚Äôt necessarily dump to JSON directly. If this happens to not be the desired behavior, set model_format=\"as_is\" return the desired value as-is.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_model_value\nGet the actual value sent to the model.\n\n\ntagify\nA method for rendering this object via htmltools/shiny.\n\n\n\n\n\ntypes.ContentToolResult.get_model_value()\nGet the actual value sent to the model.\n\n\n\ntypes.ContentToolResult.tagify()\nA method for rendering this object via htmltools/shiny.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolResult"
    ]
  },
  {
    "objectID": "reference/types.ContentToolResult.html#parameters",
    "href": "reference/types.ContentToolResult.html#parameters",
    "title": "types.ContentToolResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nvalue\n\nThe return value of the tool/function.\nrequired\n\n\nmodel_format\n\nThe format used for sending the value to the model. The default, \"auto\", first attempts to format the value as a JSON string. If that fails, it gets converted to a string via str(). To force orjson.dumps() or str(), set to \"json\" or \"str\". Finally, \"as_is\" is useful for doing your own formatting and/or passing a non-string value (e.g., a list or dict) straight to the model. Non-string values are useful for tools that return images or other ‚Äòknown‚Äô non-text content types.\nrequired\n\n\nerror\n\nAn exception that occurred while invoking the tool. If this is set, the error message sent to the model and the value is ignored.\nrequired\n\n\nextra\n\nAdditional data associated with the tool result that isn‚Äôt sent to the model.\nrequired\n\n\nrequest\n\nNot intended to be used directly. It will be set when the :class:~chatlas.Chat invokes the tool.\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolResult"
    ]
  },
  {
    "objectID": "reference/types.ContentToolResult.html#note",
    "href": "reference/types.ContentToolResult.html#note",
    "title": "types.ContentToolResult",
    "section": "",
    "text": "When model_format is \"json\" (or \"auto\"), and the value has a .to_json()/.to_dict() method, those methods are called to obtain the JSON representation of the value. This is convenient for classes, like pandas.DataFrame, that have a .to_json() method, but don‚Äôt necessarily dump to JSON directly. If this happens to not be the desired behavior, set model_format=\"as_is\" return the desired value as-is.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolResult"
    ]
  },
  {
    "objectID": "reference/types.ContentToolResult.html#methods",
    "href": "reference/types.ContentToolResult.html#methods",
    "title": "types.ContentToolResult",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_model_value\nGet the actual value sent to the model.\n\n\ntagify\nA method for rendering this object via htmltools/shiny.\n\n\n\n\n\ntypes.ContentToolResult.get_model_value()\nGet the actual value sent to the model.\n\n\n\ntypes.ContentToolResult.tagify()\nA method for rendering this object via htmltools/shiny.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolResult"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html",
    "href": "reference/ChatOpenAI.html",
    "title": "ChatOpenAI",
    "section": "",
    "text": "ChatOpenAI(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://api.openai.com/v1',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with an OpenAI model.\nOpenAI provides a number of chat based models under the ChatGPT moniker.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nNote that a ChatGPT Plus membership does not give you the ability to call models via the API. You will need to go to the developer platform to sign up (and pay for) a developer account that will give you an API key that you can use with this package.\n\n\n\n\n\nimport os\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ChatModel | str]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the OPENAI_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses OpenAI.\n'https://api.openai.com/v1'\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nPasting an API key into a chat constructor (e.g., ChatOpenAI(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nOPENAI_API_KEY=...\nfrom chatlas import ChatOpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatOpenAI()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport OPENAI_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#prerequisites",
    "href": "reference/ChatOpenAI.html#prerequisites",
    "title": "ChatOpenAI",
    "section": "",
    "text": "API key\n\n\n\nNote that a ChatGPT Plus membership does not give you the ability to call models via the API. You will need to go to the developer platform to sign up (and pay for) a developer account that will give you an API key that you can use with this package.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#examples",
    "href": "reference/ChatOpenAI.html#examples",
    "title": "ChatOpenAI",
    "section": "",
    "text": "import os\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#parameters",
    "href": "reference/ChatOpenAI.html#parameters",
    "title": "ChatOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ChatModel | str]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the OPENAI_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses OpenAI.\n'https://api.openai.com/v1'\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#returns",
    "href": "reference/ChatOpenAI.html#returns",
    "title": "ChatOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#note",
    "href": "reference/ChatOpenAI.html#note",
    "title": "ChatOpenAI",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatOpenAI(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nOPENAI_API_KEY=...\nfrom chatlas import ChatOpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatOpenAI()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport OPENAI_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/types.SubmitInputArgsT.html",
    "href": "reference/types.SubmitInputArgsT.html",
    "title": "types.SubmitInputArgsT",
    "section": "",
    "text": "types.SubmitInputArgsT\ntypes.SubmitInputArgsT\nA TypedDict representing the arguments that can be passed to the .chat() method of a Chat instance.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.SubmitInputArgsT"
    ]
  },
  {
    "objectID": "reference/content_image_file.html",
    "href": "reference/content_image_file.html",
    "title": "content_image_file",
    "section": "",
    "text": "content_image_file(path, content_type='auto', resize=MISSING)\nEncode image content from a file for chat input.\nThis function is used to prepare image files for input to the chatbot. It can handle various image formats and provides options for resizing.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[Literal['low', 'high', 'none'], str, MISSING_TYPE]\nResizing option for the image. Can be: - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - \"none\": No resizing - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\nMISSING\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#parameters",
    "href": "reference/content_image_file.html#parameters",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[Literal['low', 'high', 'none'], str, MISSING_TYPE]\nResizing option for the image. Can be: - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - \"none\": No resizing - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\nMISSING",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#returns",
    "href": "reference/content_image_file.html#returns",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#examples",
    "href": "reference/content_image_file.html#examples",
    "title": "content_image_file",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#raises",
    "href": "reference/content_image_file.html#raises",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_pdf_file.html",
    "href": "reference/content_pdf_file.html",
    "title": "content_pdf_file",
    "section": "",
    "text": "content_pdf_file(path)\nPrepare a local PDF for input to a chat.\nNot all providers support PDF input, so check the documentation for the provider you are using.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nUnion[str, os.PathLike]\nA path to a local PDF file.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "PDF input",
      "content_pdf_file"
    ]
  },
  {
    "objectID": "reference/content_pdf_file.html#parameters",
    "href": "reference/content_pdf_file.html#parameters",
    "title": "content_pdf_file",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npath\nUnion[str, os.PathLike]\nA path to a local PDF file.\nrequired",
    "crumbs": [
      "Reference",
      "PDF input",
      "content_pdf_file"
    ]
  },
  {
    "objectID": "reference/content_pdf_file.html#returns",
    "href": "reference/content_pdf_file.html#returns",
    "title": "content_pdf_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "PDF input",
      "content_pdf_file"
    ]
  },
  {
    "objectID": "reference/Chat.html",
    "href": "reference/Chat.html",
    "title": "Chat",
    "section": "",
    "text": "Chat(provider, turns=None)\nA chat object that can be used to interact with a language model.\nA Chat is an sequence of sequence of user and assistant Turns sent to a specific Provider. A Chat takes care of managing the state associated with the chat; i.e.¬†it records the messages that you send to the server, and the messages that you receive back. If you register a tool (i.e.¬†an function that the assistant can call on your behalf), it also takes care of the tool loop.\nYou should generally not create this object yourself, but instead call ChatOpenAI or friends instead.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncurrent_display\nGet the currently active markdown display, if any.\n\n\nsystem_prompt\nA property to get (or set) the system prompt for the chat.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\napp\nEnter a web-based chat app to interact with the LLM.\n\n\nchat\nGenerate a response from the chat.\n\n\nchat_async\nGenerate a response from the chat asynchronously.\n\n\ncleanup_mcp_tools\nClose MCP server connections (and their corresponding tools).\n\n\nconsole\nEnter a chat console to interact with the LLM.\n\n\nexport\nExport the chat history to a file.\n\n\nextract_data\nExtract structured data from the given input.\n\n\nextract_data_async\nExtract structured data from the given input asynchronously.\n\n\nget_last_turn\nGet the last turn in the chat with a specific role.\n\n\nget_tools\nGet the list of registered tools.\n\n\nget_turns\nGet all the turns (i.e., message contents) in the chat.\n\n\non_tool_request\nRegister a callback for a tool request event.\n\n\non_tool_result\nRegister a callback for a tool result event.\n\n\nregister_mcp_tools_http_stream_async\nRegister tools from an MCP server using streamable HTTP transport.\n\n\nregister_mcp_tools_stdio_async\nRegister tools from a MCP server using stdio (standard input/output) transport.\n\n\nregister_tool\nRegister a tool (function) with the chat.\n\n\nset_echo_options\nSet echo styling options for the chat.\n\n\nset_tools\nSet the tools for the chat.\n\n\nset_turns\nSet the turns of the chat.\n\n\nstream\nGenerate a response from the chat in a streaming fashion.\n\n\nstream_async\nGenerate a response from the chat in a streaming fashion asynchronously.\n\n\ntoken_count\nGet an estimated token count for the given input.\n\n\ntoken_count_async\nGet an estimated token count for the given input asynchronously.\n\n\ntokens\nGet the tokens for each turn in the chat.\n\n\n\n\n\nChat.app(\n    stream=True,\n    port=0,\n    launch_browser=True,\n    bg_thread=None,\n    echo=None,\n    content='all',\n    kwargs=None,\n)\nEnter a web-based chat app to interact with the LLM.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nport\nint\nThe port to run the app on (the default is 0, which will choose a random port).\n0\n\n\nlaunch_browser\nbool\nWhether to launch a browser window.\nTrue\n\n\nbg_thread\nOptional[bool]\nWhether to run the app in a background thread. If None, the app will run in a background thread if the current environment is a notebook.\nNone\n\n\necho\nOptional[EchoOptions]\nOne of the following (defaults to \"none\" when stream=True and \"text\" when stream=False): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\nNone\n\n\ncontent\nLiteral['text', 'all']\nWhether to display text content or all content (i.e., tool calls).\n'all'\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\nChat.chat(*args, echo='output', stream=True, kwargs=None)\nGenerate a response from the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúoutput‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'output'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponse\nA (consumed) response from the chat. Apply str() to this object to get the text content of the response.\n\n\n\n\n\n\n\nChat.chat_async(*args, echo='output', stream=True, kwargs=None)\nGenerate a response from the chat asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúoutput‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'output'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponseAsync\nA (consumed) response from the chat. Apply str() to this object to get the text content of the response.\n\n\n\n\n\n\n\nChat.cleanup_mcp_tools(names=None)\nClose MCP server connections (and their corresponding tools).\nThis method closes the MCP client sessions and removes the tools registered from the MCP servers. If a specific name is provided, it will only clean up the tools and session associated with that name. If no name is provided, it will clean up all registered MCP tools and sessions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnames\nOptional[Sequence[str]]\nIf provided, only clean up the tools and session associated with these names. If not provided, clean up all registered MCP tools and sessions.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nChat.console(echo='output', stream=True, kwargs=None)\nEnter a chat console to interact with the LLM.\nTo quit, input ‚Äòexit‚Äô or press Ctrl+C.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúoutput‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'output'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nChat.export(\n    filename,\n    *,\n    turns=None,\n    title=None,\n    content='text',\n    include_system_prompt=True,\n    overwrite=False,\n)\nExport the chat history to a file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nThe filename to export the chat to. Currently this must be a .md or .html file.\nrequired\n\n\nturns\nOptional[Sequence[Turn]]\nThe .get_turns() to export. If not provided, the chat‚Äôs current turns will be used.\nNone\n\n\ntitle\nOptional[str]\nA title to place at the top of the exported file.\nNone\n\n\noverwrite\nbool\nWhether to overwrite the file if it already exists.\nFalse\n\n\ncontent\nLiteral['text', 'all']\nWhether to include text content, all content (i.e., tool calls), or no content.\n'text'\n\n\ninclude_system_prompt\nbool\nWhether to include the system prompt in a \nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nPath\nThe path to the exported file.\n\n\n\n\n\n\n\nChat.extract_data(*args, data_model, echo='none', stream=False)\nExtract structured data from the given input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.extract_data_async(*args, data_model, echo='none', stream=False)\nExtract structured data from the given input asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks). Defaults to True if echo is not ‚Äúnone‚Äù.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.get_last_turn(role='assistant')\nGet the last turn in the chat with a specific role.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrole\nLiteral['assistant', 'user', 'system']\nThe role of the turn to return.\n'assistant'\n\n\n\n\n\n\n\nChat.get_tools()\nGet the list of registered tools.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[Tool]\nA list of Tool instances that are currently registered with the chat.\n\n\n\n\n\n\n\nChat.get_turns(include_system_prompt=False)\nGet all the turns (i.e., message contents) in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude_system_prompt\nbool\nWhether to include the system prompt in the turns.\nFalse\n\n\n\n\n\n\n\nChat.on_tool_request(callback)\nRegister a callback for a tool request event.\nA tool request event occurs when the assistant requests a tool to be called on its behalf. Before invoking the tool, on_tool_request handlers are called with the relevant ContentToolRequest object. This is useful if you want to handle tool requests in a custom way, such as requiring logging them or requiring user approval before invoking the tool\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncallback\nCallable[[ContentToolRequest], None]\nA function to be called when a tool request event occurs. This function must have a single argument, which will be the tool request (i.e., a ContentToolRequest object).\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA callable that can be used to remove the callback later.\n\n\n\n\n\n\n\n\nChat.on_tool_result(callback)\nRegister a callback for a tool result event.\nA tool result event occurs when a tool has been invoked and the result is ready to be provided to the assistant. After the tool has been invoked, on_tool_result handlers are called with the relevant ContentToolResult object. This is useful if you want to handle tool results in a custom way such as logging them.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncallback\nCallable[[ContentToolResult], None]\nA function to be called when a tool result event occurs. This function must have a single argument, which will be the tool result (i.e., a ContentToolResult object).\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA callable that can be used to remove the callback later.\n\n\n\n\n\n\n\n\nChat.register_mcp_tools_http_stream_async(\n    name,\n    url,\n    include_tools=(),\n    exclude_tools=(),\n    namespace=None,\n    transport_kwargs=None,\n)\nRegister tools from an MCP server using streamable HTTP transport.\nConnects to an MCP server (that communicates over a streamable HTTP transport) and registers the available tools. This is useful for utilizing tools provided by an MCP server running on a remote server (or locally) over HTTP.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRequires the mcp package to be installed. Install it with:\npip install mcp\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nA unique name for the MCP server session.\nrequired\n\n\nurl\nstr\nURL endpoint where the Streamable HTTP server is mounted (e.g., http://localhost:8000/mcp)\nrequired\n\n\ninclude_tools\nSequence[str]\nList of tool names to include. By default, all available tools are included.\n()\n\n\nexclude_tools\nSequence[str]\nList of tool names to exclude. This parameter and include_tools are mutually exclusive.\n()\n\n\nnamespace\nOptional[str]\nA namespace to prepend to tool names (i.e., namespace.tool_name) from this MCP server. This is primarily useful to avoid name collisions with other tools already registered with the chat. This namespace applies when tools are advertised to the LLM, so try to use a meaningful name that describes the server and/or the tools it provides. For example, if you have a server that provides tools for mathematical operations, you might use math as the namespace.\nNone\n\n\ntransport_kwargs\nOptional[dict[str, Any]]\nAdditional keyword arguments for the transport layer (i.e., mcp.client.streamable_http.streamablehttp_client).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n.cleanup_mcp_tools_async() : Cleanup registered MCP tools.\n.register_mcp_tools_stdio_async() : Register tools from an MCP server using stdio transport.\n\n\n\n\nUnlike the .register_mcp_tools_stdio_async() method, this method does not launch an MCP server. Instead, it assumes an HTTP server is already running at the specified URL. This is useful for connecting to an existing MCP server that is already running and serving tools.\n\n\n\nAssuming you have a Python script my_mcp_server.py that implements an MCP server like so:\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP(\"my_server\")\n\n@app.tool(description=\"Add two numbers.\")\ndef add(x: int, y: int) -&gt; int:\n    return x + y\n\napp.run(transport=\"streamable-http\")\nYou can launch this server like so:\npython my_mcp_server.py\nThen, you can register this server with the chat as follows:\nawait chat.register_mcp_tools_http_stream_async(\n    name=\"my_server\",\n    url=\"http://localhost:8080/mcp\"\n)\n\n\n\n\nChat.register_mcp_tools_stdio_async(\n    name,\n    command,\n    args,\n    include_tools=(),\n    exclude_tools=(),\n    namespace=None,\n    transport_kwargs=None,\n)\nRegister tools from a MCP server using stdio (standard input/output) transport.\nUseful for launching an MCP server and registering its tools with the chat ‚Äì all from the same Python process.\nIn more detail, this method:\n\nExecutes the given command with the provided args.\n\nThis should start an MCP server that communicates via stdio.\n\nEstablishes a client connection to the MCP server using the mcp package.\nRegisters the available tools from the MCP server with the chat.\nReturns a cleanup callback to close the MCP session and remove the tools.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRequires the mcp package to be installed. Install it with:\npip install mcp\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nA unique name for the MCP server session.\nrequired\n\n\ncommand\nstr\nSystem command to execute to start the MCP server (e.g., python).\nrequired\n\n\nargs\nlist[str]\nArguments to pass to the system command (e.g., [\"-m\", \"my_mcp_server\"]).\nrequired\n\n\ninclude_tools\nSequence[str]\nList of tool names to include. By default, all available tools are included.\n()\n\n\nexclude_tools\nSequence[str]\nList of tool names to exclude. This parameter and include_tools are mutually exclusive.\n()\n\n\nnamespace\nOptional[str]\nA namespace to prepend to tool names (i.e., namespace.tool_name) from this MCP server. This is primarily useful to avoid name collisions with other tools already registered with the chat. This namespace applies when tools are advertised to the LLM, so try to use a meaningful name that describes the server and/or the tools it provides. For example, if you have a server that provides tools for mathematical operations, you might use math as the namespace.\nNone\n\n\ntransport_kwargs\nOptional[dict[str, Any]]\nAdditional keyword arguments for the stdio transport layer (i.e., mcp.client.stdio.stdio_client).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n.cleanup_mcp_tools_async() : Cleanup registered MCP tools.\n.register_mcp_tools_http_stream_async() : Register tools from an MCP server using streamable HTTP transport.\n\n\n\n\nAssuming you have a Python script my_mcp_server.py that implements an MCP server like so\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP(\"my_server\")\n\n@app.tool(description=\"Add two numbers.\")\ndef add(y: int, z: int) -&gt; int:\n    return y - z\n\napp.run(transport=\"stdio\")\nYou can register this server with the chat as follows:\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI()\n\nawait chat.register_mcp_tools_stdio_async(\n    name=\"my_server\",\n    command=\"python\",\n    args=[\"-m\", \"my_mcp_server\"],\n)\n\n\n\n\nChat.register_tool(func, *, force=False, model=None)\nRegister a tool (function) with the chat.\nThe function will always be invoked in the current Python process.\n\n\nIf your tool has straightforward input parameters, you can just register the function directly (type hints and a docstring explaning both what the function does and what the parameters are for is strongly recommended):\nfrom chatlas import ChatOpenAI\n\n\ndef add(a: int, b: int) -&gt; int:\n    '''\n    Add two numbers together.\n\n####     Parameters {.doc-section .doc-section-----parameters}\n\n    a : int\n        The first number to add.\n    b : int\n        The second number to add.\n    '''\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add)\nchat.chat(\"What is 2 + 2?\")\nIf your tool has more complex input parameters, you can provide a Pydantic model that corresponds to the input parameters for the function, This way, you can have fields that hold other model(s) (for more complex input parameters), and also more directly document the input parameters:\nfrom chatlas import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\n\nclass AddParams(BaseModel):\n    '''Add two numbers together.'''\n\n    a: int = Field(description=\"The first number to add.\")\n\n    b: int = Field(description=\"The second number to add.\")\n\n\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add, model=AddParams)\nchat.chat(\"What is 2 + 2?\")\n\n\n\n\n\nfunc The function to be invoked when the tool is called. force If True, overwrite any existing tool with the same name. If False (the default), raise an error if a tool with the same name already exists. model A Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function‚Äôs type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.\n\n\n\nValueError If a tool with the same name already exists and force is False.\n\n\nChat.set_echo_options(rich_markdown=None, rich_console=None, css_styles=None)\nSet echo styling options for the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrich_markdown\nOptional[dict[str, Any]]\nA dictionary of options to pass to rich.markdown.Markdown(). This is only relevant when outputting to the console.\nNone\n\n\nrich_console\nOptional[dict[str, Any]]\nA dictionary of options to pass to rich.console.Console(). This is only relevant when outputting to the console.\nNone\n\n\ncss_styles\nOptional[dict[str, str]]\nA dictionary of CSS styles to apply to IPython.display.Markdown(). This is only relevant when outputing to the browser.\nNone\n\n\n\n\n\n\n\nChat.set_tools(tools)\nSet the tools for the chat.\nThis replaces any previously registered tools with the provided list of tools. This is for advanced usage ‚Äì typically, you would use .register_tool() to register individual tools as needed.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntools\nlist[Callable[‚Ä¶, Any] | Callable[‚Ä¶, Awaitable[Any]] | Tool]\nA list of Tool instances to set as the chat‚Äôs tools.\nrequired\n\n\n\n\n\n\n\nChat.set_turns(turns)\nSet the turns of the chat.\nThis method is primarily useful for clearing or setting the turns of the chat (i.e., limiting the context window).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nturns\nSequence[Turn]\nThe turns to set. Turns with the role ‚Äúsystem‚Äù are not allowed.\nrequired\n\n\n\n\n\n\n\nChat.stream(*args, content='text', echo='none', kwargs=None)\nGenerate a response from the chat in a streaming fashion.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\ncontent\nLiteral['text', 'all']\nWhether to yield just text content or include rich content objects (e.g., tool calls) when relevant.\n'text'\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponse\nAn (unconsumed) response from the chat. Iterate over this object to consume the response.\n\n\n\n\n\n\n\nChat.stream_async(*args, content='text', echo='none', kwargs=None)\nGenerate a response from the chat in a streaming fashion asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\ncontent\nLiteral['text', 'all']\nWhether to yield just text content or include rich content objects (e.g., tool calls) when relevant.\n'text'\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponseAsync\nAn (unconsumed) response from the chat. Iterate over this object to consume the response.\n\n\n\n\n\n\n\nChat.token_count(*args, data_model=None)\nGet an estimated token count for the given input.\nEstimate the token size of input content. This can help determine whether input(s) and/or conversation history (i.e., .get_turns()) should be reduced in size before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to get a token count for.\n()\n\n\ndata_model\nOptional[type[BaseModel]]\nIf the input is meant for data extraction (i.e., .extract_data()), then this should be the Pydantic model that describes the structure of the data to extract.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe token count for the input.\n\n\n\n\n\n\nRemember that the token count is an estimate. Also, models based on ChatOpenAI() currently does not take tools into account when estimating token counts.\n\n\n\nfrom chatlas import ChatAnthropic\n\nchat = ChatAnthropic()\n# Estimate the token count before sending the input\nprint(chat.token_count(\"What is 2 + 2?\"))\n\n# Once input is sent, you can get the actual input and output\n# token counts from the chat object\nchat.chat(\"What is 2 + 2?\", echo=\"none\")\nprint(chat.token_usage())\n\n\n\n\nChat.token_count_async(*args, data_model=None)\nGet an estimated token count for the given input asynchronously.\nEstimate the token size of input content. This can help determine whether input(s) and/or conversation history (i.e., .get_turns()) should be reduced in size before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to get a token count for.\n()\n\n\ndata_model\nOptional[type[BaseModel]]\nIf this input is meant for data extraction (i.e., .extract_data_async()), then this should be the Pydantic model that describes the structure of the data to extract.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe token count for the input.\n\n\n\n\n\n\n\nChat.tokens(values='discrete')\nGet the tokens for each turn in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nLiteral['cumulative', 'discrete']\nIf ‚Äúcumulative‚Äù (the default), the result can be summed to get the chat‚Äôs overall token usage (helpful for computing overall cost of the chat). If ‚Äúdiscrete‚Äù, the result can be summed to get the number of tokens the turns will cost to generate the next response (helpful for estimating cost of the next response, or for determining if you are about to exceed the token limit).\n'discrete'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[int]\nA list of token counts for each (non-system) turn in the chat. The 1st turn includes the tokens count for the system prompt (if any).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the chat‚Äôs turns (i.e., .get_turns()) are not in an expected format. This may happen if the chat history is manually set (i.e., .set_turns()). In this case, you can inspect the ‚Äúraw‚Äù token values via the .get_turns() method (each turn has a .tokens attribute).",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/Chat.html#attributes",
    "href": "reference/Chat.html#attributes",
    "title": "Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncurrent_display\nGet the currently active markdown display, if any.\n\n\nsystem_prompt\nA property to get (or set) the system prompt for the chat.",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/Chat.html#methods",
    "href": "reference/Chat.html#methods",
    "title": "Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\napp\nEnter a web-based chat app to interact with the LLM.\n\n\nchat\nGenerate a response from the chat.\n\n\nchat_async\nGenerate a response from the chat asynchronously.\n\n\ncleanup_mcp_tools\nClose MCP server connections (and their corresponding tools).\n\n\nconsole\nEnter a chat console to interact with the LLM.\n\n\nexport\nExport the chat history to a file.\n\n\nextract_data\nExtract structured data from the given input.\n\n\nextract_data_async\nExtract structured data from the given input asynchronously.\n\n\nget_last_turn\nGet the last turn in the chat with a specific role.\n\n\nget_tools\nGet the list of registered tools.\n\n\nget_turns\nGet all the turns (i.e., message contents) in the chat.\n\n\non_tool_request\nRegister a callback for a tool request event.\n\n\non_tool_result\nRegister a callback for a tool result event.\n\n\nregister_mcp_tools_http_stream_async\nRegister tools from an MCP server using streamable HTTP transport.\n\n\nregister_mcp_tools_stdio_async\nRegister tools from a MCP server using stdio (standard input/output) transport.\n\n\nregister_tool\nRegister a tool (function) with the chat.\n\n\nset_echo_options\nSet echo styling options for the chat.\n\n\nset_tools\nSet the tools for the chat.\n\n\nset_turns\nSet the turns of the chat.\n\n\nstream\nGenerate a response from the chat in a streaming fashion.\n\n\nstream_async\nGenerate a response from the chat in a streaming fashion asynchronously.\n\n\ntoken_count\nGet an estimated token count for the given input.\n\n\ntoken_count_async\nGet an estimated token count for the given input asynchronously.\n\n\ntokens\nGet the tokens for each turn in the chat.\n\n\n\n\n\nChat.app(\n    stream=True,\n    port=0,\n    launch_browser=True,\n    bg_thread=None,\n    echo=None,\n    content='all',\n    kwargs=None,\n)\nEnter a web-based chat app to interact with the LLM.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nport\nint\nThe port to run the app on (the default is 0, which will choose a random port).\n0\n\n\nlaunch_browser\nbool\nWhether to launch a browser window.\nTrue\n\n\nbg_thread\nOptional[bool]\nWhether to run the app in a background thread. If None, the app will run in a background thread if the current environment is a notebook.\nNone\n\n\necho\nOptional[EchoOptions]\nOne of the following (defaults to \"none\" when stream=True and \"text\" when stream=False): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\nNone\n\n\ncontent\nLiteral['text', 'all']\nWhether to display text content or all content (i.e., tool calls).\n'all'\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\nChat.chat(*args, echo='output', stream=True, kwargs=None)\nGenerate a response from the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúoutput‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'output'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponse\nA (consumed) response from the chat. Apply str() to this object to get the text content of the response.\n\n\n\n\n\n\n\nChat.chat_async(*args, echo='output', stream=True, kwargs=None)\nGenerate a response from the chat asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúoutput‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'output'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponseAsync\nA (consumed) response from the chat. Apply str() to this object to get the text content of the response.\n\n\n\n\n\n\n\nChat.cleanup_mcp_tools(names=None)\nClose MCP server connections (and their corresponding tools).\nThis method closes the MCP client sessions and removes the tools registered from the MCP servers. If a specific name is provided, it will only clean up the tools and session associated with that name. If no name is provided, it will clean up all registered MCP tools and sessions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnames\nOptional[Sequence[str]]\nIf provided, only clean up the tools and session associated with these names. If not provided, clean up all registered MCP tools and sessions.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nChat.console(echo='output', stream=True, kwargs=None)\nEnter a chat console to interact with the LLM.\nTo quit, input ‚Äòexit‚Äô or press Ctrl+C.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúoutput‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'output'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nChat.export(\n    filename,\n    *,\n    turns=None,\n    title=None,\n    content='text',\n    include_system_prompt=True,\n    overwrite=False,\n)\nExport the chat history to a file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nThe filename to export the chat to. Currently this must be a .md or .html file.\nrequired\n\n\nturns\nOptional[Sequence[Turn]]\nThe .get_turns() to export. If not provided, the chat‚Äôs current turns will be used.\nNone\n\n\ntitle\nOptional[str]\nA title to place at the top of the exported file.\nNone\n\n\noverwrite\nbool\nWhether to overwrite the file if it already exists.\nFalse\n\n\ncontent\nLiteral['text', 'all']\nWhether to include text content, all content (i.e., tool calls), or no content.\n'text'\n\n\ninclude_system_prompt\nbool\nWhether to include the system prompt in a \nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nPath\nThe path to the exported file.\n\n\n\n\n\n\n\nChat.extract_data(*args, data_model, echo='none', stream=False)\nExtract structured data from the given input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.extract_data_async(*args, data_model, echo='none', stream=False)\nExtract structured data from the given input asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks). Defaults to True if echo is not ‚Äúnone‚Äù.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.get_last_turn(role='assistant')\nGet the last turn in the chat with a specific role.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrole\nLiteral['assistant', 'user', 'system']\nThe role of the turn to return.\n'assistant'\n\n\n\n\n\n\n\nChat.get_tools()\nGet the list of registered tools.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[Tool]\nA list of Tool instances that are currently registered with the chat.\n\n\n\n\n\n\n\nChat.get_turns(include_system_prompt=False)\nGet all the turns (i.e., message contents) in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude_system_prompt\nbool\nWhether to include the system prompt in the turns.\nFalse\n\n\n\n\n\n\n\nChat.on_tool_request(callback)\nRegister a callback for a tool request event.\nA tool request event occurs when the assistant requests a tool to be called on its behalf. Before invoking the tool, on_tool_request handlers are called with the relevant ContentToolRequest object. This is useful if you want to handle tool requests in a custom way, such as requiring logging them or requiring user approval before invoking the tool\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncallback\nCallable[[ContentToolRequest], None]\nA function to be called when a tool request event occurs. This function must have a single argument, which will be the tool request (i.e., a ContentToolRequest object).\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA callable that can be used to remove the callback later.\n\n\n\n\n\n\n\n\nChat.on_tool_result(callback)\nRegister a callback for a tool result event.\nA tool result event occurs when a tool has been invoked and the result is ready to be provided to the assistant. After the tool has been invoked, on_tool_result handlers are called with the relevant ContentToolResult object. This is useful if you want to handle tool results in a custom way such as logging them.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncallback\nCallable[[ContentToolResult], None]\nA function to be called when a tool result event occurs. This function must have a single argument, which will be the tool result (i.e., a ContentToolResult object).\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA callable that can be used to remove the callback later.\n\n\n\n\n\n\n\n\nChat.register_mcp_tools_http_stream_async(\n    name,\n    url,\n    include_tools=(),\n    exclude_tools=(),\n    namespace=None,\n    transport_kwargs=None,\n)\nRegister tools from an MCP server using streamable HTTP transport.\nConnects to an MCP server (that communicates over a streamable HTTP transport) and registers the available tools. This is useful for utilizing tools provided by an MCP server running on a remote server (or locally) over HTTP.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRequires the mcp package to be installed. Install it with:\npip install mcp\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nA unique name for the MCP server session.\nrequired\n\n\nurl\nstr\nURL endpoint where the Streamable HTTP server is mounted (e.g., http://localhost:8000/mcp)\nrequired\n\n\ninclude_tools\nSequence[str]\nList of tool names to include. By default, all available tools are included.\n()\n\n\nexclude_tools\nSequence[str]\nList of tool names to exclude. This parameter and include_tools are mutually exclusive.\n()\n\n\nnamespace\nOptional[str]\nA namespace to prepend to tool names (i.e., namespace.tool_name) from this MCP server. This is primarily useful to avoid name collisions with other tools already registered with the chat. This namespace applies when tools are advertised to the LLM, so try to use a meaningful name that describes the server and/or the tools it provides. For example, if you have a server that provides tools for mathematical operations, you might use math as the namespace.\nNone\n\n\ntransport_kwargs\nOptional[dict[str, Any]]\nAdditional keyword arguments for the transport layer (i.e., mcp.client.streamable_http.streamablehttp_client).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n.cleanup_mcp_tools_async() : Cleanup registered MCP tools.\n.register_mcp_tools_stdio_async() : Register tools from an MCP server using stdio transport.\n\n\n\n\nUnlike the .register_mcp_tools_stdio_async() method, this method does not launch an MCP server. Instead, it assumes an HTTP server is already running at the specified URL. This is useful for connecting to an existing MCP server that is already running and serving tools.\n\n\n\nAssuming you have a Python script my_mcp_server.py that implements an MCP server like so:\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP(\"my_server\")\n\n@app.tool(description=\"Add two numbers.\")\ndef add(x: int, y: int) -&gt; int:\n    return x + y\n\napp.run(transport=\"streamable-http\")\nYou can launch this server like so:\npython my_mcp_server.py\nThen, you can register this server with the chat as follows:\nawait chat.register_mcp_tools_http_stream_async(\n    name=\"my_server\",\n    url=\"http://localhost:8080/mcp\"\n)\n\n\n\n\nChat.register_mcp_tools_stdio_async(\n    name,\n    command,\n    args,\n    include_tools=(),\n    exclude_tools=(),\n    namespace=None,\n    transport_kwargs=None,\n)\nRegister tools from a MCP server using stdio (standard input/output) transport.\nUseful for launching an MCP server and registering its tools with the chat ‚Äì all from the same Python process.\nIn more detail, this method:\n\nExecutes the given command with the provided args.\n\nThis should start an MCP server that communicates via stdio.\n\nEstablishes a client connection to the MCP server using the mcp package.\nRegisters the available tools from the MCP server with the chat.\nReturns a cleanup callback to close the MCP session and remove the tools.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRequires the mcp package to be installed. Install it with:\npip install mcp\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nA unique name for the MCP server session.\nrequired\n\n\ncommand\nstr\nSystem command to execute to start the MCP server (e.g., python).\nrequired\n\n\nargs\nlist[str]\nArguments to pass to the system command (e.g., [\"-m\", \"my_mcp_server\"]).\nrequired\n\n\ninclude_tools\nSequence[str]\nList of tool names to include. By default, all available tools are included.\n()\n\n\nexclude_tools\nSequence[str]\nList of tool names to exclude. This parameter and include_tools are mutually exclusive.\n()\n\n\nnamespace\nOptional[str]\nA namespace to prepend to tool names (i.e., namespace.tool_name) from this MCP server. This is primarily useful to avoid name collisions with other tools already registered with the chat. This namespace applies when tools are advertised to the LLM, so try to use a meaningful name that describes the server and/or the tools it provides. For example, if you have a server that provides tools for mathematical operations, you might use math as the namespace.\nNone\n\n\ntransport_kwargs\nOptional[dict[str, Any]]\nAdditional keyword arguments for the stdio transport layer (i.e., mcp.client.stdio.stdio_client).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n.cleanup_mcp_tools_async() : Cleanup registered MCP tools.\n.register_mcp_tools_http_stream_async() : Register tools from an MCP server using streamable HTTP transport.\n\n\n\n\nAssuming you have a Python script my_mcp_server.py that implements an MCP server like so\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP(\"my_server\")\n\n@app.tool(description=\"Add two numbers.\")\ndef add(y: int, z: int) -&gt; int:\n    return y - z\n\napp.run(transport=\"stdio\")\nYou can register this server with the chat as follows:\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI()\n\nawait chat.register_mcp_tools_stdio_async(\n    name=\"my_server\",\n    command=\"python\",\n    args=[\"-m\", \"my_mcp_server\"],\n)\n\n\n\n\nChat.register_tool(func, *, force=False, model=None)\nRegister a tool (function) with the chat.\nThe function will always be invoked in the current Python process.\n\n\nIf your tool has straightforward input parameters, you can just register the function directly (type hints and a docstring explaning both what the function does and what the parameters are for is strongly recommended):\nfrom chatlas import ChatOpenAI\n\n\ndef add(a: int, b: int) -&gt; int:\n    '''\n    Add two numbers together.\n\n####     Parameters {.doc-section .doc-section-----parameters}\n\n    a : int\n        The first number to add.\n    b : int\n        The second number to add.\n    '''\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add)\nchat.chat(\"What is 2 + 2?\")\nIf your tool has more complex input parameters, you can provide a Pydantic model that corresponds to the input parameters for the function, This way, you can have fields that hold other model(s) (for more complex input parameters), and also more directly document the input parameters:\nfrom chatlas import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\n\nclass AddParams(BaseModel):\n    '''Add two numbers together.'''\n\n    a: int = Field(description=\"The first number to add.\")\n\n    b: int = Field(description=\"The second number to add.\")\n\n\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add, model=AddParams)\nchat.chat(\"What is 2 + 2?\")",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/Chat.html#parameters-14",
    "href": "reference/Chat.html#parameters-14",
    "title": "Chat",
    "section": "",
    "text": "func The function to be invoked when the tool is called. force If True, overwrite any existing tool with the same name. If False (the default), raise an error if a tool with the same name already exists. model A Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function‚Äôs type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/Chat.html#raises",
    "href": "reference/Chat.html#raises",
    "title": "Chat",
    "section": "",
    "text": "ValueError If a tool with the same name already exists and force is False.\n\n\nChat.set_echo_options(rich_markdown=None, rich_console=None, css_styles=None)\nSet echo styling options for the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrich_markdown\nOptional[dict[str, Any]]\nA dictionary of options to pass to rich.markdown.Markdown(). This is only relevant when outputting to the console.\nNone\n\n\nrich_console\nOptional[dict[str, Any]]\nA dictionary of options to pass to rich.console.Console(). This is only relevant when outputting to the console.\nNone\n\n\ncss_styles\nOptional[dict[str, str]]\nA dictionary of CSS styles to apply to IPython.display.Markdown(). This is only relevant when outputing to the browser.\nNone\n\n\n\n\n\n\n\nChat.set_tools(tools)\nSet the tools for the chat.\nThis replaces any previously registered tools with the provided list of tools. This is for advanced usage ‚Äì typically, you would use .register_tool() to register individual tools as needed.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntools\nlist[Callable[‚Ä¶, Any] | Callable[‚Ä¶, Awaitable[Any]] | Tool]\nA list of Tool instances to set as the chat‚Äôs tools.\nrequired\n\n\n\n\n\n\n\nChat.set_turns(turns)\nSet the turns of the chat.\nThis method is primarily useful for clearing or setting the turns of the chat (i.e., limiting the context window).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nturns\nSequence[Turn]\nThe turns to set. Turns with the role ‚Äúsystem‚Äù are not allowed.\nrequired\n\n\n\n\n\n\n\nChat.stream(*args, content='text', echo='none', kwargs=None)\nGenerate a response from the chat in a streaming fashion.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\ncontent\nLiteral['text', 'all']\nWhether to yield just text content or include rich content objects (e.g., tool calls) when relevant.\n'text'\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponse\nAn (unconsumed) response from the chat. Iterate over this object to consume the response.\n\n\n\n\n\n\n\nChat.stream_async(*args, content='text', echo='none', kwargs=None)\nGenerate a response from the chat in a streaming fashion asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\ncontent\nLiteral['text', 'all']\nWhether to yield just text content or include rich content objects (e.g., tool calls) when relevant.\n'text'\n\n\necho\nEchoOptions\nOne of the following (default is ‚Äúnone‚Äù): - \"text\": Echo just the text content of the response. - \"output\": Echo text and tool call content. - \"all\": Echo both the assistant and user turn. - \"none\": Do not echo any content.\n'none'\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponseAsync\nAn (unconsumed) response from the chat. Iterate over this object to consume the response.\n\n\n\n\n\n\n\nChat.token_count(*args, data_model=None)\nGet an estimated token count for the given input.\nEstimate the token size of input content. This can help determine whether input(s) and/or conversation history (i.e., .get_turns()) should be reduced in size before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to get a token count for.\n()\n\n\ndata_model\nOptional[type[BaseModel]]\nIf the input is meant for data extraction (i.e., .extract_data()), then this should be the Pydantic model that describes the structure of the data to extract.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe token count for the input.\n\n\n\n\n\n\nRemember that the token count is an estimate. Also, models based on ChatOpenAI() currently does not take tools into account when estimating token counts.\n\n\n\nfrom chatlas import ChatAnthropic\n\nchat = ChatAnthropic()\n# Estimate the token count before sending the input\nprint(chat.token_count(\"What is 2 + 2?\"))\n\n# Once input is sent, you can get the actual input and output\n# token counts from the chat object\nchat.chat(\"What is 2 + 2?\", echo=\"none\")\nprint(chat.token_usage())\n\n\n\n\nChat.token_count_async(*args, data_model=None)\nGet an estimated token count for the given input asynchronously.\nEstimate the token size of input content. This can help determine whether input(s) and/or conversation history (i.e., .get_turns()) should be reduced in size before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to get a token count for.\n()\n\n\ndata_model\nOptional[type[BaseModel]]\nIf this input is meant for data extraction (i.e., .extract_data_async()), then this should be the Pydantic model that describes the structure of the data to extract.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe token count for the input.\n\n\n\n\n\n\n\nChat.tokens(values='discrete')\nGet the tokens for each turn in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nLiteral['cumulative', 'discrete']\nIf ‚Äúcumulative‚Äù (the default), the result can be summed to get the chat‚Äôs overall token usage (helpful for computing overall cost of the chat). If ‚Äúdiscrete‚Äù, the result can be summed to get the number of tokens the turns will cost to generate the next response (helpful for estimating cost of the next response, or for determining if you are about to exceed the token limit).\n'discrete'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[int]\nA list of token counts for each (non-system) turn in the chat. The 1st turn includes the tokens count for the system prompt (if any).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the chat‚Äôs turns (i.e., .get_turns()) are not in an expected format. This may happen if the chat history is manually set (i.e., .set_turns()). In this case, you can inspect the ‚Äúraw‚Äù token values via the .get_turns() method (each turn has a .tokens attribute).",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/image_url.html",
    "href": "reference/image_url.html",
    "title": "content_image_url",
    "section": "",
    "text": "content_image_url(url, detail='auto')\nEncode image content from a URL for chat input.\nThis function is used to prepare image URLs for input to the chatbot. It can handle both regular URLs and data URLs.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid."
  },
  {
    "objectID": "reference/image_url.html#parameters",
    "href": "reference/image_url.html#parameters",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'"
  },
  {
    "objectID": "reference/image_url.html#returns",
    "href": "reference/image_url.html#returns",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object."
  },
  {
    "objectID": "reference/image_url.html#examples",
    "href": "reference/image_url.html#examples",
    "title": "content_image_url",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)"
  },
  {
    "objectID": "reference/image_url.html#raises",
    "href": "reference/image_url.html#raises",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid."
  },
  {
    "objectID": "reference/content_pdf_url.html",
    "href": "reference/content_pdf_url.html",
    "title": "content_pdf_url",
    "section": "",
    "text": "content_pdf_url(url)\nUse a remote PDF for input to a chat.\nNot all providers support PDF input, so check the documentation for the provider you are using.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nA URL to a remote PDF file.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "PDF input",
      "content_pdf_url"
    ]
  },
  {
    "objectID": "reference/content_pdf_url.html#parameters",
    "href": "reference/content_pdf_url.html#parameters",
    "title": "content_pdf_url",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nA URL to a remote PDF file.\nrequired",
    "crumbs": [
      "Reference",
      "PDF input",
      "content_pdf_url"
    ]
  },
  {
    "objectID": "reference/content_pdf_url.html#returns",
    "href": "reference/content_pdf_url.html#returns",
    "title": "content_pdf_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "PDF input",
      "content_pdf_url"
    ]
  },
  {
    "objectID": "reference/Provider.html",
    "href": "reference/Provider.html",
    "title": "Provider",
    "section": "",
    "text": "Provider\nProvider()\nA model provider interface for a Chat.\nThis abstract class defines the interface a model provider must implement in order to be used with a Chat instance. The provider is responsible for performing the actual chat completion, and for handling the streaming of the completion results.\nNote that this class is exposed for developers who wish to implement their own provider. In general, you should not need to interact with this class directly.",
    "crumbs": [
      "Reference",
      "Implement a model provider",
      "Provider"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html",
    "href": "reference/ChatPerplexity.html",
    "title": "ChatPerplexity",
    "section": "",
    "text": "ChatPerplexity(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://api.perplexity.ai/',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on perplexity.ai.\nPerplexity AI is a platform for running LLMs that are capable of searching the web in real-time to help them answer questions with information that may not have been available when the model was trained.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nSign up at https://www.perplexity.ai to get an API key.\n\n\n\n\n\nimport os\nfrom chatlas import ChatPerplexity\n\nchat = ChatPerplexity(api_key=os.getenv(\"PERPLEXITY_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the PERPLEXITY_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Perplexity‚Äôs API.\n'https://api.perplexity.ai/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nThis function is a lightweight wrapper around chatlas.ChatOpenAI with the defaults tweaked for perplexity.ai.\n\n\n\nPasting an API key into a chat constructor (e.g., ChatPerplexity(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nPERPLEXITY_API_KEY=...\nfrom chatlas import ChatPerplexity\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatPerplexity()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport PERPLEXITY_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#prerequisites",
    "href": "reference/ChatPerplexity.html#prerequisites",
    "title": "ChatPerplexity",
    "section": "",
    "text": "API key\n\n\n\nSign up at https://www.perplexity.ai to get an API key.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#examples",
    "href": "reference/ChatPerplexity.html#examples",
    "title": "ChatPerplexity",
    "section": "",
    "text": "import os\nfrom chatlas import ChatPerplexity\n\nchat = ChatPerplexity(api_key=os.getenv(\"PERPLEXITY_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#parameters",
    "href": "reference/ChatPerplexity.html#parameters",
    "title": "ChatPerplexity",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the PERPLEXITY_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Perplexity‚Äôs API.\n'https://api.perplexity.ai/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#returns",
    "href": "reference/ChatPerplexity.html#returns",
    "title": "ChatPerplexity",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#note",
    "href": "reference/ChatPerplexity.html#note",
    "title": "ChatPerplexity",
    "section": "",
    "text": "This function is a lightweight wrapper around chatlas.ChatOpenAI with the defaults tweaked for perplexity.ai.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#note-1",
    "href": "reference/ChatPerplexity.html#note-1",
    "title": "ChatPerplexity",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatPerplexity(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nPERPLEXITY_API_KEY=...\nfrom chatlas import ChatPerplexity\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatPerplexity()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport PERPLEXITY_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatSnowflake.html",
    "href": "reference/ChatSnowflake.html",
    "title": "ChatSnowflake",
    "section": "",
    "text": "ChatSnowflake(\n    system_prompt=None,\n    model=None,\n    turns=None,\n    connection_name=None,\n    account=None,\n    user=None,\n    password=None,\n    private_key_file=None,\n    private_key_file_pwd=None,\n    kwargs=None,\n)\nChat with a Snowflake Cortex LLM\nhttps://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatSnowflake, requires the snowflake-ml-python package: pip install \"chatlas[snowflake]\".\n\n\n\n\n\n\n\n\nSnowflake credentials\n\n\n\nSnowflake provides a handful of ways to authenticate, but it‚Äôs recommended to use key-pair auth to generate a private_key_file. It‚Äôs also recommended to place your credentials in a connections.toml file.\nThis way, once your credentials are in the connections.toml file, you can simply call ChatSnowflake(connection_name=\"my_connection\") to authenticate. If you don‚Äôt want to use a connections.toml file, you can specify the connection parameters directly (with account, user, password, etc.).\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nconnection_name\nOptional[str]\nThe name of the connection (i.e., section) within the connections.toml file. This is useful if you want to keep your credentials in a connections.toml file rather than specifying them directly in the arguments. https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-session#connect-by-using-the-connections-toml-file\nNone\n\n\naccount\nOptional[str]\nYour Snowflake account identifier. Required if connection_name is not provided. https://docs.snowflake.com/en/user-guide/admin-account-identifier\nNone\n\n\nuser\nOptional[str]\nYour Snowflake user name. Required if connection_name is not provided.\nNone\n\n\npassword\nOptional[str]\nYour Snowflake password. Required if doing password authentication and connection_name is not provided.\nNone\n\n\nprivate_key_file\nOptional[str]\nThe path to your private key file. Required if you are using key pair authentication. https://docs.snowflake.com/en/user-guide/key-pair-auth\nNone\n\n\nprivate_key_file_pwd\nOptional[str]\nThe password for your private key file. Required if you are using key pair authentication. https://docs.snowflake.com/en/user-guide/key-pair-auth\nNone\n\n\nkwargs\nOptional[dict[str, str | int]]\nAdditional keyword arguments passed along to the Snowflake connection builder. These can include any parameters supported by the snowflake-ml-python package. https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-session#connect-by-specifying-connection-parameters\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatSnowflake"
    ]
  },
  {
    "objectID": "reference/ChatSnowflake.html#prerequisites",
    "href": "reference/ChatSnowflake.html#prerequisites",
    "title": "ChatSnowflake",
    "section": "",
    "text": "Python requirements\n\n\n\nChatSnowflake, requires the snowflake-ml-python package: pip install \"chatlas[snowflake]\".\n\n\n\n\n\n\n\n\nSnowflake credentials\n\n\n\nSnowflake provides a handful of ways to authenticate, but it‚Äôs recommended to use key-pair auth to generate a private_key_file. It‚Äôs also recommended to place your credentials in a connections.toml file.\nThis way, once your credentials are in the connections.toml file, you can simply call ChatSnowflake(connection_name=\"my_connection\") to authenticate. If you don‚Äôt want to use a connections.toml file, you can specify the connection parameters directly (with account, user, password, etc.).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatSnowflake"
    ]
  },
  {
    "objectID": "reference/ChatSnowflake.html#parameters",
    "href": "reference/ChatSnowflake.html#parameters",
    "title": "ChatSnowflake",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nconnection_name\nOptional[str]\nThe name of the connection (i.e., section) within the connections.toml file. This is useful if you want to keep your credentials in a connections.toml file rather than specifying them directly in the arguments. https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-session#connect-by-using-the-connections-toml-file\nNone\n\n\naccount\nOptional[str]\nYour Snowflake account identifier. Required if connection_name is not provided. https://docs.snowflake.com/en/user-guide/admin-account-identifier\nNone\n\n\nuser\nOptional[str]\nYour Snowflake user name. Required if connection_name is not provided.\nNone\n\n\npassword\nOptional[str]\nYour Snowflake password. Required if doing password authentication and connection_name is not provided.\nNone\n\n\nprivate_key_file\nOptional[str]\nThe path to your private key file. Required if you are using key pair authentication. https://docs.snowflake.com/en/user-guide/key-pair-auth\nNone\n\n\nprivate_key_file_pwd\nOptional[str]\nThe password for your private key file. Required if you are using key pair authentication. https://docs.snowflake.com/en/user-guide/key-pair-auth\nNone\n\n\nkwargs\nOptional[dict[str, str | int]]\nAdditional keyword arguments passed along to the Snowflake connection builder. These can include any parameters supported by the snowflake-ml-python package. https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-session#connect-by-specifying-connection-parameters\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatSnowflake"
    ]
  },
  {
    "objectID": "reference/Turn.html",
    "href": "reference/Turn.html",
    "title": "Turn",
    "section": "",
    "text": "Turn(\n    role,\n    contents,\n    *,\n    tokens=None,\n    finish_reason=None,\n    completion=None,\n    **kwargs,\n)\nA user or assistant turn\nEvery conversation with a chatbot consists of pairs of user and assistant turns, corresponding to an HTTP request and response. These turns are represented by the Turn object, which contains a list of Contents representing the individual messages within the turn. These might be text, images, tool requests (assistant only), or tool responses (user only).\nNote that a call to .chat() and related functions may result in multiple user-assistant turn cycles. For example, if you have registered tools, chatlas will automatically handle the tool calling loop, which may result in any number of additional cycles.\n\n\nfrom chatlas import Turn, ChatOpenAI, ChatAnthropic\n\nchat = ChatOpenAI()\nstr(chat.chat(\"What is the capital of France?\"))\nturns = chat.get_turns()\nassert len(turns) == 2\nassert isinstance(turns[0], Turn)\nassert turns[0].role == \"user\"\nassert turns[1].role == \"assistant\"\n\n# Load context into a new chat instance\nchat2 = ChatAnthropic(turns=turns)\nturns2 = chat2.get_turns()\nassert turns == turns2\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrole\nLiteral['user', 'assistant', 'system']\nEither ‚Äúuser‚Äù, ‚Äúassistant‚Äù, or ‚Äúsystem‚Äù.\nrequired\n\n\ncontents\nstr | Sequence[Content | str]\nA list of Content objects.\nrequired\n\n\ntokens\nOptional[tuple[int, int]]\nA numeric vector of length 2 representing the number of input and output tokens (respectively) used in this turn. Currently only recorded for assistant turns.\nNone\n\n\nfinish_reason\nOptional[str]\nA string indicating the reason why the conversation ended. This is only relevant for assistant turns.\nNone\n\n\ncompletion\nOptional[CompletionT]\nThe completion object returned by the provider. This is useful if there‚Äôs information returned by the provider that chatlas doesn‚Äôt otherwise expose. This is only relevant for assistant turns.\nNone",
    "crumbs": [
      "Reference",
      "Turns",
      "Turn"
    ]
  },
  {
    "objectID": "reference/Turn.html#examples",
    "href": "reference/Turn.html#examples",
    "title": "Turn",
    "section": "",
    "text": "from chatlas import Turn, ChatOpenAI, ChatAnthropic\n\nchat = ChatOpenAI()\nstr(chat.chat(\"What is the capital of France?\"))\nturns = chat.get_turns()\nassert len(turns) == 2\nassert isinstance(turns[0], Turn)\nassert turns[0].role == \"user\"\nassert turns[1].role == \"assistant\"\n\n# Load context into a new chat instance\nchat2 = ChatAnthropic(turns=turns)\nturns2 = chat2.get_turns()\nassert turns == turns2",
    "crumbs": [
      "Reference",
      "Turns",
      "Turn"
    ]
  },
  {
    "objectID": "reference/Turn.html#parameters",
    "href": "reference/Turn.html#parameters",
    "title": "Turn",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nrole\nLiteral['user', 'assistant', 'system']\nEither ‚Äúuser‚Äù, ‚Äúassistant‚Äù, or ‚Äúsystem‚Äù.\nrequired\n\n\ncontents\nstr | Sequence[Content | str]\nA list of Content objects.\nrequired\n\n\ntokens\nOptional[tuple[int, int]]\nA numeric vector of length 2 representing the number of input and output tokens (respectively) used in this turn. Currently only recorded for assistant turns.\nNone\n\n\nfinish_reason\nOptional[str]\nA string indicating the reason why the conversation ended. This is only relevant for assistant turns.\nNone\n\n\ncompletion\nOptional[CompletionT]\nThe completion object returned by the provider. This is useful if there‚Äôs information returned by the provider that chatlas doesn‚Äôt otherwise expose. This is only relevant for assistant turns.\nNone",
    "crumbs": [
      "Reference",
      "Turns",
      "Turn"
    ]
  },
  {
    "objectID": "reference/types.ContentImageRemote.html",
    "href": "reference/types.ContentImageRemote.html",
    "title": "types.ContentImageRemote",
    "section": "",
    "text": "types.ContentImageRemote()\nImage content from a URL.\nThis is the return type for content_image_url. It‚Äôs not meant to be used directly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\n\nThe URL of the image.\nrequired\n\n\ndetail\n\nA detail setting for the image. Can be \"auto\", \"low\", or \"high\".\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImageRemote"
    ]
  },
  {
    "objectID": "reference/types.ContentImageRemote.html#parameters",
    "href": "reference/types.ContentImageRemote.html#parameters",
    "title": "types.ContentImageRemote",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nurl\n\nThe URL of the image.\nrequired\n\n\ndetail\n\nA detail setting for the image. Can be \"auto\", \"low\", or \"high\".\nrequired",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImageRemote"
    ]
  },
  {
    "objectID": "reference/image_file.html",
    "href": "reference/image_file.html",
    "title": "content_image_file",
    "section": "",
    "text": "content_image_file(path, content_type='auto', resize='low')\nEncode image content from a file for chat input.\nThis function is used to prepare image files for input to the chatbot. It can handle various image formats and provides options for resizing.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[str, Literal['none', 'low', 'high']]\nResizing option for the image. Can be: - \"none\": No resizing - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\n'low'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid."
  },
  {
    "objectID": "reference/image_file.html#parameters",
    "href": "reference/image_file.html#parameters",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[str, Literal['none', 'low', 'high']]\nResizing option for the image. Can be: - \"none\": No resizing - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\n'low'"
  },
  {
    "objectID": "reference/image_file.html#returns",
    "href": "reference/image_file.html#returns",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object."
  },
  {
    "objectID": "reference/image_file.html#examples",
    "href": "reference/image_file.html#examples",
    "title": "content_image_file",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)"
  },
  {
    "objectID": "reference/image_file.html#raises",
    "href": "reference/image_file.html#raises",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid."
  },
  {
    "objectID": "reference/types.TokenUsage.html",
    "href": "reference/types.TokenUsage.html",
    "title": "types.TokenUsage",
    "section": "",
    "text": "types.TokenUsage\ntypes.TokenUsage()\nToken usage for a given provider (name).",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.TokenUsage"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html",
    "href": "reference/content_image_plot.html",
    "title": "content_image_plot",
    "section": "",
    "text": "content_image_plot(width=768, height=768, dpi=72)\nEncode the current matplotlib plot as an image for chat input.\nThis function captures the current matplotlib plot, resizes it to the specified dimensions, and prepares it for chat input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#parameters",
    "href": "reference/content_image_plot.html#parameters",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#returns",
    "href": "reference/content_image_plot.html#returns",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#raises",
    "href": "reference/content_image_plot.html#raises",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#examples",
    "href": "reference/content_image_plot.html#examples",
    "title": "content_image_plot",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/types.MISSING_TYPE.html",
    "href": "reference/types.MISSING_TYPE.html",
    "title": "types.MISSING_TYPE",
    "section": "",
    "text": "types.MISSING_TYPE\ntypes.MISSING_TYPE()\nA singleton representing a missing value.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.MISSING_TYPE"
    ]
  },
  {
    "objectID": "reference/types.ContentText.html",
    "href": "reference/types.ContentText.html",
    "title": "types.ContentText",
    "section": "",
    "text": "types.ContentText\ntypes.ContentText(**data)\nText content for a Turn",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentText"
    ]
  },
  {
    "objectID": "reference/types.ChatResponseAsync.html",
    "href": "reference/types.ChatResponseAsync.html",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "types.ChatResponseAsync(generator)\nChat response (async) object.\nAn object that, when displayed, will simulatenously consume (if not already consumed) and display the response in a streaming fashion.\nThis is useful for interactive use: if the object is displayed, it can be viewed as it is being generated. And, if the object is not displayed, it can act like an iterator that can be consumed by something else.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.\n\n\n\n\n\n\nconsumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_content\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponseAsync.get_content()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/types.ChatResponseAsync.html#attributes",
    "href": "reference/types.ChatResponseAsync.html#attributes",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/types.ChatResponseAsync.html#properties",
    "href": "reference/types.ChatResponseAsync.html#properties",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "consumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/types.ChatResponseAsync.html#methods",
    "href": "reference/types.ChatResponseAsync.html#methods",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_content\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponseAsync.get_content()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html",
    "href": "reference/types.ChatResponse.html",
    "title": "types.ChatResponse",
    "section": "",
    "text": "types.ChatResponse(generator)\nChat response object.\nAn object that, when displayed, will simulatenously consume (if not already consumed) and display the response in a streaming fashion.\nThis is useful for interactive use: if the object is displayed, it can be viewed as it is being generated. And, if the object is not displayed, it can act like an iterator that can be consumed by something else.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.\n\n\n\n\n\n\nconsumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_content\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponse.get_content()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html#attributes",
    "href": "reference/types.ChatResponse.html#attributes",
    "title": "types.ChatResponse",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html#properties",
    "href": "reference/types.ChatResponse.html#properties",
    "title": "types.ChatResponse",
    "section": "",
    "text": "consumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html#methods",
    "href": "reference/types.ChatResponse.html#methods",
    "title": "types.ChatResponse",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_content\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponse.get_content()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/types.Content.html",
    "href": "reference/types.Content.html",
    "title": "types.Content",
    "section": "",
    "text": "types.Content\ntypes.Content()\nBase class for all content types that can be appear in a Turn",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.Content"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html",
    "href": "reference/ChatOllama.html",
    "title": "ChatOllama",
    "section": "",
    "text": "ChatOllama(\n    model=None,\n    *,\n    system_prompt=None,\n    turns=None,\n    base_url='http://localhost:11434',\n    seed=None,\n    kwargs=None,\n)\nChat with a local Ollama model.\nOllama makes it easy to run a wide-variety of open-source models locally, making it a great choice for privacy and security.\n\n\n\n\n\n\n\n\nOllama runtime\n\n\n\nChatOllama requires the ollama executable to be installed and running on your machine.\n\n\n\n\n\n\n\n\nPull model(s)\n\n\n\nOnce ollama is running locally, download a model from the command line (e.g.¬†ollama pull llama3.2).\n\n\n\n\n\nfrom chatlas import ChatOllama\n\nchat = ChatOllama(model=\"llama3.2\")\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat. If None, a list of locally installed models will be printed.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses ollama‚Äôs API.\n'http://localhost:11434'\n\n\nseed\nOptional[int]\nOptional integer seed that helps to make output more reproducible.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\nThis function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for ollama.\n\n\n\nChatOllama currently doesn‚Äôt work with streaming tools, and tool calling more generally doesn‚Äôt seem to work very well with currently available models.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#prerequisites",
    "href": "reference/ChatOllama.html#prerequisites",
    "title": "ChatOllama",
    "section": "",
    "text": "Ollama runtime\n\n\n\nChatOllama requires the ollama executable to be installed and running on your machine.\n\n\n\n\n\n\n\n\nPull model(s)\n\n\n\nOnce ollama is running locally, download a model from the command line (e.g.¬†ollama pull llama3.2).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#examples",
    "href": "reference/ChatOllama.html#examples",
    "title": "ChatOllama",
    "section": "",
    "text": "from chatlas import ChatOllama\n\nchat = ChatOllama(model=\"llama3.2\")\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#parameters",
    "href": "reference/ChatOllama.html#parameters",
    "title": "ChatOllama",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat. If None, a list of locally installed models will be printed.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses ollama‚Äôs API.\n'http://localhost:11434'\n\n\nseed\nOptional[int]\nOptional integer seed that helps to make output more reproducible.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#note",
    "href": "reference/ChatOllama.html#note",
    "title": "ChatOllama",
    "section": "",
    "text": "This function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for ollama.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#limitations",
    "href": "reference/ChatOllama.html#limitations",
    "title": "ChatOllama",
    "section": "",
    "text": "ChatOllama currently doesn‚Äôt work with streaming tools, and tool calling more generally doesn‚Äôt seem to work very well with currently available models.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "get-started/async.html",
    "href": "get-started/async.html",
    "title": "Async",
    "section": "",
    "text": "Important Chat methods such as .chat(), .stream(), etc., are synchronous, but are also available in an asynchronous form. Most important amongst these is .stream_async() ‚Äì the recommended way to stream in a production environment where multiple users may be streaming at the same time.\nThe next article uses .stream_async() to build performant chatbot apps, but below is a more minimal example of how to it works. Note that, in order to use async methods, you need to await the result inside an async function.\nimport asyncio\nimport chatlas as ctl\n\nchat = ctl.ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\nasync def do_stream(prompt: str):\n    stream = await chat.stream_async(prompt)\n    async for chunk in stream:\n        print(chunk)\n\nasyncio.run(do_stream(\"My name is Chatlas.\"))\nHello\nChatlas.\nHow\ncan\nI\nhelp?",
    "crumbs": [
      "Get started",
      "Async"
    ]
  },
  {
    "objectID": "get-started/stream.html",
    "href": "get-started/stream.html",
    "title": "Streams",
    "section": "",
    "text": "The .stream() method returns a generator that yields the model‚Äôs response one chunk at a time. This makes it a better choice than .chat() for programming bespoke experiences such as chatbot apps, where some other framework is responsible for consuming and displaying the response.\nJust like .chat(), once the model is done responding (i.e., the generator is exhausted), the user and assistant turns are stored on the Chat instance. That means, you can save and manage them same as you would with .chat().",
    "crumbs": [
      "Get started",
      "Streams"
    ]
  },
  {
    "objectID": "get-started/stream.html#content-types",
    "href": "get-started/stream.html#content-types",
    "title": "Streams",
    "section": "Content types",
    "text": "Content types\n.stream() also provides access to rich content types beyond just text. To gain access to these, set the content parameter to \"all\". If the response includes things like tool calls, the stream will yield the relevant content types as they are generated. As we‚Äôll learn later, this can be useful for displaying tool calls in something like a chatbot app.\ndef get_current_weather(lat: float, lng: float):\n    \"Get the current weather for a given location.\"\n    return \"sunny\"\n\nchat.register_tool(get_current_weather)\n\nstream = chat.stream(\n  \"How's the weather in San Francisco?\", \n  content=\"all\"\n)\nfor chunk in stream:\n   print(type(chunk))\n&lt;class 'chatlas._content.ContentToolRequest'&gt;\n&lt;class 'chatlas._content.ContentToolResult'&gt;\n&lt;class 'str'&gt;\n&lt;class 'str'&gt;\n&lt;class 'str'&gt;\n&lt;class 'str'&gt;",
    "crumbs": [
      "Get started",
      "Streams"
    ]
  },
  {
    "objectID": "get-started/stream.html#wrapping-generators",
    "href": "get-started/stream.html#wrapping-generators",
    "title": "Streams",
    "section": "Wrapping generators",
    "text": "Wrapping generators\nSometimes it‚Äôs useful to wrap the .stream() generator up into another generator function.\nThis is useful for adding additional functionality, such as:\n\nAdding a delay between chunks\nFiltering out certain content types\nAdding custom formatting\netc.\n\nimport time\n\ndef stream_with_delay(prompt: str, delay: float = 0.5):\n    \"\"\"\n    Stream the model's response with a delay between chunks.\n    \"\"\"\n    stream = chat.stream(prompt)\n    for chunk in stream:\n        time.sleep(delay)\n        yield chunk\n\nfor chunk in stream_with_delay(\"How's the weather in San Francisco?\"):\n    print(chunk)\nThe current weather in San Francisco is sunny.",
    "crumbs": [
      "Get started",
      "Streams"
    ]
  },
  {
    "objectID": "get-started/chat.html",
    "href": "get-started/chat.html",
    "title": "Hello chat",
    "section": "",
    "text": "Once you‚Äôve choosen a model, initialize the relevant Chat client instance. Here you access to handful of parameters, most importantly the model and system_prompt:\nSoon we‚Äôll learn more about the system prompt, but for now, just know that it is the primary place for you (the developer) to influence the model‚Äôs behavior.",
    "crumbs": [
      "Get started",
      "Hello chat"
    ]
  },
  {
    "objectID": "get-started/chat.html#submit-input",
    "href": "get-started/chat.html#submit-input",
    "title": "Hello chat",
    "section": "Submit input",
    "text": "Submit input\nUse the .chat() method the submit input and get a streaming response echoed back to an interactive display, like a notebook or console. This echoing behavior is great for interactive prototyping, but not for other settings, such as web apps or GUIs. In the latter case, use the .stream() method instead to get a response generator that you can consume and display however you like.\n\nNotebookConsole\n\n\n\n\nVideo of streaming chat in a notebook\n\n\n\n\n\nVideo of streaming chat in a console\n\n\n\n\n\n\n\n\n\n\nReturn value\n\n\n\n\n\nApply str() to the return value of .chat() to get the model‚Äôs response as a string. If you find yourself doing this, however, consider using .stream() instead. This is because .chat() will wait/block until the model has finished generating its response, which can take a while for large models.\nresponse = chat.chat(\"What's my name?\")\nstr(response)\n\n\n\n\n\n\n\n\n\nEcho options\n\n\n\n\n\nControl what content gets echoed through the echo parameter. By default, the model‚Äôs text response, as well as tool calls and their results, are echoed. This is convenient for verifying what information was passed to the model, but can be a bit verbose. Instead, you may want to restrict what is echoed to just the model‚Äôs text response.\n# Echo only the model's text response\nchat.chat(\"What's my name?\", echo=\"text\")",
    "crumbs": [
      "Get started",
      "Hello chat"
    ]
  },
  {
    "objectID": "get-started/chat.html#multi-modal-input",
    "href": "get-started/chat.html#multi-modal-input",
    "title": "Hello chat",
    "section": "Multi-modal input",
    "text": "Multi-modal input\nThe .chat() method also accepts input other than text, such as images, pdfs, and more. These content objects can be created using a function such as content_image_url(), content_pdf_file(), etc.\nimport chatlas as ctl\nchat = ctl.ChatOpenAI()\nchat.chat(\n  ctl.content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n  \"Can you explain this logo?\"\n)\nThe Python logo features two intertwined snakes in yellow and blue,\nrepresenting the Python programming language. The design symbolizes...\n\n\n\n\n\n\nSupported content types\n\n\n\nNot every model supports every content type. Please refer to the documentation for the specific model you‚Äôre using to see which content types are supported.",
    "crumbs": [
      "Get started",
      "Hello chat"
    ]
  },
  {
    "objectID": "get-started/chat.html#chat-history",
    "href": "get-started/chat.html#chat-history",
    "title": "Hello chat",
    "section": "Chat history",
    "text": "Chat history\nNote that chat is a stateful object, and accumulates conversation history by default. This is the behavior you typically want to multi-turn conversations since it allows the model to remember previous interactions.\nimport chatlas as ctl\nchat = ctl.ChatOpenAI()\nchat.chat(\"My name is Chatlas.\")\nchat.chat(\"What's my name?\")\nYour name is Chatlas.\nThis means that the model is provided the entire conversation history on each new submission. This again is typically the desirable behavior, but sometimes you may want to fork, reset, condense, or otherwise manage history.",
    "crumbs": [
      "Get started",
      "Hello chat"
    ]
  },
  {
    "objectID": "get-started/chat.html#dedicated-chat",
    "href": "get-started/chat.html#dedicated-chat",
    "title": "Hello chat",
    "section": "Dedicated chat",
    "text": "Dedicated chat\nWhen you‚Äôre first starting out testing the capabilities of a model, repeatedly calling .chat() is a bit tedious. Instead, consider using the .console() or .app() methods to launch a dedicated chat interface. They will save you a bit of typing and, in the case of .app(), enables a more interactive, browser-based, experience.\n\n\n\n\n\n\nRemember that chat is a stateful object, so the history is retained across calls to .console() and .app().\n\n\n\n\nWeb appConsole\n\n\n\n\n\nScreenshot of a dedicated chat web app\n\n\n\n\n\n\n\nScreenshot of a dedicated chat console",
    "crumbs": [
      "Get started",
      "Hello chat"
    ]
  },
  {
    "objectID": "get-started/chat.html#save-history",
    "href": "get-started/chat.html#save-history",
    "title": "Hello chat",
    "section": "Save history",
    "text": "Save history\nPrinting chat at the console shows the conversation history, but you can also .export() it to a more readable markdown or HTML file. When exporting to HTML, you‚Äôll get a display similar to the dedicated chat app.\nchat.export(\"chat.html\")\n\n\n\n\n\n\nSerializing history\n\n\n\n\n\nSince Turns inherit from pydantic‚Äôs BaseModel, you can also serialize/unserialize them to JSON, which is useful for saving/loading the history to/from a database or file.\nturns = chat.get_turns()\nturns_json = [x.model_dump_json() for x in turns]\nturns_restored = [Turn.model_validate_json(x) for x in turns_json]",
    "crumbs": [
      "Get started",
      "Hello chat"
    ]
  },
  {
    "objectID": "get-started/chat.html#manage-history",
    "href": "get-started/chat.html#manage-history",
    "title": "Hello chat",
    "section": "Manage history",
    "text": "Manage history\nThe chat history is stored as a list of Turn objects. To get/set them, use .get_turns() / .set_turns().\n\nReset\nHere‚Äôs an example of how to reset the history:\nchat.set_turns([])\nchat.chat(\"What's my name?\")\nI don‚Äôt know your name unless you choose to share it with me. \n\n\nFork\nYou can also fork the history by copying the chat object. This is useful if you want to create a new conversation with a different context, but still want to keep the original conversation history intact.\nimport copy\n\nchat_fork = copy.deepcopy(chat)\nchat_fork.chat(\"My name is Chatlas.\")\nchat_fork.chat(\"What's my name?\")\nYour name is Chatlas.\nchat.chat(\"What's my name?\")\nI don‚Äôt know your name unless you choose to share it with me.\n\n\nCondense\nYou can also condense the history asking the LLM to summarize it. This is useful if you want to keep the context of the conversation, but don‚Äôt want to provide the entire history to the model on each new submission.\nchat.chat(\"My name is Chatlas.\")\nchat.chat(\"Can you summarize our conversation so far?\")\nchat.set_turns([chat.get_last_turn()])",
    "crumbs": [
      "Get started",
      "Hello chat"
    ]
  },
  {
    "objectID": "get-started/tools.html",
    "href": "get-started/tools.html",
    "title": "Tool calling",
    "section": "",
    "text": "Tool calling helps extend the capabilities of an LLM by allowing it to call external functions or APIs. This is particularly useful for tasks that require precise calculations, information retrieval, or actions that are beyond the model‚Äôs built-in capabilities.\nAn obvious example of where tool calling is useful is when a response needs up-to-date information, such as the weather or stock prices:\nimport chatlas as ctl\n\nchat = ctl.ChatOpenAI()\nchat.chat(\"How's the weather in San Francisco?\")\n\nI‚Äôm unable to provide real-time weather updates. To get the most current weather information for San Francisco, I recommend checking a reliable weather website or using a weather app.",
    "crumbs": [
      "Get started",
      "Tool calling"
    ]
  },
  {
    "objectID": "get-started/tools.html#footnotes",
    "href": "get-started/tools.html#footnotes",
    "title": "Tool calling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome models may need some additional (system) prompting to help it understand the appropriate situations to use the function.‚Ü©Ô∏é",
    "crumbs": [
      "Get started",
      "Tool calling"
    ]
  },
  {
    "objectID": "get-started/system-prompt.html",
    "href": "get-started/system-prompt.html",
    "title": "System prompt",
    "section": "",
    "text": "The system_prompt is the primary place for you (the developer) to influence the behavior of the model. A well-crafted system prompt steers the model toward delightful, accurate, and safe answers. Later on, I‚Äôll offer some resources and tips for effective prompting, but for now, just know that:\nHere‚Äôs a simple example of setting the model‚Äôs role/persona using the system_prompt, and filling in the role using an f-string:\nAs the system_prompt grows in size, consider moving it to (markdown) file. Also, since more complex prompts may include things like JSON (which conflicts with f-string‚Äôs { } syntax), consider using a more robust templating approach.",
    "crumbs": [
      "Get started",
      "System prompt"
    ]
  },
  {
    "objectID": "get-started/system-prompt.html#templates",
    "href": "get-started/system-prompt.html#templates",
    "title": "System prompt",
    "section": "Templates",
    "text": "Templates\nThe interpolate_file() function allows you to interpolate variables into a prompt template stored in a file. By default, it expects variables to be wrapped using a {{ x }} syntax, which has a much lower chance of conflicting with complex prompts in unexpected ways, and is powered by the fantastic Jinja2 templating engine:\nwith open('prompt.md', 'w') as f:\n    f.write('I want you to act like {{ role }}')\nimport chatlas as ctl\nchat = ctl.ChatOpenAI()\n\nchat.system_prompt = ctl.interpolate_file(\n  \"prompt.md\",\n  variables={\"role\": \"Yoda\"}\n)\nprint(chat.system_prompt)\nI want you to act like Yoda.\nAs you iterate on your prompt, you‚Äôll want to keep with a small set of challenging/important examples that you can regularly re-check with your latest version of the prompt. Writing automated tests for this can be challenging since LLMs aren‚Äôt deterministic by nature.2 Eventually, you may want a more systematic way to evaluate the prompt to ensure it continues to produce quality output.",
    "crumbs": [
      "Get started",
      "System prompt"
    ]
  },
  {
    "objectID": "get-started/system-prompt.html#inspiration-and-guides",
    "href": "get-started/system-prompt.html#inspiration-and-guides",
    "title": "System prompt",
    "section": "Inspiration and guides",
    "text": "Inspiration and guides\nNowadays there are many prompt ‚Äúlibraries‚Äù and ‚Äúguides‚Äù available online, offering a wide-array of inspiration and intuition for writing your own prompts.\nI particular like Anthropic‚Äôs prompt library. If you have an Anthropic account, this also pairs well with their prompt generator, improver, and more generally their prompting guide. Although the guide is specifically for Anthropic, I suspect it will be useful for other model providers as well. That said, other major players like OpenAI and Google have their own prompting guides, which are worth checking out as well.\nIf you‚Äôve never written a prompt, I also recommend reading Ethan Mollick‚Äôs Getting started with AI: Good enough prompting. This quote in particular has some sage advice for how to think about prompting in general:\n\nTreat AI like an infinitely patient new coworker who forgets everything you tell them each new conversation, one that comes highly recommended but whose actual abilities are not that clear. Two parts of this are analogous to working with humans (being new on the job and being a coworker) and two of them are very alien (forgetting everything and being infinitely patient). We should start with where AIs are closest to humans, because that is the key to good-enough prompting",
    "crumbs": [
      "Get started",
      "System prompt"
    ]
  },
  {
    "objectID": "get-started/system-prompt.html#various-tips",
    "href": "get-started/system-prompt.html#various-tips",
    "title": "System prompt",
    "section": "Various tips",
    "text": "Various tips\nThis section offers some specific tips for writing effective system prompts. These tips are based on our own experience on projects like querychat (a tool for running SQL queries through a chat interface).\n\nSet the scene\nTo help the LLM produce output that feels natural for the end user, it can be helpful to explain the how the user will interact with the LLM (e.g., ‚ÄúYou are a chatbot displayed in a dashboard‚Äù).\n\n\nDefine a purpose\nGive the LLM a sense of what the user is trying to accomplish. For example, querychat‚Äôs prompt includes the phrase ‚ÄúYou will be asked to perform various tasks on the data, such as filtering, sorting, and answering questions.‚Äù This helps the model understand that it should be focused on data analysis and not just general conversation.\n\n\nInfluence behavior\nLLMs tend to optimize for user satisfaction, which unfortunately means they are often overly agreeable, and aren‚Äôt necessarily concerned about accuracy. When accuracy is paramount, include instructions that encourage the model to be more cautious. For example, you might say ‚ÄúOnly answer if you are 100% sure of the answer, otherwise say ‚ÄòI don‚Äôt know‚Äô or ‚ÄòI‚Äôm not sure‚Äô.‚Äù\nLLMs can also be overly complimentary, verbose, and polite. For example, if you ask a model to summarize a long document, it may include a lot of unnecessary praise or compliments. To reduced this behavior, include instructions that encourage the model to be more concise and focused on the task at hand.\nLLMs also tend to provide an answer even when it‚Äôs not clear what the user is asking. In this case, it can help to include instructions that encourage the model to ask for clarification before providing an answer.\n\n\nUse specific examples\nModels tend to perform better when they have specific examples to use as a reference. The examples can help reinforce what is ‚Äúgood‚Äù vs ‚Äúbad‚Äù behavior as well as when and how to perform certain tasks. It‚Äôs also helpful exercise for you, the developer, to explain more precisely about how you want the model to behave in certain situations.\n\n\nOutline tasks\nIf the LLM is equipped with tools, you may want to be explicit about when and how to use them. Hopefully each tool has a docstring that explains what it does, but you may also want to include some specific examples of when to use each tool. It can also help to explain how the tool behavior influences with the larger user experience (e.g., what you plan on displaying to the user ‚Äì if anything ‚Äì when a tool is used).\n\n\nProvide missing info\nLLMs are trained on a wide variety of data, but they don‚Äôt necessarily have access to real-time or proprietary information. When the amount of ‚Äúmissing‚Äù information can reasonably fit in a context window, it‚Äôs often best to just include that information in the system prompt. For example, if you‚Äôre building a chatbot that needs to know about a specific table schema (like querychat does) to offer accurate SQL queries, include that information in the system prompt. However, if the amount of missing information takes up a significant portion of a context window, it may be better to use a different approach like RAG or tool calling.\n\n\nFollow-up suggestions\nLLMs are quite good at suggesting follow-up user prompts to explore an idea (or new ideas) further. They tend to do this by default, but you may want to encourage offering multiple suggestions, and to be more specific about the types of follow-up questions that would be useful. This is especially useful when the model is being used in a chat interface, where the user may not know what to ask next. Also, with a web frameworks like Shiny, it‚Äôs easy to turn these into input suggestions (i.e., links the user can click to quickly ask the model a follow-up question).",
    "crumbs": [
      "Get started",
      "System prompt"
    ]
  },
  {
    "objectID": "get-started/system-prompt.html#footnotes",
    "href": "get-started/system-prompt.html#footnotes",
    "title": "System prompt",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is because the model is stateless, meaning it doesn‚Äôt remember anything from previous requests. From a cost/efficiency perspective, repeatedly sending a large prompt is usually not a problem, especially with the help of things like prompt caching.‚Ü©Ô∏é\nSome model providers (e.g., ChatOpenAI) allow you to set a random seed, but this feature is not available for all model providers.‚Ü©Ô∏é",
    "crumbs": [
      "Get started",
      "System prompt"
    ]
  },
  {
    "objectID": "get-started/parameters.html",
    "href": "get-started/parameters.html",
    "title": "Parameters",
    "section": "",
    "text": "chatlas leverages typing and IDE autocomplete to help you discover and set parameters for your model provider. This includes both model parameters and client parameters. The former is primarily for customizing the behavior of the model, while the latter is for customizing the behavior of the HTTP client used to communicate with the model provider. The goal is to make it easy to discover and set these parameters in a type safe way, regardless of the model provider you‚Äôre using.",
    "crumbs": [
      "Get started",
      "Parameters"
    ]
  },
  {
    "objectID": "get-started/parameters.html#model-parameters",
    "href": "get-started/parameters.html#model-parameters",
    "title": "Parameters",
    "section": "Model parameters",
    "text": "Model parameters\nBoth the .chat() and .stream() methods accept a dictionary of model parameters via the kwargs parameter. Some of these parameters are common across providers (e.g., temperature, top_p), while others are specific to the provider (e.g., max_tokens, stop). Assuming your IDE has autocomplete / typing support, provider-specific parameters are shown as you type in keys and values can be type checked.\n\n\n\nScreenshot of IDE with typing support showing available parameters for model provider",
    "crumbs": [
      "Get started",
      "Parameters"
    ]
  },
  {
    "objectID": "get-started/parameters.html#client-parameters",
    "href": "get-started/parameters.html#client-parameters",
    "title": "Parameters",
    "section": "Client parameters",
    "text": "Client parameters\nWhen you initialize a Chat client, you can pass in a dictionary of client parameters via the kwargs parameter. These parameters are used to customize the behavior of the HTTP client used to communicate with the model provider. This can be useful for things like setting the timeout, number of retries, and other HTTP client-specific settings.\n\n\n\nScreenshot of IDE with typing support showing available parameters for HTTP client\n\n\nLLMs have model parameters that can be tweaked to change their behavior. Some of these parameters are unique to the model provider, while others (e.g., temperature, top_p) are quite common across providers. In addition, HTTP client parameters can be helpful for customizing the behavior of the HTTP client used to communicate with the model provider.",
    "crumbs": [
      "Get started",
      "Parameters"
    ]
  },
  {
    "objectID": "structured-data/entity-recognition.html",
    "href": "structured-data/entity-recognition.html",
    "title": "Entity recognition",
    "section": "",
    "text": "The following example, which closely inspired by the Claude documentation, shows how .extract_data() can be used to perform entity recognition.\n\nfrom chatlas import ChatOpenAI\nfrom pydantic import BaseModel, Field\nimport pandas as pd\n\n# | warning: false\ntext = \"John works at Google in New York. He met with Sarah, the CEO of Acme Inc., last week in San Francisco.\"\n\n\nclass NamedEntity(BaseModel):\n    \"\"\"Named entity in the text.\"\"\"\n\n    name: str = Field(description=\"The extracted entity name\")\n\n    type_: str = Field(description=\"The entity type, e.g. 'person', 'location', 'organization'\")\n\n    context: str = Field(description=\"The context in which the entity appears in the text.\")\n\n\nclass NamedEntities(BaseModel):\n    \"\"\"Named entities in the text.\"\"\"\n\n    entities: list[NamedEntity] = Field(description=\"Array of named entities\")\n\n\nchat = ChatOpenAI()\ndata = chat.extract_data(text, data_model=NamedEntities)\npd.DataFrame(data[\"entities\"])\n\n\n\n\n\n\n\n\nname\ntype_\ncontext\n\n\n\n\n0\nJohn\nperson\nsubject working at Google in New York\n\n\n1\nGoogle\norganization\norganization where John works\n\n\n2\nNew York\nlocation\nlocation where John works\n\n\n3\nSarah\nperson\nCEO of Acme Inc., met with John\n\n\n4\nAcme Inc.\norganization\ncompany where Sarah is CEO\n\n\n5\nSan Francisco\nlocation\nlocation where John met Sarah last week",
    "crumbs": [
      "Structured data",
      "Entity recognition"
    ]
  },
  {
    "objectID": "structured-data/classification.html",
    "href": "structured-data/classification.html",
    "title": "Classification",
    "section": "",
    "text": "The following example, which closely inspired by the Claude documentation, shows how .extract_data() can be used to perform text classification.\n\nfrom typing import Literal\n\nfrom chatlas import ChatOpenAI\nfrom pydantic import BaseModel, Field\nimport pandas as pd\n\ntext = \"The new quantum computing breakthrough could revolutionize the tech industry.\"\n\nclass Classification(BaseModel):\n    name: Literal[\n        \"Politics\", \"Sports\", \"Technology\", \"Entertainment\", \"Business\", \"Other\"\n    ] = Field(description=\"The category name\")\n\n    score: float = Field(\n        description=\"The classification score for the category, ranging from 0.0 to 1.0.\"\n    )\n\nclass Classifications(BaseModel):\n    \"\"\"Array of classification results. The scores should sum to 1.\"\"\"\n    classifications: list[Classification]\n\n\nchat = ChatOpenAI()\ndata = chat.extract_data(text, data_model=Classifications)\npd.DataFrame(data[\"classifications\"])\n\n\n\n\n\n\n\n\nname\nscore\n\n\n\n\n0\nTechnology\n0.75\n\n\n1\nBusiness\n0.15\n\n\n2\nOther\n0.05\n\n\n3\nEntertainment\n0.02\n\n\n4\nPolitics\n0.02\n\n\n5\nSports\n0.01",
    "crumbs": [
      "Structured data",
      "Classification"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "chatlas\nYour friendly guide to building LLM chat apps in Python with less effort and more clarity.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "Overview",
    "section": "Quick start",
    "text": "Quick start\nGet started in 3 simple steps:\n\nChoose a model provider, such as ChatOpenAI or ChatAnthropic.\nVisit the provider‚Äôs reference page to get setup with necessary credentials.\nCreate the relevant Chat client and start chatting!\n\nfrom chatlas import ChatOpenAI\n\n# Optional (but recommended) model and system_prompt\nchat = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\n# Optional tool registration\ndef get_current_weather(lat: float, lng: float):\n    \"Get the current weather for a given location.\"\n    return \"sunny\"\n\nchat.register_tool(get_current_weather)\n\n# Send user prompt to the model for a response.\nchat.chat(\"How's the weather in San Francisco?\")\n\n# üõ†Ô∏è tool request\nget_current_weather(37.7749, -122.4194)\n# ‚úÖ tool result\nsunny\nThe current weather in San Francisco is sunny.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Overview",
    "section": "Install",
    "text": "Install\nInstall the latest stable release from PyPI:\npip install -U chatlas",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#why-chatlas",
    "href": "index.html#why-chatlas",
    "title": "Overview",
    "section": "Why chatlas?",
    "text": "Why chatlas?\nüöÄ Opinionated design: most problems just need the right model, system prompt, and tool calls. Spend more time mastering the fundamentals and less time navigating needless complexity.\nüß© Model agnostic: try different models with minimal code changes.\nüåä Stream output: automatically in notebooks, at the console, and your favorite IDE. You can also stream responses into bespoke applications (e.g., chatbots).\nüõ†Ô∏è Tool calling: give the LLM ‚Äúagentic‚Äù capabilities by simply writing Python function(s).\nüîÑ Multi-turn chat: history is retained by default, making the common case easy.\nüñºÔ∏è Multi-modal input: submit input like images, pdfs, and more.\nüìÇ Structured output: easily extract structure from unstructured input.\n‚è±Ô∏è Async: supports async operations for efficiency and scale.\n‚úèÔ∏è Autocomplete: easily discover and use provider-specific parameters like temperature, max_tokens, and more.\nüîç Inspectable: tools for debugging and monitoring in production.\nüîå Extensible: add new model providers, content types, and more.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Overview",
    "section": "Next steps",
    "text": "Next steps\nNext we‚Äôll learn more about what model providers are available and how to approach picking a particular model. If you already have a model in mind, or just want to see what chatlas can do, skip ahead to hello chat.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "misc/vocabulary.html",
    "href": "misc/vocabulary.html",
    "title": "Vocabulary",
    "section": "",
    "text": "If you‚Äôre new to LLMs, you may be confused by some of the vocabulary. This is a quick guide to define the most important terms you‚Äôll need to know to get started with chatlas and LLMs in general. Unfortunately the vocab is all a little entangled: to understand one term you‚Äôll often have to know a little about some of the others. So we‚Äôll start with some simple definitions of the most important terms then iteratively go a little deeper.\nIt all starts with a prompt, which is the text (typically a question or a request) that you send to the LLM. This starts a conversation, a sequence of turns that alternate between user prompts and model responses. Inside the model, both the prompt and response are represented by a sequence of tokens, which represent either individual words or subcomponents of a word. The tokens are used to compute the cost of using a model and to measure the size of the context, the combination of the current prompt and any previous prompts and responses used to generate the next response.\nIt‚Äôs useful to make the distinction between providers and models. A provider is a web API that gives access to one or more models. The distinction is a bit subtle because providers are often synonymous with a model, like OpenAI and GPT, Anthropic and Claude, and Google and Gemini. But other providers, like Ollama, can host many different models, typically open source models like LLaMa and Mistral. Still other providers support both open and closed models, typically by partnering with a company that provides a popular closed model. For example, Azure OpenAI offers both open source models and OpenAI‚Äôs GPT, while AWS Bedrock offers both open source models and Anthropic‚Äôs Claude.\n\nWhat is a token?\nAn LLM is a model, and like all models needs some way to represent its inputs numerically. For LLMs, that means we need some way to convert words to numbers. This is the goal of the tokenizer. For example, using the GPT 4o tokenizer, the string ‚ÄúWhen was R created?‚Äù is converted to 5 tokens: 5958 (‚ÄúWhen‚Äù), 673 (‚Äù was‚Äù), 460 (‚Äù R‚Äù), 5371 (‚Äù created‚Äù), 30 (‚Äú?‚Äù). As you can see, many simple strings can be represented by a single token. But more complex strings require multiple tokens. For example, the string ‚Äúcounterrevolutionary‚Äù requires 4 tokens: 32128 (‚Äúcounter‚Äù), 264 (‚Äúre‚Äù), 9477 (‚Äúvolution‚Äù), 815 (‚Äúary‚Äù). (You can see how various strings are tokenized at http://tiktokenizer.vercel.app/).\nIt‚Äôs important to have a rough sense of how text is converted to tokens because tokens are used to determine the cost of a model and how much context can be used to predict the next response. On average an English word needs ~1.5 tokens so a page might require 375-400 tokens and a complete book might require 75,000 to 150,000 tokens. Other languages will typically require more tokens, because (in brief) LLMs are trained on data from the internet, which is primarily in English.\nLLMs are priced per million tokens. State of the art models (like GPT-4o or Claude 3.5 sonnet) cost $2-3 per million input tokens, and $10-15 per million output tokens. Cheaper models can cost much less, e.g.¬†GPT-4o mini costs $0.15 per million input tokens and $0.60 per million output tokens. Even $10 of API credit will give you a lot of room for experimentation, particularly with cheaper models, and prices are likely to decline as model performance improves.\nTokens also used to measure the context window, which is how much text the LLM can use to generate the next response. As we‚Äôll discuss shortly, the context length includes the full state of your conversation so far (both your prompts and the model‚Äôs responses), which means that cost grow rapidly with the number of conversational turns.\n\n\nWhat is a conversation?\nA conversation with an LLM takes place through a series of HTTP requests and responses: you send your question to the LLM as an HTTP request, and it sends back its reply as an HTTP response. In other words, a conversation consists of a sequence of a paired turns: a sent prompt and a returned response.\nIt‚Äôs important to note that a request includes not only the current user prompt, but every previous user prompt and model response. This means that:\n\nThe cost of a conversation grows quadratically with the number of turns: if you want to save money, keep your conversations short.\nEach response is affected by all previous prompts and responses. This can make a converstion get stuck in a local optimum, so it‚Äôs generally better to iterate by starting a new conversation with a better prompt rather than having a long back-and-forth.\nchatlas has full control over the conversational history. Because it‚Äôs chatlas‚Äôs responsibility to send the previous turns of the conversation, it‚Äôs possible to start a conversation with one model and finish it with another.\n\n\n\nWhat is a prompt?\nThe user prompt is the question that you send to the model. There are two other important prompts that underlie the user prompt:\n\nThe core system prompt, which is unchangeable, set by the model provider, and affects every conversation. You can see what these look like from Anthropic, who publishes their core system prompts.\nThe system prompt, which is set when you create a new conversation, and affects every response. It‚Äôs used to provide additional instructions to the model, shaping its responses to your needs. For example, you might use the system prompt to ask the model to always respond in Spanish or to write dependency-free base R code. You can also use the system prompt to provide the model with information it wouldn‚Äôt otherwise know, like the details of your database schema, or your preferred plotly theme and color palette.\n\nWhen you use a chat app like ChatGPT or claude.ai you can only iterate on the user prompt. But when you‚Äôre programming with LLMs, you‚Äôll primarily iterate on the system prompt. For example, if you‚Äôre developing an app that helps a user write Python code, you‚Äôd work with the system prompt to ensure that user gets the style of code they want.\nWriting a good prompt is key to effective use of LLMs. For some tips on writing a good system prompt, see the System Prompt page.",
    "crumbs": [
      "Miscellaneous",
      "Vocabulary"
    ]
  },
  {
    "objectID": "misc/RAG.html",
    "href": "misc/RAG.html",
    "title": "RAG",
    "section": "",
    "text": "Retrieval-Augmented Generation (RAG) is a technique that can improve LLM output by grounding it with external, trusted content. RAG workflows can have varying degrees of sophistication, but at their core, they all share a retrieval step that fetches relevant information from a knowledge store. The retrieved information (along with the user query) is then provided as additional context to the LLM for response generation. In this article, you‚Äôll learn how to do exactly this to improve the quality of responses in your chatlas applications.\nIn theory, grounding the LLM‚Äôs response in trusted content helps to reduce hallucination. But in practice, RAG can be fickle ‚Äì it‚Äôs hard to always retrieve the right information for every user query, and it‚Äôs not always predictable how the LLM will actually use the retrieved content. For this reason, it‚Äôs helpful for your RAG workflow to be transparent (so it‚Äôs easy to debug, understand, and modify) because some trial and error will be necessary. It can also be very helpful to combine RAG with additional techniques, such as:\nIn any RAG workflow, you‚Äôll always want to apply the 1st technique (i.e., set guidelines). However, before diving into the 2nd technique (i.e., dynamic retrieval), let‚Äôs first learn the basics.",
    "crumbs": [
      "Miscellaneous",
      "RAG"
    ]
  },
  {
    "objectID": "misc/RAG.html#basic-retrieval",
    "href": "misc/RAG.html#basic-retrieval",
    "title": "RAG",
    "section": "Basic retrieval",
    "text": "Basic retrieval\nBasic retrieval is the simplest form of RAG, where you retrieve (a fixed amount of) relevant content from a knowledge store based on the user‚Äôs query and provide it to the chat model. It looks something roughly like this:\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(\n    system_prompt=\"You are a helpful, but terse, assistant. \"\n    \"If you can't answer the question based on the trusted content, say so.\",\n)\n\nuser_query = \"Who created the unicorn programming language?\"\n\n# A placeholder for your retrieval logic\ntrusted_content = retrieve_trusted_content(user_query)  \n\nchat.chat(trusted_content, user_query)\nIn the sections that follow, we‚Äôll implement the retrieve_trusted_content() step of this workflow. And, as we‚Äôll see, there are several moving parts to consider when implementing this step.\nObviously, in order to retrieve trusted content, we first need some content to retrieve. Typically content is retrieved from a knowledge store ‚Äì essentially a database that stores documents in a way that allows for efficient retrieval based on semantic similarity. A knowledge store also often takes the form of a vector store or embedding index because of it‚Äôs efficiency in storing and retrieving content based on semantic similarity. This approach requires embedding the content into numerical vectors, which can be done using various machine learning models.\n\nCreate store\nPython has a plethora of options for working with knowledge stores (llama-index, pinecone, etc.). It doesn‚Äôt really matter which one you choose, but due to its popularity, maturity, and simplicity, lets demonstrate with the llama-index library:\npip install llama-index\nWith llama-index, it‚Äôs easy to create a knowledge store from a wide variety of input formats, such as text files, web pages, and much more. That said, for this example, I‚Äôll assume you have a directory (data) with some text files that you want to use as trusted content. This snippet will ingest the files, embed them, and create a vector store index that is ready for retrieval.\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n\ndocs = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(docs)\n\n\n\n\n\n\nEmbed pre-requisites\n\n\n\nBy default, VectorStoreIndex tries to use an OpenAI model to embed the docs, which will fail if you don‚Äôt have an OpenAI API key set up. Either set the OPENAI_API_KEY environment variable to your OpenAI API key, or see the next tip if you‚Äôd rather use a free embedding model.\n\n\n\n\n\n\n\n\nCustom embed model\n\n\n\n\n\nThe embedding model used by VectorStoreIndex can be customized via in the Settings object. For example, to use a (free) Hugging Face embedding model, first install:\npip install llama-index-embeddings-huggingface\nThen set the embed_model in the Settings object to reference the Hugging Face model you want to use. This way, you can use a free and open source embedding model without needing an API key.\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\n\n\n\n\n\n\n\n\nCustom vector stores\n\n\n\nThe code provided here just uses llama-index‚Äôs default vector store, but it supports a wide variety of vector stores, such as DuckDB, Pinecone, and much more.\n\n\n\n\n\n\n\n\nChunking defaults\n\n\n\nIf your documents are large (e.g., long articles or books), it‚Äôs a good idea to split them into smaller chunks to improve retrieval performance. This is important since, if the content relevant to a user query is only a small part of a larger document, retrieving the entire document probably won‚Äôt be efficient or effective. When creating the index, llama-index will automatically chunk the documents into smaller pieces, which can be configured via the Settings object:\nfrom llama_index.core import Settings\n\nSettings.chunk_size = 512\nSettings.chunk_overlap = 50\n\n\n\n\nSave store\nIf you have a large number of documents, creating a vector store index can be time-consuming, so you don‚Äôt want to recreate it every time you run your application. Thankfully, you can save the index to a directory on disk so you don‚Äôt have to recreate it every time you run your application. This can be done with:\nindex.storage_context.persist(persist_dir=\"./storage\")\nNow, when we go to retrieve content in our app, we can load the index from disk instead of recreating it every time.\n\n\nRetrieve content\nWith our index now available on disk, we‚Äôre ready to implement retrieve_trusted_content() ‚Äì the step that retrieves relevant content from the knowledge store based on the user query.\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n# Load the knowledge store (index) from disk\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)\n\ndef retrieve_trusted_content(query):\n    retriever = index.as_retriever(similarity_top_k=5)\n    nodes = retriever.retrieve(query)\n    return [f\"&lt;excerpt&gt;{x.text}&lt;/excerpt&gt;\" for x in nodes]\nThis particular implementation retrieves the top 5 most relevant documents from the index based on the user query, but you can adjust the number of results by changing the similarity_top_k parameter. There‚Äôs no magic number for this parameter, but llama-index defaults to 2, so you may want to increase it if you find that the retrieved content is too sparse or not relevant enough. That said, you can also leverage",
    "crumbs": [
      "Miscellaneous",
      "RAG"
    ]
  },
  {
    "objectID": "misc/RAG.html#dynamic-retrieval",
    "href": "misc/RAG.html#dynamic-retrieval",
    "title": "RAG",
    "section": "Dynamic retrieval",
    "text": "Dynamic retrieval\nDynamic retrieval is similar to basic retrieval, except that instead of the retrieval being a single fixed step before response generation, it is provided as a tool to the LLM. This results in a much more robust and flexible RAG workflow, as the LLM can decide if, when, and how much contents to retrieve from the knowledge store before generating a response. It can also decide what to provide as input to the retrieval step(s), rather than just using the user query directly, which can be useful if the user query is ambiguous or incomplete.\nTo implement dynamic retrieval, we can just take the retrieve_trusted_content() function we just implemented as a tool with the chat model. When doing this, make sure you provide a clear description of the tool‚Äôs purpose and how it should be used, as this will help the LLM understand how to use it effectively. You could even add a parameter to the tool that allows the LLM to specify how many results it wants to retrieve, which can be useful for more complex queries.\nfrom chatlas import ChatOpenAI\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n# Load the knowledge store (index) from disk\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)\n\ndef retrieve_trusted_content(query: str, top_k: int = 5):\n    \"\"\"\n    Retrieve relevant content from the knowledge store.\n\n    Parameters\n    ----------\n    query\n        The query used to semantically search the knowledge store.\n    top_k\n        The number of results to retrieve from the knowledge store.\n    \"\"\"\n    retriever = index.as_retriever(similarity_top_k=top_k)\n    nodes = retriever.retrieve(query)\n    return [f\"&lt;excerpt&gt;{x.text}&lt;/excerpt&gt;\" for x in nodes]\n\nchat = ChatOpenAI(\n    system_prompt=\"You are a helpful, but terse, assistant. \"\n    \"If you can't answer the question based on the trusted content, say so.\"\n)\n\nchat.register_tool(retrieve_trusted_content)\n\nchat.chat(\"Who created the unicorn programming language?\")",
    "crumbs": [
      "Miscellaneous",
      "RAG"
    ]
  },
  {
    "objectID": "tool-calling/displays.html",
    "href": "tool-calling/displays.html",
    "title": "Displays and results",
    "section": "",
    "text": "When delivering experiences such as in a chatbot app, it‚Äôs strongly recommended to give your users:\n\nA visual indication when a tool is requested by the model\nA choice to approve or deny that request\nA clear display of tool results\n\nIn tool calling, we saw how .chat() automatically handles 1 and 3 for you, providing a nice developer experience out of the box. However, when streaming in something like a chatbot app, you‚Äôll need to do a bit more work to provide these features.\n\nContent objects\nTo display tool calls when streaming, first set the content parameter to \"all\". This way, when a tool call occurs, the stream will include ContentToolRequest and ContentToolResult objects, with information about the tool call. These classes have smart defaults for methods such as _repr_markdown_() and _repr_html_(). As a result, they will render sensibly in Jupyter notebooks and other environments that support rich content displays. They also have methods for specific web frameworks like Shiny, giving you output more tailored for the framework you‚Äôre using.\nFor a quick example, here‚Äôs a Shiny chatbot that displays tool calls in a user-friendly way.\n\n\nclient.py\n\nimport requests\nfrom chatlas import ChatAnthropic\n\nchat_client = ChatAnthropic()\n\ndef get_current_weather(lat: float, lng: float):\n    \"\"\"Get the current temperature given a latitude and longitude.\"\"\"\n\n    lat_lng = f\"latitude={lat}&longitude={lng}\"\n    url = f\"https://api.open-meteo.com/v1/forecast?{lat_lng}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n    res = requests.get(url)\n    return res.json()[\"current\"]\n\nchat_client.register_tool(get_current_weather)\n\n\n\napp.py\n\nfrom client import chat_client\nfrom shiny.express import ui\n\nchat = ui.Chat(id=\"chat\")\nchat.ui(messages=[\"Hello! How can I help you today?\"])\n\n@chat.on_user_submit\nasync def _(user_input: str):\n    response = await chat_client.stream_async(\n      user_input,\n      content=\"all\"\n    )\n    await chat.append_message_stream(response)\n\n\n\n\nScreenshot of a tool result in Shiny.\n\n\n\n\nCustom displays\nTo customize how a tool result is actually rendered, you can leverage the fact that the tool can return a ContentToolResult instance instead of a simple value. By subclassing this class and overriding it‚Äôs default methods, you can create custom, rich, interactive displays for your tool results in various contexts. Here‚Äôs an extension of the previous example to displays the weather result on an interactive map using ipywidgets and ipyleaflet.\n\n\nShow code\n\nfrom chatlas import ContentToolResult\nimport ipywidgets\nfrom ipyleaflet import Map, CircleMarker\nfrom shinywidgets import register_widget, output_widget\n\nclass WeatherToolResult(ContentToolResult):\n    def tagify(self):\n        if self.error:\n            return super().tagify()\n\n        args = self.arguments\n        loc = (args[\"latitude\"], args[\"longitude\"])\n        info = (\n            f\"&lt;h6&gt;Current weather&lt;/h6&gt;\"\n            f\"Temperature: {self.value['temperature_2m']}¬∞C&lt;br&gt;\"\n            f\"Wind: {self.value['wind_speed_10m']} m/s&lt;br&gt;\"\n            f\"Time: {self.value['time']}\"\n        )\n\n        m = Map(center=loc, zoom=10)\n        m.add_layer(\n            CircleMarker(location=loc, popup=ipywidgets.HTML(info))\n        )\n\n        register_widget(self.id, m)\n        return output_widget(self.id)\n\ndef get_current_weather(lat: float, lng: float):\n    \"\"\"Get the current temperature given a latitude and longitude.\"\"\"\n\n    lat_lng = f\"latitude={lat}&longitude={lng}\"\n    url = f\"https://api.open-meteo.com/v1/forecast?{lat_lng}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n    response = requests.get(url)\n    json = response.json()\n    return WeatherToolResult(value=json[\"current\"])\n\n\n\n\nScreenshot of a tool result as an interactive map\n\n\n\n\nCustom model results\nBy default, tool results are formatted as a JSON string, which is suitable for most use cases. However, that might not be ideal for all scenarios, especially if your tool does something sophisticated like return an image for the model to consume. In such cases, you can use the ContentToolResult class to return the result in a different format. For example, if you want to pass the return value of the tool function directly to the model without any formatting, set the model_format parameter to \"as_is\":\nimport base64\nimport requests\n\nimport chatlas as ctl\n\ndef get_picture():\n    \"Returns an image\"\n    url = \"https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png\"\n    bytez = requests.get(url).content\n    res = [\n        {\n            \"type\": \"image\",\n            \"source\": {\n                \"type\": \"base64\",\n                \"media_type\": \"image/png\",\n                \"data\": base64.b64encode(bytez).decode(\"utf-8\"),\n            },\n        }\n    ]\n    return ctl.ContentToolResult(value=res, model_format=\"as_is\")\n\nchat = ctl.ChatAnthropic()\nchat.register_tool(get_picture)\n\nres = chat.chat(\n    \"You have a tool called 'get_picture' available to you. \"\n    \"When called, it returns an image. Tell me what you see in the image.\",\n    echo=\"text\"\n)\n\nThe image shows four translucent colored dice arranged together. There‚Äôs a red die in the foreground, a blue die in the upper\nleft, a green die in the upper right, and a yellow die at the bottom. All dice appear to be standard six-sided dice with white dots (pips) representing the numbers 1 through 6. The dice have a glossy, semi-transparent appearance that gives them a\ncolorful, vibrant look against the white background. The image has a shallow depth of field, creating a slight blur effect on\nthe dice that aren‚Äôt in the foreground, which emphasizes the red die in the center.",
    "crumbs": [
      "Tool calling",
      "Displays and results"
    ]
  }
]